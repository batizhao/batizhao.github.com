<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Gunner</title>
  <icon>https://www.gravatar.com/avatar/5ecd272cf8499dfea716cd38f49ff67f</icon>
  <subtitle>枪手</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://batizhao.github.io/"/>
  <updated>2018-09-12T08:51:17.079Z</updated>
  <id>http://batizhao.github.io/</id>
  
  <author>
    <name>巴蒂</name>
    <email>zhaobati@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>在 Kubernetes 上部署 Hyperledger Fabric v1.2（二）</title>
    <link href="http://batizhao.github.io/2018/08/13/deploy-fabric-on-kubernetes-two/"/>
    <id>http://batizhao.github.io/2018/08/13/deploy-fabric-on-kubernetes-two/</id>
    <published>2018-08-13T01:27:04.000Z</published>
    <updated>2018-09-12T08:51:17.079Z</updated>
    
    <content type="html"><![CDATA[<p>在上一篇文章中，我们介绍了在 Kubernetes 上运行 Fabric 的机制和架构。 这篇文章会详细讲解安装的具体步骤。</p><h2 id="准备环境"><a href="#准备环境" class="headerlink" title="准备环境"></a>准备环境</h2><h3 id="CMD-客户机"><a href="#CMD-客户机" class="headerlink" title="CMD 客户机"></a>CMD 客户机</h3><ul><li>自己的 Mac（10.4.249.231）</li><li>可以通过 kubectl 操作远程的 Kubernetes 集群</li><li>安装 Python3，部署脚本是用 Python 写的</li></ul><h3 id="集群环境"><a href="#集群环境" class="headerlink" title="集群环境"></a>集群环境</h3><ul><li>CentOS7</li><li>Kubernetes v1.11.0</li><li>Docker 18.03.1-ce</li></ul><h3 id="NFS"><a href="#NFS" class="headerlink" title="NFS"></a>NFS</h3><ul><li>给集群做共享存储，挂载证书和一些 channel 文件。</li></ul><a id="more"></a><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>本文用到的代码在 <a href="https://github.com/batizhao/fabric-on-kubernetes" target="_blank" rel="noopener">https://github.com/batizhao/fabric-on-kubernetes</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fabric-on-kubernetes</span><br><span class="line"> |--README.md</span><br><span class="line"> |--generateALL.sh             // 生成 K8S yaml file</span><br><span class="line"> |--transform                  // 使用 kubectl 部署或者卸载 Fabric </span><br><span class="line"> |--templates                  // K8S yaml 模板</span><br><span class="line"> |--crypto-config.yaml         // Fabric 集群配置文件</span><br><span class="line"> |--configtx.yaml              // channel 和创世块配置</span><br></pre></td></tr></table></figure><h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><h3 id="A-crypto-config-yaml"><a href="#A-crypto-config-yaml" class="headerlink" title="A. crypto-config.yaml"></a><strong>A. crypto-config.yaml</strong></h3><p>cryptogen 工具根据 crypto-config.yaml 来生成 Fabric 成员的证书，一个简单的例子如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">OrdererOrgs:</span></span><br><span class="line"><span class="attr">  - Name:</span> <span class="string">Orderer1</span></span><br><span class="line"><span class="attr">    Domain:</span> <span class="string">orgorderer1</span></span><br><span class="line"><span class="attr">    Template:</span></span><br><span class="line"><span class="attr">      Count:</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">PeerOrgs:</span></span><br><span class="line"><span class="attr">  - Name:</span> <span class="string">Org1</span></span><br><span class="line"><span class="attr">    Domain:</span> <span class="string">org1</span></span><br><span class="line"><span class="attr">    EnableNodeOUs:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    Template:</span></span><br><span class="line"><span class="attr">      Count:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">    Users:</span></span><br><span class="line"><span class="attr">      Count:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  - Name:</span> <span class="string">Org2</span></span><br><span class="line"><span class="attr">    Domain:</span> <span class="string">org2</span></span><br><span class="line"><span class="attr">    EnableNodeOUs:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    Template:</span></span><br><span class="line"><span class="attr">      Count:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">    Users:</span></span><br><span class="line"><span class="attr">      Count:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p>其中 OrdererOrgs 和 PeerOrgs 关键字区分 organization 的类型，两种组织的内部结构如下：</p><p>1) OrdererOrgs 中定义了一个名字为 Orderer ，域名为 orgorderer1 的 org ，并且它指定 template 中 count 的数值为 1，则在该 org 下只有一个 orderer ，其 id 为 orderer0 。</p><p>2) PeerOrgs 中定义了两个 org ，分别为 Org1 和 Org2 ，对应的域名为 org1、 org2 与 orderer 类似，每个 org 生成了两个 peers ，虽然 org1 中 peer0 和 org2 中 peer0 的 ID 重复，但是他不属于同一个 org ，通过域名很容易就能区分出它们。</p><p>需要注意的是，由于 K8S 中的 namespace 不支持 ‘.’ 和大写字母，因此各个组织的域名不能包含这些字符。</p><p>更多关于 crypto-config.yaml 的配置方式，请参考 Fabric 源码中的关于 cryptogen 的描述 （ fabric/common/tools/cryptogen/main.go）</p><p>cryptogen 工具会生成 crypto-config 目录，该目录的结构如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">crypto-config</span><br><span class="line">|--- ordererOrganizations</span><br><span class="line">|     |--- orgorderer1</span><br><span class="line">|           |--- msp</span><br><span class="line">|           |--- ca</span><br><span class="line">|           |--- tlsca</span><br><span class="line">|           |--- users</span><br><span class="line">|           |--- orderers</span><br><span class="line">|           |--- orderer0.orgorderer1</span><br><span class="line">|                 |--- msp</span><br><span class="line">|                 |--- tls</span><br><span class="line">|</span><br><span class="line">|--- peerOrganizations</span><br><span class="line">      |--- org1</span><br><span class="line">      |     |--- msp</span><br><span class="line">      |     |--- ca</span><br><span class="line">      |     |--- tlsca</span><br><span class="line">      |     |--- users</span><br><span class="line">      |     |--- peers</span><br><span class="line">      |           |--- peer0.org1</span><br><span class="line">      |           |     |--- msp</span><br><span class="line">      |           |     |--- tls</span><br><span class="line">      |           |--- peer1.org1</span><br><span class="line">      |                 |--- msp</span><br><span class="line">      |                 |--- tls</span><br><span class="line">      |--- org2</span><br><span class="line">            |--- msp</span><br><span class="line">            |--- ca</span><br><span class="line">            |--- tlsca</span><br><span class="line">            |--- users</span><br><span class="line">            |--- peers</span><br><span class="line">                   |--- peer0.org2</span><br><span class="line">                   |     |--- msp</span><br><span class="line">                   |     |--- tls</span><br><span class="line">                   |--- peer1.org2</span><br><span class="line">                         |--- msp</span><br><span class="line">                         |--- tls</span><br></pre></td></tr></table></figure><p>可以看出，每个 org 都包含了 msp、 ca、 tlsca 和 users 目录，然后根据 org 类型的不同，还分别有 peers 和 orderers 目录，里面存放着 org 中每个成员的 msp 和 tls 文件。</p><h3 id="B-configtx-yaml"><a href="#B-configtx-yaml" class="headerlink" title="B. configtx.yaml"></a><strong>B. configtx.yaml</strong></h3><p>configtxgen 工具根据该文件生成 Orderer 初始化的时候要使用的 genesis.block（创世块），获知 organization 的各种信息。因此，用户要根据 crypto-config.yaml 中关于 organization 的定义来修改 configtx.yaml 以生成合适的 genesis.block 。例如，用户在 crypto-config.yaml 中增加了一个 Org3 ，并且要创建一个包含 Org1， Org2， Org3 的集群，则应该通过以下两步修改 configtx.yaml ：</p><p>在 profile 中增加 Org3:</p><p><img src="/images/2018-08-10-deploy-fabric-on-kubernetes-two-1.jpeg" alt="图1"></p><p>在 Organization 中增加 Org3 的 MSPDir</p><p><img src="/images/2018-08-10-deploy-fabric-on-kubernetes-two-2.jpeg" alt="图2"></p><p>注意的是每个 organization 中的 MSPDir 的值必须是这种形式：</p><p>crypto-config/{OrgType}/{OrgName}/msp</p><h2 id="模板文件"><a href="#模板文件" class="headerlink" title="模板文件"></a>模板文件</h2><p>在 Kubernetes 中部署 Fabric 时，需要为每个节点编写相应的配置文件。由于节点数可能很多，这是既复杂又易错的重复劳动。为提高效率，可通过模板自动生成配置文件。本文使用了 5 个模板文件，可用脚本替换其中的变量，均在笔者给出示例代码中的 templates 目录中，这些模板的作用如下：</p><h3 id="A-namespace-yaml"><a href="#A-namespace-yaml" class="headerlink" title="A. namespace.yaml"></a><strong>A. namespace.yaml</strong></h3><p>定义 Fabric 集群在 K8s 中的 namespace ，它对应着 organization 的域名。为了在多节点共享证书等文件，使用了 NFS 服务器作为存储。在 K8s 中通过相应的 PV 和 PVC ，namespace 下的 Pod 可以通过 PVC 来获取与之相应的文件。</p><h3 id="B-cli-yaml"><a href="#B-cli-yaml" class="headerlink" title="B. cli.yaml"></a><strong>B. cli.yaml</strong></h3><p>CLI pod 模板，每个 organization 中都配备了一个 CLI pod，目的是提供命令行界面，可统一管理组织内的所有 peer ，其中包括 channel 的创建， chaincode 的安装等。CLI Pod 的 CORE_PEER_ADDRESS 环境变量默认值为 org 中的第一个 peer，可以通过修改该环境变量来连接不同的 peer 。</p><p>yaml 文件中的 command 是为了防止 CLI pod 自动退出，CLI 的默认工作目录为 /opt/gopath/src/github.com/hyperledger/fabric/peer 。由于该目录下的 channel-artifacts 挂载了 NFS 上 /opt/share/channel-artifacts，因此把创建 channel 时返回的 xxx.block 文件放在该目录下供所有 CLI Pod共享。</p><h3 id="C-ca-yaml"><a href="#C-ca-yaml" class="headerlink" title="C. ca.yaml"></a><strong>C. ca.yaml</strong></h3><p>Fabric 的 CA 服务的 pod 定义模板，用于 organization 中的证书管理，其 yaml 文件除了定义 deployment 外，还定义了 service 。service 通过 selector 与 deployment 绑定，其中 deployment 中的 label 是 selector 与其绑定的根据。 </p><h3 id="D-orderer-yaml"><a href="#D-orderer-yaml" class="headerlink" title="D. orderer.yaml"></a><strong>D. orderer.yaml</strong></h3><p>Orderer 的 pod 定义模板，需要注意的是，cryptogen 并不会生成 genesis.block ，然而缺少该文件时，orderer 会启动失败，因此在启动 orderer 之前需要预先生成 genesis.block ，并将其放在相应的 org 目录下。</p><h3 id="E-peer-yaml"><a href="#E-peer-yaml" class="headerlink" title="E. peer.yaml"></a><strong>E. peer.yaml</strong></h3><p>每个 peer pod 的定义模板。在该 yaml 中分别定义了 peer 和 couchDB 两个 container 。在实例化 chaincode (cc) 时，peer 需要连接 Docker 引擎来创建 cc 容器，因此要把 worker 宿主机的 var/run/docker.sock 映射到 peer 容器内部。</p><h2 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h2><p>在 Fabric 设计中， chaincode 目前是以 Docker 容器的方式运行在 peer 容器所在的宿主机上，peer 容器需要调用 Docker 引擎的接口来构建和创建 chaincode 容器，调用接口是通过这个连接：</p><p>unix:///var/run/docker.sock</p><p>通过 docker.sock 创建的容器脱离在 Kubernetes 的体系之外，虽然它仍在 Flannel 的网络上，但却无法获得 peer 节点的 IP 地址。这是因为创建该容器的 Docker 引擎使用宿主机默认的 DNS 解析来 peer 的域名，所以无法找到。</p><p>为了解决解析域名的问题，需要在每个 worker 的 DOCKER_OPTS 中加入相关参数，我的 kube-dns 的 IP 为10.68.0.2，宿主机网络 DNS 的 IP 地址假设为 10.4.246.1，为使得 chaincode 的容器可以解析到 peer 节点，在每个 Docker 节点，修改步骤如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> echo 'DOCKER_OPTS="--dns=10.68.0.2 --dns=10.4.246.1 --dns-search default.svc.cluster.local --dns-search svc.cluster.local --dns-opt ndots:2 --dns-opt timeout:2 --dns-opt attempts:2"' &gt;&gt; /etc/default/docker</span><br><span class="line"><span class="meta">#</span> echo 'EnvironmentFile=-/etc/default/docker' &gt;&gt; /etc/systemd/system/docker.service</span><br><span class="line"><span class="meta">#</span> systemctl daemon-reload &amp;&amp; systemctl restart docker &amp;&amp; systemctl status docker</span><br></pre></td></tr></table></figure><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p>以下操作都在 CMD 客户机上进行，NFS 的共享目录为 /opt/share。</p><h3 id="在-CMD-中挂载-NFS-目录"><a href="#在-CMD-中挂载-NFS-目录" class="headerlink" title="在 CMD 中挂载 NFS 目录"></a>在 CMD 中挂载 NFS 目录</h3><p>在 nfs server 上创建 /data 和 /opt/share 两个目录，保证 CMD 和集群可以有权限读写。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> cat /etc/exports</span><br><span class="line">/opt/share 10.4.249.231(rw,insecure,no_root_squash)</span><br><span class="line">/data 10.4.249.231(rw,insecure,no_root_squash)</span><br><span class="line"></span><br><span class="line">/data 172.31.21.0/24(rw,insecure,no_root_squash)</span><br><span class="line">/opt/share 172.31.21.0/24(rw,insecure,no_root_squash)</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> exportfs -rv</span><br><span class="line">exporting 10.4.249.231:/data</span><br><span class="line">exporting 10.4.249.231:/opt/share</span><br><span class="line">exporting 172.31.21.0/24:/opt/share</span><br><span class="line">exporting 172.31.21.0/24:/data</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> showmount -e 172.31.21.208</span><br><span class="line">Exports list on 172.31.21.208:</span><br><span class="line">/data                               172.31.21.0/24 10.4.249.231</span><br><span class="line">/opt/share                          172.31.21.0/24 10.4.249.231</span><br></pre></td></tr></table></figure></p><p>在 CMD 上创建 /opt/share 和 /opt/data 目录并 mount</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> mkdir -p /opt/share &amp;&amp; sudo chmod 777 /opt/share</span><br><span class="line"><span class="meta">#</span> mkdir -p /opt/data &amp;&amp; sudo chmod 777 /opt/data</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> sudo mount -t nfs 172.31.21.208:/opt/share /opt/share</span><br><span class="line"><span class="meta">#</span> sudo mount -t nfs 172.31.21.208:/data /opt/data</span><br></pre></td></tr></table></figure><h3 id="下载源码和-Fabric-脚本"><a href="#下载源码和-Fabric-脚本" class="headerlink" title="下载源码和 Fabric 脚本"></a>下载源码和 Fabric 脚本</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> git clone https://github.com/batizhao/fabric-on-kubernetes.git</span><br><span class="line"><span class="meta">#</span> cd fabric-on-kubernetes</span><br><span class="line"><span class="meta">#</span> wget https://nexus.hyperledger.org/content/repositories/releases/org/hyperledger/fabric/hyperledger-fabric/darwin-amd64-1.2.0/hyperledger-fabric-darwin-amd64-1.2.0.tar.gz &amp;&amp; tar -zxvf hyperledger-fabric-darwin-amd64-1.2.0.tar.gz &amp;&amp; rm -rf hyperledger-fabric-darwin-amd64-1.2.0.tar.gz &amp;&amp; rm -rf config</span><br></pre></td></tr></table></figure><p>这段会在当前目录下生成一个 bin 目录，包含了运行 Fabric 的二进制脚本和证书、创世块生成工具。</p><p>在部署之前，还需要修改 cli 和 namespace 中的 nfs 地址。</p><h3 id="部署-1"><a href="#部署-1" class="headerlink" title="部署"></a>部署</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> ./generateALL.sh</span><br><span class="line">org1</span><br><span class="line">org2</span><br><span class="line">2018-09-12 14:01:38.669 CST [common/tools/configtxgen] main -&gt; WARN 001 Omitting the channel ID for configtxgen is deprecated.  Explicitly passing the channel ID will be required in the future, defaulting to 'testchainid'.</span><br><span class="line">2018-09-12 14:01:38.669 CST [common/tools/configtxgen] main -&gt; INFO 002 Loading configuration</span><br><span class="line">2018-09-12 14:01:38.717 CST [msp] getMspConfig -&gt; INFO 003 Loading NodeOUs</span><br><span class="line">2018-09-12 14:01:38.719 CST [msp] getMspConfig -&gt; INFO 004 Loading NodeOUs</span><br><span class="line">2018-09-12 14:01:38.719 CST [common/tools/configtxgen] doOutputBlock -&gt; INFO 005 Generating genesis block</span><br><span class="line">2018-09-12 14:01:38.720 CST [common/tools/configtxgen] doOutputBlock -&gt; INFO 006 Writing genesis block</span><br><span class="line">2018-09-12 14:01:38.788 CST [common/tools/configtxgen] main -&gt; INFO 001 Loading configuration</span><br><span class="line">2018-09-12 14:01:38.812 CST [common/tools/configtxgen] doOutputChannelCreateTx -&gt; INFO 002 Generating new channel configtx</span><br><span class="line">2018-09-12 14:01:38.814 CST [msp] getMspConfig -&gt; INFO 003 Loading NodeOUs</span><br><span class="line">2018-09-12 14:01:38.816 CST [msp] getMspConfig -&gt; INFO 004 Loading NodeOUs</span><br><span class="line">2018-09-12 14:01:38.817 CST [common/tools/configtxgen] doOutputChannelCreateTx -&gt; INFO 005 Writing new channel tx</span><br><span class="line">2018-09-12 14:01:38.858 CST [common/tools/configtxgen] main -&gt; INFO 001 Loading configuration</span><br><span class="line">2018-09-12 14:01:38.879 CST [common/tools/configtxgen] doOutputAnchorPeersUpdate -&gt; INFO 002 Generating anchor peer update</span><br><span class="line">2018-09-12 14:01:38.881 CST [common/tools/configtxgen] doOutputAnchorPeersUpdate -&gt; INFO 003 Writing anchor peer update</span><br><span class="line">2018-09-12 14:01:38.934 CST [common/tools/configtxgen] main -&gt; INFO 001 Loading configuration</span><br><span class="line">2018-09-12 14:01:38.958 CST [common/tools/configtxgen] doOutputAnchorPeersUpdate -&gt; INFO 002 Generating anchor peer update</span><br><span class="line">2018-09-12 14:01:38.958 CST [common/tools/configtxgen] doOutputAnchorPeersUpdate -&gt; INFO 003 Writing anchor peer update</span><br></pre></td></tr></table></figure><p>这段调用 configtxgen 生成各组织的证书，调用 python 生成 Kubernetes 的 yaml 文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> python3 transform/run.py</span><br><span class="line">namespace "orgorderer1" created</span><br><span class="line">persistentvolume "orgorderer1-pv" created</span><br><span class="line">persistentvolumeclaim "orgorderer1-pv" created</span><br><span class="line">deployment "orderer0-orgorderer1" created</span><br><span class="line">service "orderer0" created</span><br><span class="line">namespace "org2" created</span><br><span class="line">persistentvolume "org2-pv" created</span><br><span class="line">persistentvolumeclaim "org2-pv" created</span><br><span class="line">deployment "ca" created</span><br><span class="line">service "ca" created</span><br><span class="line">persistentvolume "org2-artifacts-pv" created</span><br><span class="line">persistentvolumeclaim "org2-artifacts-pv" created</span><br><span class="line">deployment "cli" created</span><br><span class="line">deployment "peer0-org2" created</span><br><span class="line">service "peer0" created</span><br><span class="line">deployment "peer1-org2" created</span><br><span class="line">service "peer1" created</span><br><span class="line">namespace "org1" created</span><br><span class="line">persistentvolume "org1-pv" created</span><br><span class="line">persistentvolumeclaim "org1-pv" created</span><br><span class="line">deployment "ca" created</span><br><span class="line">service "ca" created</span><br><span class="line">persistentvolume "org1-artifacts-pv" created</span><br><span class="line">persistentvolumeclaim "org1-artifacts-pv" created</span><br><span class="line">deployment "cli" created</span><br><span class="line">deployment "peer0-org1" created</span><br><span class="line">service "peer0" created</span><br><span class="line">deployment "peer1-org1" created</span><br><span class="line">service "peer1" created</span><br></pre></td></tr></table></figure><p>这段会通过 python 脚本调用 kubectl 在 Kubernetes 上创建 Fabric 集群。</p><h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> kubectl get pod --all-namespaces</span><br><span class="line">NAMESPACE     NAME                                   READY     STATUS    RESTARTS   AGE</span><br><span class="line">org1          ca-744f5bfdbb-nb5pj                    1/1       Running   0          4m</span><br><span class="line">org1          cli-59d46f884-p5grk                    1/1       Running   0          4m</span><br><span class="line">org1          peer0-org1-6f8bd58fc8-dfvxp            2/2       Running   1          4m</span><br><span class="line">org1          peer1-org1-554b6d8fb-kjrqj             2/2       Running   0          4m</span><br><span class="line">org2          ca-6bd89dbc8d-zdgnb                    1/1       Running   1          4m</span><br><span class="line">org2          cli-7798868bf9-htkk2                   1/1       Running   0          4m</span><br><span class="line">org2          peer0-org2-f9fb8b694-7qhm8             2/2       Running   0          4m</span><br><span class="line">org2          peer1-org2-7b9854f9fb-xmps4            2/2       Running   0          4m</span><br><span class="line">orgorderer1   orderer0-orgorderer1-df4769577-mtzmp   1/1       Running   0          4m</span><br></pre></td></tr></table></figure><p>几分钟以后，可以看到所有 Pod 都 Running 了。</p><h2 id="测试-Fabric-集群"><a href="#测试-Fabric-集群" class="headerlink" title="测试 Fabric 集群"></a>测试 Fabric 集群</h2><p>当所有 Pod Running 之后，可以进入开发、部署 chaincode 的阶段。</p><h3 id="进入-CLI-容器"><a href="#进入-CLI-容器" class="headerlink" title="进入 CLI 容器"></a>进入 CLI 容器</h3><h4 id="A-查找-org1-下边的容器"><a href="#A-查找-org1-下边的容器" class="headerlink" title="A. 查找 org1 下边的容器"></a>A. 查找 org1 下边的容器</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> kubectl get pod -n org1</span><br><span class="line">NAME                          READY     STATUS    RESTARTS   AGE</span><br><span class="line">ca-744f5bfdbb-nb5pj           1/1       Running   0          2d</span><br><span class="line">cli-59d46f884-p5grk           1/1       Running   0          2d</span><br><span class="line">peer0-org1-6f8bd58fc8-dfvxp   2/2       Running   1          2d</span><br><span class="line">peer1-org1-554b6d8fb-kjrqj    2/2       Running   0          2d</span><br></pre></td></tr></table></figure><p>登录 org1 cli 容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> kubectl exec -it cli-59d46f884-p5grk bash -n org1</span><br><span class="line">root@cli-59d46f884-p5grk:/opt/gopath/src/github.com/hyperledger/fabric/peer#</span><br></pre></td></tr></table></figure><p>创建channel </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> peer channel create -o orderer0.orgorderer1:7050 -c mychannel -f ./channel-artifacts/channel.tx </span><br><span class="line">...</span><br><span class="line">2018-08-13 11:00:22.265 UTC [cli/common] readBlock -&gt; INFO 05e Received block: 0</span><br></pre></td></tr></table></figure><p>拷贝 mychannel.block 到 channel-artifacts 目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> cp mychannel.block channel-artifacts</span><br></pre></td></tr></table></figure><p>加入 mychannel</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> peer channel join -b channel-artifacts/mychannel.block</span><br><span class="line">...</span><br><span class="line">2018-08-13 11:02:54.961 UTC [channelCmd] executeJoin -&gt; INFO 041 Successfully submitted proposal to join channel</span><br></pre></td></tr></table></figure><p>更新 anchor peer，每个 org 只需执行一次</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> peer channel update -o orderer0.orgorderer1:7050 -c mychannel -f./channel-artifacts/Org1MSPanchors.tx</span><br><span class="line">...</span><br><span class="line">2018-08-13 11:03:47.907 UTC [channelCmd] update -&gt; INFO 04d Successfully submitted channel update</span><br></pre></td></tr></table></figure><h4 id="B-查找-org2-下边的容器"><a href="#B-查找-org2-下边的容器" class="headerlink" title="B. 查找 org2 下边的容器"></a>B. 查找 org2 下边的容器</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> kubectl get pod -n org2</span><br><span class="line">NAME                          READY     STATUS    RESTARTS   </span><br><span class="line">ca-8665cf9b9b-v5m8p           1/1       Running   0          </span><br><span class="line">cli-7798868bf9-zmq8d          1/1       Running   1          </span><br><span class="line">peer0-org2-f9fb8b694-mw54z    2/2       Running   0          </span><br><span class="line">peer1-org2-7b9854f9fb-t5plg   2/2       Running   0</span><br></pre></td></tr></table></figure><p>登录 org2 cli 容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> kubectl exec -it cli-7798868bf9-zmq8d bash -n org2</span><br><span class="line">root@cli-7798868bf9-zmq8d:/opt/gopath/src/github.com/hyperledger/fabric/peer#</span><br></pre></td></tr></table></figure><p>加入 mychannel</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> peer channel join -b channel-artifacts/mychannel.block</span><br><span class="line">...</span><br><span class="line">2018-08-13 11:02:54.961 UTC [channelCmd] executeJoin -&gt; INFO 041 Successfully submitted proposal to join channel</span><br></pre></td></tr></table></figure><p>更新 anchor peer，每个 org 只需执行一次</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> peer channel update -o orderer0.orgorderer1:7050 -c mychannel -f./channel-artifacts/Org2MSPanchors.tx</span><br><span class="line">...</span><br><span class="line">2018-08-13 11:06:48.166 UTC [channelCmd] update -&gt; INFO 04d Successfully submitted channel update</span><br></pre></td></tr></table></figure><h3 id="安装-chaincode"><a href="#安装-chaincode" class="headerlink" title="安装 chaincode"></a>安装 chaincode</h3><p>分别登录 org1 cli ，org2 cli 容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> kubectl exec -it cli-59d46f884-p5grk bash -n org1</span><br><span class="line">root@cli-59d46f884-p5grk:/opt/gopath/src/github.com/hyperledger/fabric/peer#</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> kubectl exec -it cli-7798868bf9-zmq8d bash -n org2</span><br><span class="line">root@cli-7798868bf9-zmq8d:/opt/gopath/src/github.com/hyperledger/fabric/peer#</span><br></pre></td></tr></table></figure><p>安装 chaincode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> peer chaincode install -n mycc -v 1.0 -p github.com/hyperledger/fabric/peer/channel-artifacts/chaincode</span><br><span class="line">...</span><br><span class="line">2018-08-13 13:29:05.614 UTC [chaincodeCmd] install -&gt; INFO 050 Installed remotely response:&lt;status:200 payload:"OK" &gt;</span><br></pre></td></tr></table></figure><p>实例化 chaincode（只要在任意 org 执行一次）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> peer chaincode instantiate -o orderer0.orgorderer1:7050 \</span><br><span class="line">                             -C mychannel -n mycc -v 1.0 \</span><br><span class="line">                             -c '&#123;"Args":["init","a", "100", "b","200"]&#125;' \</span><br><span class="line">                             -P "OR ('Org1MSP.peer','Org2MSP.peer')"</span><br><span class="line">...                             </span><br><span class="line">2018-08-13 13:39:28.069 UTC [chaincodeCmd] checkChaincodeCmdParams -&gt; INFO 04a Using default escc</span><br><span class="line">2018-08-13 13:39:28.069 UTC [chaincodeCmd] checkChaincodeCmdParams -&gt; INFO 04b Using default vscc</span><br><span class="line">2018-08-13 13:39:28.069 UTC [chaincodeCmd] getChaincodeSpec -&gt; DEBU 04c java chaincode disabled</span><br><span class="line">2018-08-13 13:39:28.069 UTC [msp/identity] Sign -&gt; DEBU 04d Sign: plaintext: 0A8D070A6608031A0B089094C6DB0510...535010030A04657363630A0476736363</span><br><span class="line">2018-08-13 13:39:28.069 UTC [msp/identity] Sign -&gt; DEBU 04e Sign: digest: F5EB7F5FEB4C3CE151402B3A43E285BA2FD7B0FA6FBC355376417407BC9CAC27</span><br><span class="line">2018-08-13 13:39:34.045 UTC [msp/identity] Sign -&gt; DEBU 04f Sign: plaintext: 0A8D070A6608031A0B089094C6DB0510...394757C502AA08930A47A8BF4BA9263D</span><br><span class="line">2018-08-13 13:39:34.045 UTC [msp/identity] Sign -&gt; DEBU 050 Sign: digest: F55A876EC071C16D5FE5CEB6A155F075B65A9EBAEFD26B5D916D10921A25D6A8</span><br></pre></td></tr></table></figure><p>查询账本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> peer chaincode query -C mychannel -n mycc -c '&#123;"Args":["query","a"]&#125;'</span><br><span class="line">100</span><br></pre></td></tr></table></figure><p>a to b 转帐 10</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> peer chaincode invoke -o orderer0.orgorderer1:7050\</span><br><span class="line">                        -C mychannel -n mycc \</span><br><span class="line">                        --peerAddresses peer0.org1:7051 \</span><br><span class="line">                        -c '&#123;"Args":["invoke","a","b","10"]&#125;'</span><br><span class="line">...</span><br><span class="line">2018-08-13 13:54:46.298 UTC [chaincodeCmd] chaincodeInvokeOrQuery -&gt; INFO 050 Chaincode invoke successful. result: status:200</span><br></pre></td></tr></table></figure><p>查询账本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> peer chaincode query -C mychannel -n mycc -c '&#123;"Args":["query","a"]&#125;'</span><br><span class="line">90</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> peer chaincode query -C mychannel -n mycc -c '&#123;"Args":["query","b"]&#125;'</span><br><span class="line">210</span><br></pre></td></tr></table></figure><h3 id="Chaincode-Java-Client"><a href="#Chaincode-Java-Client" class="headerlink" title="Chaincode Java Client"></a>Chaincode Java Client</h3><p><a href="https://github.com/batizhao/fabric-java-client" target="_blank" rel="noopener">这里</a> 用 Java 实现了一个 Chaincode API 调用，效果同上边的命令行。</p><h2 id="清除集群"><a href="#清除集群" class="headerlink" title="清除集群"></a>清除集群</h2><p>当需要删除集群的时候，可以通过 transform 目录下的 delete.py 脚本来清理环境，该脚本会遍历 crypto-config 目录，找出所有的 yaml 文件，并通过 kuberclt delete -f xxx.yaml 的方式将资源逐个删除。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> python3 transform/delete.py</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文介绍的部署方法，是基于 Kubernetes 容器云平台实现 BaaS 的基础步骤。先介绍了 Fabric 的架构、详细部署过程，之后介绍了 chaincode 的部署和调用。在此之上，可以增加更多的区块链层管理功能，图形化运维界面，使得开发人员投入更多的精力到应用的业务逻辑上。</p><p>在此之前，有试用过 <a href="https://github.com/hyperledger/cello" target="_blank" rel="noopener">Heperledger Cello</a>，发现这个项目还没法用。在网上找到 <a href="https://medium.com/@zhanghenry/how-to-deploy-hyperledger-fabric-on-kubernetes-2-751abf44c807" target="_blank" rel="noopener">How to Deploy Hyperledger Fabric on Kubernetes</a> 这篇博文，和国内这篇 <a href="https://blog.csdn.net/Blockchain_lemon/article/details/77744173" target="_blank" rel="noopener">用Kubernetes部署超级账本Fabric的区块链即服务</a> 做参考，应该是同一个人写的。只不过写于 2017 年 Fabric v1.0 的时候，现在已经是 v1.2，所以对项目做了些修改才能运行。这应该也是目前唯一的 Fabric v1.2 on Kubernetes 部署指引。后续会通过实现一些 chaincode 来更深入的理解 Fabric 原理。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在上一篇文章中，我们介绍了在 Kubernetes 上运行 Fabric 的机制和架构。 这篇文章会详细讲解安装的具体步骤。&lt;/p&gt;
&lt;h2 id=&quot;准备环境&quot;&gt;&lt;a href=&quot;#准备环境&quot; class=&quot;headerlink&quot; title=&quot;准备环境&quot;&gt;&lt;/a&gt;准备环境&lt;/h2&gt;&lt;h3 id=&quot;CMD-客户机&quot;&gt;&lt;a href=&quot;#CMD-客户机&quot; class=&quot;headerlink&quot; title=&quot;CMD 客户机&quot;&gt;&lt;/a&gt;CMD 客户机&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;自己的 Mac（10.4.249.231）&lt;/li&gt;
&lt;li&gt;可以通过 kubectl 操作远程的 Kubernetes 集群&lt;/li&gt;
&lt;li&gt;安装 Python3，部署脚本是用 Python 写的&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;集群环境&quot;&gt;&lt;a href=&quot;#集群环境&quot; class=&quot;headerlink&quot; title=&quot;集群环境&quot;&gt;&lt;/a&gt;集群环境&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;CentOS7&lt;/li&gt;
&lt;li&gt;Kubernetes v1.11.0&lt;/li&gt;
&lt;li&gt;Docker 18.03.1-ce&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;NFS&quot;&gt;&lt;a href=&quot;#NFS&quot; class=&quot;headerlink&quot; title=&quot;NFS&quot;&gt;&lt;/a&gt;NFS&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;给集群做共享存储，挂载证书和一些 channel 文件。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="blockchain" scheme="http://batizhao.github.io/categories/blockchain/"/>
    
    
      <category term="kubernetes" scheme="http://batizhao.github.io/tags/kubernetes/"/>
    
      <category term="hyperledger" scheme="http://batizhao.github.io/tags/hyperledger/"/>
    
  </entry>
  
  <entry>
    <title>在 Kubernetes 上部署 Hyperledger Fabric v1.2（一）</title>
    <link href="http://batizhao.github.io/2018/08/10/deploy-fabric-on-kubernetes/"/>
    <id>http://batizhao.github.io/2018/08/10/deploy-fabric-on-kubernetes/</id>
    <published>2018-08-10T03:03:24.000Z</published>
    <updated>2018-08-14T01:57:28.615Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Fabric 是 Hyperledger 超级账本中的一个子项目，由 Linux 基金会主办。它提供了一个开发区块链应用程序的框架。在 2017 年 1 月份 Fabric v1.0 发布，人们急于使用 Fabric 构建区块链应用程序来解决他们的业务问题。然而，由于部署和管理 Fabric 体系过于复杂，遇到很多的困难。在 v1.0 发布之后，时隔一年，在今年 7 月份，Fabric v1.2 版本发布。</p><p>为了简化操作，我们需要一些工具来帮助我们更好地管理 Fabric 分布式系统。Kubernetes 看起来似乎是理想的平台。需要注意的是，Kubernetes 是 CNCF 基金会下的头牌项目，并且 Linux 基金会也是 CNCF 基金会的成员之一。</p><p>首先，Fabric 建议是运行在 Docker 容器中的。它的 chaincode（智能合约）也利用容器运行在 sandbox 中。 Fabric 系统由在多个容器中运行的组件组成。 另一方面，Kubernetes 正在成为自动化、容器化应用程序的部署、扩展和管理的事实上的标准。两者有天然的契合。</p><p>其次，Fabric 组件可以通过在 Kubernetes 上部署来实现高可用性。 Kubernetes 有一个名为 replicator 的功能，可以监控运行的 pod 并自动修复崩溃的 pod。</p><p>第三，Kubernetes 支持多租户。我们可以在同一个 Kubernetes 平台上运行多个隔离的 Fabric 实例。 这有利于区块链应用程序的开发和测试。</p><p>在以下部分中，我们介绍了在 Kubernetes 上部署 Fabric 的方法。 我们假设读者具有 Fabric，Docker 容器和Kubernetes 的基本知识。</p><a id="more"></a><h2 id="网络拓扑"><a href="#网络拓扑" class="headerlink" title="网络拓扑"></a>网络拓扑</h2><p><img src="/images/2018-08-10-deploy-fabric-on-kubernetes-1.png" alt="图1"></p><p>我们的网络拓扑结构如 图1 所示。物理网络由蓝线表示。 Kubernetes 有一个或多个主节点和工作节点。除此之外，我们还有一台 CMD 机器作为客户端来发布部署命令。 NFS 服务器用作配置文件和其他数据的共享文件系统。所有这些节点都通过物理网络（例如192.168.0.1/24）连接。</p><p>Kubernetes 的网络模型使所有 pod 都可以直接相互连接，无论它们在哪个节点上。通过使用 Kubernetes 的 CNI 插件，例如 Flannel，可以很容易地为此目的创建覆盖网络。如 图1 中的红线所示（Flannel 组件的一些细节被省略），Kubernetes 将所有 Pods 连接到 Flannel 网络，允许这些 Pods 的容器正确地相互通信。</p><p>可以在附加配置文件中指定 Flannel 网络的 IP 地址范围以及 kube_dns 的 IP 地址。我们需要确保 kube_dns 的 IP 地址必须在指定的地址范围内。例如，在 图1 中，Flannel 网络是 10.0.0.1/16，kube_dns 地址是 10.0.0.10。</p><h2 id="Fabric-组件和-Pods-映射关系"><a href="#Fabric-组件和-Pods-映射关系" class="headerlink" title="Fabric 组件和 Pods 映射关系"></a>Fabric 组件和 Pods 映射关系</h2><p><img src="/images/2018-08-10-deploy-fabric-on-kubernetes-2.png" alt="图2"></p><p>Fabric 是一个包含多个节点的分布式系统。 节点可以属于不同的组织。 如 图2 所示，每个组织都有自己的 Peers 节点集（为简单起见，并未显示所有节点）。 Orderers 还组建了一个公共共识服务。 要将 Fabric 部署到 Kubernetes，我们需要将所有组件转换为 Pod 以进行部署，并使用命名空间来隔离组织。</p><p>在 Kubernetes 中，命名空间是一个重要的概念。 它用于在多个用户之间划分群集资源。 在 Fabric 中，可以将组织映射到名称空间，以便它们具有其专用资源。 在此映射之后，可以通过域名区分每个组织的 Peers。 此外，我们可以通过设置网络策略来隔离不同的组织。</p><p>如 图2 所示，假设 Fabric 网络中有 N 个 Peer 组织和 M 个 Order 组织。 以下是我们如何在Kubernetes 上划分它们：</p><h3 id="组织"><a href="#组织" class="headerlink" title="组织"></a>组织</h3><p><img src="/images/2018-08-10-deploy-fabric-on-kubernetes-3.png" alt="图3"></p><p>我们为第 N 个对等组织分配名称 orgN。 它在 Kubernetes 中的相应命名空间也称为 orgN。 Fabric orgN 的所有组件都将放入 Kubernetes 的命名空间 orgN 中。 每个组织的命名空间下都有多个 Pod。 Pod 是 Kubernetes 中的部署单元，它由一个或多个容器组成。 我们可以将每个组织的 Fabric 容器捆绑到几个 Pod 中。 这些 Pod 类型如下：</p><ul><li><strong>Peer Pod</strong>：包含 Fabric peer、couchDB （可选，默认是 levelDB）、代表组织的 peer 节点。 每个组织可以有一个或多个 Peer Pods。</li><li><strong>CA Server Pod</strong>：组织的 Fabric CA Server 节点。 通常，一个组织中需要一个 CA Pod。</li><li><strong>CLI Pod</strong>：为命令行工具提供操作组织节点的环境，Fabric 的 Peer 环境变量在此 Pod 中配置（可选）。</li></ul><h3 id="共识排序"><a href="#共识排序" class="headerlink" title="共识排序"></a>共识排序</h3><p><img src="/images/2018-08-10-deploy-fabric-on-kubernetes-4.png" alt="图4"></p><p>Fabric 中可能有一个或多个 Orderers。 我们将第 M 个订购者组织的名称设置为 orgordererM。 它在 Kubernetes 上的相应命名空间是 orgordererM。 它有一个或多个 Pod 来运行 orderer 节点。</p><h3 id="整体拓扑"><a href="#整体拓扑" class="headerlink" title="整体拓扑"></a>整体拓扑</h3><p>如果 Kafka 用于共识过程，我们可以将 Kafka 放入单独的命名空间。 它仅用于运行和管理 Zookeeper 和 Kafka 容器。</p><p>总而言之，整体部署如下所示：</p><p><img src="/images/2018-08-10-deploy-fabric-on-kubernetes-5.png" alt="图5"></p><h2 id="共享存储"><a href="#共享存储" class="headerlink" title="共享存储"></a>共享存储</h2><p>在部署 Fabric 之前，我们需要准备其组件的配置文件，例如 Peer 和 Orderer。这是一个非常复杂的过程，往往容易出错。幸运的是，我们创建了一个工具来自动生成这些配置文件。生成的文件存储在 NFS 等共享文件系统中。</p><p>当我们稍后启动 Fabric 的 Pod 时，我们将不同的配置文件子集安装到 Pod 中，以便它们具有特定于其所属组织的配置。</p><p>在 Kubernetes 中，我们可以使用持久卷（PV）和持久卷声明（PVC）将文件或目录挂载到 Pod 中。我们为 Fabric 中的每个组织创建 PV 和 PVC，以实现资源隔离。每个组织只应在 NFS 服务器中看到自己的目录。</p><p>在创建 PV 之后，我们定义 PVC，以便 Fabric 节点可以使用 PV 来访问相应的目录和文件。</p><p>以对等组织 org1 为例。首先，我们创建一个命名空间 org1 及其 PV。 PV 映射到 NFS 上的目录 / opt/share/crypto-config/peerOrganizations/org1。其次，我们创建一个 PVC 来消耗 PV。命名空间 org1 下的所有 pod 使用相同的 PVC。但是，我们只通过在 pod 配置文件中指定安装路径，将必要的文件映射到每个 pod 中。</p><p>图6 显示了 Pod 与 NFS 共享目录之间的关系。变量 $PVC 表示 PVC 挂载点，在此示例中为 /opt/share/crypto-config/peerOrganizations/org1。</p><p><img src="/images/2018-08-10-deploy-fabric-on-kubernetes-6.png" alt="图6"></p><h2 id="通信"><a href="#通信" class="headerlink" title="通信"></a>通信</h2><p>当所有 Fabric 的组件都放入 Kubernetes 的 Pod 中时，我们需要考虑这些 Pod 之间的网络连接。 Kubernetes 中的每个 Pod 都有一个内部 IP 地址，但是很难使用 IP 和端口在 Pod 之间进行通信，因为 IP 地址对于 Pod 来说是短暂的。 当 Pod 重新启动时，其 IP 地址也会发生变化。 因此，有必要在 Kubernetes 中为 Pod 创建服务，以便它们可以通过服务名称相互通信。 服务的命名应遵循以下原则来显示它所绑定的 Pod 信息：</p><ul><li>服务和 Pod 的名称空间应该是一致的。</li><li>服务名称应与 Pod 中容器的 ID 一致。</li></ul><p>例如，Fabric 的组织 org1 的 peer0 映射到命名空间 org1 下名为 peer0 的 Pod。 绑定到它的服务应该命名为peer0.org1，其中 peer0 是服务的名称，org1 是服务的名称空间。 其他 Pod 可以通过服务名称 peer0.org1 连接到 org1 的 peer0，该名称显示为 peer0 的主机名。</p><p>具体的部署过程请看第二部分。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://hackernoon.com/how-to-deploy-hyperledger-fabric-on-kubernetes-1-a2ceb3ada078" target="_blank" rel="noopener">How to Deploy Hyperledger Fabric on Kubernetes (1)</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;Fabric 是 Hyperledger 超级账本中的一个子项目，由 Linux 基金会主办。它提供了一个开发区块链应用程序的框架。在 2017 年 1 月份 Fabric v1.0 发布，人们急于使用 Fabric 构建区块链应用程序来解决他们的业务问题。然而，由于部署和管理 Fabric 体系过于复杂，遇到很多的困难。在 v1.0 发布之后，时隔一年，在今年 7 月份，Fabric v1.2 版本发布。&lt;/p&gt;
&lt;p&gt;为了简化操作，我们需要一些工具来帮助我们更好地管理 Fabric 分布式系统。Kubernetes 看起来似乎是理想的平台。需要注意的是，Kubernetes 是 CNCF 基金会下的头牌项目，并且 Linux 基金会也是 CNCF 基金会的成员之一。&lt;/p&gt;
&lt;p&gt;首先，Fabric 建议是运行在 Docker 容器中的。它的 chaincode（智能合约）也利用容器运行在 sandbox 中。 Fabric 系统由在多个容器中运行的组件组成。 另一方面，Kubernetes 正在成为自动化、容器化应用程序的部署、扩展和管理的事实上的标准。两者有天然的契合。&lt;/p&gt;
&lt;p&gt;其次，Fabric 组件可以通过在 Kubernetes 上部署来实现高可用性。 Kubernetes 有一个名为 replicator 的功能，可以监控运行的 pod 并自动修复崩溃的 pod。&lt;/p&gt;
&lt;p&gt;第三，Kubernetes 支持多租户。我们可以在同一个 Kubernetes 平台上运行多个隔离的 Fabric 实例。 这有利于区块链应用程序的开发和测试。&lt;/p&gt;
&lt;p&gt;在以下部分中，我们介绍了在 Kubernetes 上部署 Fabric 的方法。 我们假设读者具有 Fabric，Docker 容器和Kubernetes 的基本知识。&lt;/p&gt;
    
    </summary>
    
      <category term="blockchain" scheme="http://batizhao.github.io/categories/blockchain/"/>
    
    
      <category term="kubernetes" scheme="http://batizhao.github.io/tags/kubernetes/"/>
    
      <category term="hyperledger" scheme="http://batizhao.github.io/tags/hyperledger/"/>
    
  </entry>
  
  <entry>
    <title>在 Mac 上设置 kubectl 工具</title>
    <link href="http://batizhao.github.io/2018/03/12/setup-kubectl-config/"/>
    <id>http://batizhao.github.io/2018/03/12/setup-kubectl-config/</id>
    <published>2018-03-12T05:23:33.000Z</published>
    <updated>2018-03-12T05:35:58.248Z</updated>
    
    <content type="html"><![CDATA[<p>kubectl 是 k8s 集群的命令行工具，集群安装好以后，可以在集群外部操作相关命令。</p><p>最简单的情况，在 Mac 上安装好 kubectl 工具后，可以直接 scp 集群中的 config 文件到本地，这样就有了访问和管理 k8s 集群的能力。但是，如果你有好几个集群同时需要管理，这种方式可能不是最好。</p><p>可能通过以下方式优雅的加入相关配置到 config 中。</p><h2 id="设置-cluster"><a href="#设置-cluster" class="headerlink" title="设置 cluster"></a>设置 cluster</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> kubectl config set-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/etc/kubernetes/ssl/ca.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=https://172.31.21.173:8443</span><br></pre></td></tr></table></figure><p>如果遇到 x509 的错误，可以使用以下命令代替<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> kubectl config set-cluster kubernetes \</span><br><span class="line">        --insecure-skip-tls-verify=true \</span><br><span class="line">        --server=https://172.31.21.173:8443</span><br></pre></td></tr></table></figure></p><h2 id="设置-credentials"><a href="#设置-credentials" class="headerlink" title="设置 credentials"></a>设置 credentials</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> kubectl config set-credentials admin \</span><br><span class="line">        --client-certificate=/etc/kubernetes/ssl/admin.pem \</span><br><span class="line">        --embed-certs=true \</span><br><span class="line">        --client-key=/etc/kubernetes/ssl/admin-key.pem</span><br></pre></td></tr></table></figure><h2 id="设置-context"><a href="#设置-context" class="headerlink" title="设置 context"></a>设置 context</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> kubectl config set-context kubernetes \</span><br><span class="line">        --cluster=kubernetes --user=admin</span><br></pre></td></tr></table></figure><h2 id="设置默认-context"><a href="#设置默认-context" class="headerlink" title="设置默认 context"></a>设置默认 context</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> kubectl config use-context kubernetes</span><br></pre></td></tr></table></figure><p>生成的内容会增加到 ~/.kube/config 中。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;kubectl 是 k8s 集群的命令行工具，集群安装好以后，可以在集群外部操作相关命令。&lt;/p&gt;
&lt;p&gt;最简单的情况，在 Mac 上安装好 kubectl 工具后，可以直接 scp 集群中的 config 文件到本地，这样就有了访问和管理 k8s 集群的能力。但是，如果你
      
    
    </summary>
    
      <category term="docker" scheme="http://batizhao.github.io/categories/docker/"/>
    
    
      <category term="linux" scheme="http://batizhao.github.io/tags/linux/"/>
    
      <category term="java" scheme="http://batizhao.github.io/tags/java/"/>
    
      <category term="kubernetes" scheme="http://batizhao.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>在 Kubernetes 环境中 debug Java 程序</title>
    <link href="http://batizhao.github.io/2018/03/02/debug-in-k8s/"/>
    <id>http://batizhao.github.io/2018/03/02/debug-in-k8s/</id>
    <published>2018-03-02T08:19:23.000Z</published>
    <updated>2018-03-02T08:34:11.299Z</updated>
    
    <content type="html"><![CDATA[<h2 id="服务端"><a href="#服务端" class="headerlink" title="服务端"></a>服务端</h2><p>Docker 镜像启动命令</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CMD</span> ["java", "-Dfile.encoding=utf-8", "-jar", "-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005", "/opt/app.jar"]</span><br></pre></td></tr></table></figure><p>K8s pod</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">oss</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        titans:</span> <span class="string">oss</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">titans-oss</span></span><br><span class="line"><span class="attr">        image:</span> <span class="number">172.31</span><span class="number">.21</span><span class="number">.226</span><span class="string">/isoftone/titans-oss</span></span><br><span class="line"><span class="attr">        imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">8008</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">"web"</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">5005</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">"jvm-debug"</span></span><br></pre></td></tr></table></figure><a id="more"></a><p>启动以后查看 pod</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -o wide</span><br><span class="line">NAME                       READY     STATUS    RESTARTS   AGE       IP             NODE</span><br><span class="line">oss-d74785795-fb2h2        1/1       Running   0          15m       10.244.1.112   k8s-node-1</span><br></pre></td></tr></table></figure><h2 id="本地"><a href="#本地" class="headerlink" title="本地"></a>本地</h2><p>连接<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> kubectl port-forward oss-d74785795-fb2h2 5005:5005</span><br><span class="line">Forwarding from 127.0.0.1:5005 -&gt; 5005</span><br></pre></td></tr></table></figure></p><p>打开 IDEA，确保以下参数，然后 debug</p><p><img src="/images/2018-03-02-debug-in-k8s.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;服务端&quot;&gt;&lt;a href=&quot;#服务端&quot; class=&quot;headerlink&quot; title=&quot;服务端&quot;&gt;&lt;/a&gt;服务端&lt;/h2&gt;&lt;p&gt;Docker 镜像启动命令&lt;/p&gt;
&lt;figure class=&quot;highlight dockerfile&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;CMD&lt;/span&gt; [&quot;java&quot;, &quot;-Dfile.encoding=utf-8&quot;, &quot;-jar&quot;, &quot;-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005&quot;, &quot;/opt/app.jar&quot;]&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;K8s pod&lt;/p&gt;
&lt;figure class=&quot;highlight yaml&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;apiVersion:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;apps/v1beta1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;kind:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;Deployment&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;metadata:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;  name:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;oss&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;spec:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;  replicas:&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;  template:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;    metadata:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;      labels:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;        titans:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;oss&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;    spec:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;      containers:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;      - name:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;titans-oss&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;        image:&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;172.31&lt;/span&gt;&lt;span class=&quot;number&quot;&gt;.21&lt;/span&gt;&lt;span class=&quot;number&quot;&gt;.226&lt;/span&gt;&lt;span class=&quot;string&quot;&gt;/isoftone/titans-oss&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;        imagePullPolicy:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;Always&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;        ports:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;        - containerPort:&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;8008&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;          name:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;web&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;        - containerPort:&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;5005&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;attr&quot;&gt;          name:&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;jvm-debug&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="docker" scheme="http://batizhao.github.io/categories/docker/"/>
    
    
      <category term="linux" scheme="http://batizhao.github.io/tags/linux/"/>
    
      <category term="java" scheme="http://batizhao.github.io/tags/java/"/>
    
      <category term="kubernetes" scheme="http://batizhao.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>使用 Gluster FS 做 Kubernetes 持久化存储</title>
    <link href="http://batizhao.github.io/2018/02/08/installing-glusterfs/"/>
    <id>http://batizhao.github.io/2018/02/08/installing-glusterfs/</id>
    <published>2018-02-08T06:28:41.000Z</published>
    <updated>2018-03-09T07:19:07.410Z</updated>
    
    <content type="html"><![CDATA[<h2 id="安装-Gluster-FS"><a href="#安装-Gluster-FS" class="headerlink" title="安装 Gluster FS"></a>安装 Gluster FS</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 先安装 gluster 源</span><br><span class="line"><span class="meta">$</span> yum install centos-release-gluster -y</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 安装 glusterfs 组件</span><br><span class="line"><span class="meta">$</span> yum install -y glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma glusterfs-geo-replication glusterfs-devel</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 创建 glusterfs 目录</span><br><span class="line"><span class="meta">$</span> mkdir /opt/glusterd</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 修改 glusterd 目录</span><br><span class="line"><span class="meta">$</span> sed -i 's/var\/lib/opt/g' /etc/glusterfs/glusterd.vol</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 启动 glusterfs</span><br><span class="line"><span class="meta">$</span> systemctl start glusterd.service</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 设置开机启动</span><br><span class="line"><span class="meta">$</span> systemctl enable glusterd.service</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>查看状态</span><br><span class="line"><span class="meta">$</span> systemctl status glusterd.service</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="配置-Gluster-FS"><a href="#配置-Gluster-FS" class="headerlink" title="配置 Gluster FS"></a>配置 Gluster FS</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 配置 hosts</span><br><span class="line"><span class="meta">$</span> vi /etc/hosts</span><br><span class="line">172.31.21.208   gfs01.isoftone.com </span><br><span class="line">172.31.21.209   gfs02.isoftone.com </span><br><span class="line">172.31.21.210   gfs03.isoftone.com</span><br><span class="line">172.31.21.211   gfs04.isoftone.com</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 开放端口</span><br><span class="line"><span class="meta">$</span> iptables -I INPUT -p tcp --dport 24007 -j ACCEPT</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 创建存储目录</span><br><span class="line"><span class="meta">$</span> mkdir /opt/gfs_data</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 添加节点到 集群</span><br><span class="line"><span class="meta">#</span> 执行操作的本机不需要probe 本机</span><br><span class="line">[root@sz-pg-oam-docker-test-001 ~]#</span><br><span class="line">gluster peer probe gfs02.isoftone.com</span><br><span class="line">gluster peer probe gfs03.isoftone.com</span><br><span class="line">gluster peer probe gfs04.isoftone.com</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 查看集群状态</span><br><span class="line"><span class="meta">$</span> gluster peer status</span><br><span class="line">Number of Peers: 3</span><br><span class="line"></span><br><span class="line">Hostname: gfs02.isoftone.com</span><br><span class="line">Uuid: 0697cfd7-4c88-4d29-b907-59894b158df7</span><br><span class="line">State: Peer in Cluster (Connected)</span><br><span class="line"></span><br><span class="line">Hostname: gfs03.isoftone.com</span><br><span class="line">Uuid: 7754d301-00ac-4c28-ac22-4d5f89d2d1d3</span><br><span class="line">State: Peer in Cluster (Connected)</span><br><span class="line"></span><br><span class="line">Hostname: gfs04.isoftone.com</span><br><span class="line">Uuid: 7b33777c-0d3b-45f9-acbd-4f8d09da6431</span><br><span class="line">State: Peer in Cluster (Connected)</span><br></pre></td></tr></table></figure><h2 id="配置-Gluster-FS-Volume"><a href="#配置-Gluster-FS-Volume" class="headerlink" title="配置 Gluster FS Volume"></a>配置 Gluster FS Volume</h2><p>GlusterFS中的volume的模式有很多中，包括以下几种：</p><ul><li><strong>分布卷（默认模式）</strong>：即DHT, 也叫 分布卷: 将文件已hash算法随机分布到 一台服务器节点中存储。</li><li><strong>复制模式</strong>：即AFR, 创建volume 时带 replica x 数量: 将文件复制到 replica x 个节点中。</li><li><strong>条带模式</strong>：即Striped, 创建volume 时带 stripe x 数量： 将文件切割成数据块，分别存储到 stripe x 个节点中 ( 类似raid 0 )。</li><li><strong>分布式条带模式</strong>：最少需要4台服务器才能创建。 创建volume 时 stripe 2 server = 4 个节点： 是DHT 与 Striped 的组合型。</li><li><strong>分布式复制模式</strong>：最少需要4台服务器才能创建。 创建volume 时 replica 2 server = 4 个节点：是DHT 与 AFR 的组合型。</li><li><strong>条带复制卷模式</strong>：最少需要4台服务器才能创建。 创建volume 时 stripe 2 replica 2 server = 4 个节点： 是 Striped 与 AFR 的组合型。</li><li><strong>三种模式混合</strong>： 至少需要8台 服务器才能创建。 stripe 2 replica 2 , 每4个节点 组成一个 组。</li></ul><p>这几种模式的示例图参考：<a href="http://www.cnblogs.com/jicki/p/5801712.html" target="_blank" rel="noopener">CentOS7安装GlusterFS</a>。</p><p>因为我们只有四台主机，在此我们使用默认的<strong>分布式复制模式</strong>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 创建分布卷</span><br><span class="line"><span class="meta">$</span> gluster volume create k8s-volume replica 2 transport tcp gfs01.isoftone.com:/opt/gfs_data gfs02.isoftone.com:/opt/gfs_data gfs03.isoftone.com:/opt/gfs_data gfs04.isoftone.com:/opt/gfs_data force</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 查看volume状态</span><br><span class="line"><span class="meta">$</span> gluster volume info</span><br><span class="line">Volume Name: k8s-volume</span><br><span class="line">Type: Distributed-Replicate</span><br><span class="line">Volume ID: f5438a2b-22b3-4608-9a45-ed9fa0028d38</span><br><span class="line">Status: Created</span><br><span class="line">Snapshot Count: 0</span><br><span class="line">Number of Bricks: 2 x 2 = 4</span><br><span class="line">Transport-type: tcp</span><br><span class="line">Bricks:</span><br><span class="line">Brick1: gfs01.isoftone.com:/opt/gfs_data</span><br><span class="line">Brick2: gfs02.isoftone.com:/opt/gfs_data</span><br><span class="line">Brick3: gfs03.isoftone.com:/opt/gfs_data</span><br><span class="line">Brick4: gfs04.isoftone.com:/opt/gfs_data</span><br><span class="line">Options Reconfigured:</span><br><span class="line">transport.address-family: inet</span><br><span class="line">nfs.disable: on</span><br><span class="line">performance.client-io-threads: off</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 启动 k8s-volume 卷</span><br><span class="line"><span class="meta">$</span> gluster volume start k8s-volume</span><br></pre></td></tr></table></figure><h2 id="调优-Gluster-FS"><a href="#调优-Gluster-FS" class="headerlink" title="调优 Gluster FS"></a>调优 Gluster FS</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 开启 指定 volume 的配额</span><br><span class="line"><span class="meta">$</span> gluster volume quota k8s-volume enable</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 限制 指定 volume 的配额</span><br><span class="line"><span class="meta">$</span> gluster volume quota k8s-volume limit-usage / 1TB</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 设置 cache 大小, 默认32MB</span><br><span class="line"><span class="meta">$</span> gluster volume set k8s-volume performance.cache-size 4GB</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 设置 io 线程, 太大会导致进程崩溃</span><br><span class="line"><span class="meta">$</span> gluster volume set k8s-volume performance.io-thread-count 16</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 设置 网络检测时间, 默认42s</span><br><span class="line"><span class="meta">$</span> gluster volume set k8s-volume network.ping-timeout 10</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 设置 写缓冲区的大小, 默认1M</span><br><span class="line"><span class="meta">$</span> gluster volume set k8s-volume performance.write-behind-window-size 1024MB</span><br></pre></td></tr></table></figure><h2 id="在-Kubernetes-使用-Gluster-FS"><a href="#在-Kubernetes-使用-Gluster-FS" class="headerlink" title="在 Kubernetes 使用 Gluster FS"></a>在 Kubernetes 使用 Gluster FS</h2><h3 id="安装-Gluster-FS-客户端"><a href="#安装-Gluster-FS-客户端" class="headerlink" title="安装 Gluster FS 客户端"></a>安装 Gluster FS 客户端</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 在所有 k8s node 中安装 glusterfs 客户端</span><br><span class="line"><span class="meta">$</span> yum install -y glusterfs glusterfs-fuse</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 配置 hosts</span><br><span class="line"><span class="meta">$</span> vi /etc/hosts</span><br><span class="line">172.31.21.208   gfs01.isoftone.com </span><br><span class="line">172.31.21.209   gfs02.isoftone.com </span><br><span class="line">172.31.21.210   gfs03.isoftone.com</span><br><span class="line">172.31.21.211   gfs04.isoftone.com</span><br></pre></td></tr></table></figure><h3 id="创建-endpoints"><a href="#创建-endpoints" class="headerlink" title="创建 endpoints"></a>创建 endpoints</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> curl -O https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/glusterfs/glusterfs-endpoints.yaml</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 导入 glusterfs-pod.json</span><br><span class="line"><span class="meta">$</span> kubectl apply -f glusterfs-endpoints.yaml</span><br><span class="line"><span class="meta">$</span> kubectl get ep</span><br><span class="line">NAME                ENDPOINTS                                                     AGE</span><br><span class="line">glusterfs-cluster   172.31.21.208:1,172.31.21.209:1,172.31.21.210:1 + 1 more...   59m</span><br></pre></td></tr></table></figure><h3 id="创建-PersistentVolume"><a href="#创建-PersistentVolume" class="headerlink" title="创建 PersistentVolume"></a>创建 PersistentVolume</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> curl -O https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/glusterfs/glusterfs-pv.yaml</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> kubectl apply -f glusterfs-pv.yaml</span><br><span class="line"><span class="meta">$</span> kubectl get pv | grep glus</span><br><span class="line">glusterfs-pv   8Gi        RWX            Retain           Bound     default/glusterfs-pvc                             55m</span><br></pre></td></tr></table></figure><h3 id="创建-PersistentVolumeClaim"><a href="#创建-PersistentVolumeClaim" class="headerlink" title="创建 PersistentVolumeClaim"></a>创建 PersistentVolumeClaim</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> curl -O https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/glusterfs/glusterfs-pvc.yaml</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> kubectl apply -f glusterfs-pvc.yaml</span><br><span class="line"><span class="meta">$</span> kubectl get pvc | grep glus</span><br><span class="line">glusterfs-pvc   Bound     glusterfs-pv   8Gi        RWX                           56m</span><br></pre></td></tr></table></figure><h3 id="创建测试-nginx-挂载-volume"><a href="#创建测试-nginx-挂载-volume" class="headerlink" title="创建测试 nginx 挂载 volume"></a>创建测试 nginx 挂载 volume</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> curl -O https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/glusterfs/nginx-deployment.yaml</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> kubectl apply -f nginx-deployment.yaml</span><br><span class="line"><span class="meta">$</span> kubectl get deploy,pod</span><br><span class="line">NAME              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deploy/nginx-dm   2         2         2            2           54m</span><br><span class="line"></span><br><span class="line">NAME                           READY     STATUS    RESTARTS   AGE</span><br><span class="line">po/nginx-dm-5d49ccb7f5-h62dc   1/1       Running   0          54m</span><br><span class="line">po/nginx-dm-5d49ccb7f5-szsfg   1/1       Running   0          54m</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 登陆 k8s node 物理机，使用 df 可查看挂载目录</span><br><span class="line"><span class="meta">$</span> df -h | grep glusterfs</span><br><span class="line">172.31.21.208:k8s-volume  560G  8.7G  552G    2% /var/lib/kubelet/pods/c403020c-0ca8-11e8-ace7-000c29c217e5/volumes/kubernetes.io~glusterfs/glusterfs-pv</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 查看挂载</span><br><span class="line"><span class="meta">$</span> kubectl exec -it nginx-dm-5d49ccb7f5-h62dc -- df -h|grep k8s-volume</span><br><span class="line">172.31.21.208:k8s-volume                                                                             560G  8.7G  552G   2% /usr/share/nginx/html</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 创建文件 测试</span><br><span class="line"><span class="meta">$</span> kubectl exec -it nginx-dm-5d49ccb7f5-h62dc -- touch /usr/share/nginx/html/index.html</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> kubectl exec -it nginx-dm-5d49ccb7f5-h62dc -- ls -lt /usr/share/nginx/html/index.html</span><br><span class="line">-rw-r--r-- 1 root root 0 Feb  8 09:24 /usr/share/nginx/html/index.html</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 验证 glusterfs</span><br><span class="line"><span class="meta">#</span> 因为我们使用分布复制卷 replica 2，所以可以看到有 2 个节点中有文件</span><br><span class="line">[root@glusterfs-01 gfs_data]# ls</span><br><span class="line">index.html</span><br><span class="line">[root@glusterfs-02 gfs_data]# ls</span><br><span class="line">index.html</span><br><span class="line">[root@glusterfs-03 gfs_data]# ls</span><br><span class="line">[root@glusterfs-04 gfs_data]# ls</span><br></pre></td></tr></table></figure><h2 id="安装-Heketi"><a href="#安装-Heketi" class="headerlink" title="安装 Heketi"></a>安装 Heketi</h2><p>GlusterFS 是个开源的分布式文件系统，而 Heketi 在其上提供了 REST 形式的 API，二者协同为 Kubernetes 提供了存储卷的自动供给能力。</p><ul><li>Heketi 服务器：172.31.21.208</li><li>Gluster 服务器：<ul><li>172.31.21.208</li><li>172.31.21.209</li><li>172.31.21.210</li><li>172.31.21.211</li></ul></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 安装</span><br><span class="line"><span class="meta">$</span> yum -y install epel-release</span><br><span class="line"><span class="meta">$</span> yum -y install heketi heketi-client</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 管理证书</span><br><span class="line"><span class="meta">$</span> ssh-keygen -f /etc/heketi/heketi_key -t rsa -N ''</span><br><span class="line"><span class="meta">$</span> chown heketi:heketi /etc/heketi/heketi_key*</span><br></pre></td></tr></table></figure><p>制作完成后会在当前目录下生成 heketi_key、heketi_key.pub。接下来，修改 /etc/heketi/heketi.json 中的 keyfile 指向生成的 key。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 将公钥 heketi_key.pub 拷贝到所有 glusterfs 节点上 /etc/heketi/keketi_key.pub</span><br><span class="line"><span class="meta">$</span> ssh-copy-id -i /etc/heketi/heketi_key.pub root@172.31.21.209</span><br><span class="line"><span class="meta">$</span> ssh-copy-id -i /etc/heketi/heketi_key.pub root@172.31.21.210</span><br><span class="line"><span class="meta">$</span> ssh-copy-id -i /etc/heketi/heketi_key.pub root@172.31.21.211</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> cat /etc/heketi/heketi.json</span><br><span class="line">&#123;</span><br><span class="line">  "_port_comment": "Heketi Server Port Number",</span><br><span class="line">  "port": "8080",</span><br><span class="line"></span><br><span class="line">  "_use_auth": "Enable JWT authorization. Please enable for deployment",</span><br><span class="line">  "use_auth": false,</span><br><span class="line"></span><br><span class="line">  "_jwt": "Private keys for access",</span><br><span class="line">  "jwt": &#123;</span><br><span class="line">    "_admin": "Admin has access to all APIs",</span><br><span class="line">    "admin": &#123;</span><br><span class="line">      "key": "123456"</span><br><span class="line">    &#125;,</span><br><span class="line">    "_user": "User only has access to /volumes endpoint",</span><br><span class="line">    "user": &#123;</span><br><span class="line">      "key": "123456"</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  "_glusterfs_comment": "GlusterFS Configuration",</span><br><span class="line">  "glusterfs": &#123;</span><br><span class="line">    "_executor_comment": [</span><br><span class="line">      "Execute plugin. Possible choices: mock, ssh",</span><br><span class="line">      "mock: This setting is used for testing and development.",</span><br><span class="line">      "      It will not send commands to any node.",</span><br><span class="line">      "ssh:  This setting will notify Heketi to ssh to the nodes.",</span><br><span class="line">      "      It will need the values in sshexec to be configured.",</span><br><span class="line">      "kubernetes: Communicate with GlusterFS containers over",</span><br><span class="line">      "            Kubernetes exec api."</span><br><span class="line">    ],</span><br><span class="line">    "executor": "ssh",</span><br><span class="line"></span><br><span class="line">    "_sshexec_comment": "SSH username and private key file information",</span><br><span class="line">    "sshexec": &#123;</span><br><span class="line">      "keyfile": "/etc/heketi/heketi_key",</span><br><span class="line">      "user": "root",</span><br><span class="line">      "port": "Optional: ssh port.  Default is 22",</span><br><span class="line">      "fstab": "Optional: Specify fstab file on node.  Default is /etc/fstab"</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    "_db_comment": "Database file name",</span><br><span class="line">    "db": "/var/lib/heketi/heketi.db",</span><br><span class="line"></span><br><span class="line">    "_loglevel_comment": [</span><br><span class="line">      "Set log level. Choices are:",</span><br><span class="line">      "  none, critical, error, warning, info, debug",</span><br><span class="line">      "Default is warning"</span><br><span class="line">    ],</span><br><span class="line">    "loglevel" : "debug"</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 启动</span><br><span class="line">systemctl enable heketi &amp;&amp; systemctl restart heketi &amp;&amp; systemctl status heketi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 测试</span><br><span class="line"><span class="meta">$</span> curl http://localhost:8080/hello</span><br><span class="line">Hello from Heketi</span><br></pre></td></tr></table></figure><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><p><a href="https://jimmysong.io/kubernetes-handbook/practice/using-glusterfs-for-persistent-storage.html" target="_blank" rel="noopener">使用glusterfs做持久化存储</a></p></li><li><p><a href="http://blog.csdn.net/liukuan73/article/details/78477520" target="_blank" rel="noopener">CentOS7上Glusterfs的安装及使用（gluster/heketi)</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;安装-Gluster-FS&quot;&gt;&lt;a href=&quot;#安装-Gluster-FS&quot; class=&quot;headerlink&quot; title=&quot;安装 Gluster FS&quot;&gt;&lt;/a&gt;安装 Gluster FS&lt;/h2&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;/span&gt; 先安装 gluster 源&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; yum install centos-release-gluster -y&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;/span&gt; 安装 glusterfs 组件&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; yum install -y glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma glusterfs-geo-replication glusterfs-devel&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;/span&gt; 创建 glusterfs 目录&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; mkdir /opt/glusterd&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;/span&gt; 修改 glusterd 目录&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; sed -i &#39;s/var\/lib/opt/g&#39; /etc/glusterfs/glusterd.vol&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;/span&gt; 启动 glusterfs&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; systemctl start glusterd.service&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;/span&gt; 设置开机启动&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; systemctl enable glusterd.service&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;/span&gt;查看状态&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; systemctl status glusterd.service&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="docker" scheme="http://batizhao.github.io/categories/docker/"/>
    
    
      <category term="linux" scheme="http://batizhao.github.io/tags/linux/"/>
    
      <category term="kubernetes" scheme="http://batizhao.github.io/tags/kubernetes/"/>
    
      <category term="glusterfs" scheme="http://batizhao.github.io/tags/glusterfs/"/>
    
  </entry>
  
  <entry>
    <title>在 Mac 上通过 Minikube 安装本地 K8s 集群</title>
    <link href="http://batizhao.github.io/2018/01/18/Running-Kubernetes-Locally-via-Minikube/"/>
    <id>http://batizhao.github.io/2018/01/18/Running-Kubernetes-Locally-via-Minikube/</id>
    <published>2018-01-18T08:41:33.000Z</published>
    <updated>2018-01-18T09:13:33.166Z</updated>
    
    <content type="html"><![CDATA[<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h3 id="安装-xhyve-驱动程序。"><a href="#安装-xhyve-驱动程序。" class="headerlink" title="安装 xhyve 驱动程序。"></a>安装 xhyve 驱动程序。</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> brew install docker-machine-driver-xhyve</span><br><span class="line"><span class="meta">$</span> sudo chown root:wheel $(brew --prefix)/opt/docker-machine-driver-xhyve/bin/docker-machine-driver-xhyve</span><br><span class="line"><span class="meta">$</span> sudo chmod u+s $(brew --prefix)/opt/docker-machine-driver-xhyve/bin/docker-machine-driver-xhyve</span><br></pre></td></tr></table></figure><h3 id="安装-kubectl"><a href="#安装-kubectl" class="headerlink" title="安装 kubectl"></a>安装 kubectl</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/darwin/amd64/kubectl</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> chmod +x ./kubectl</span><br><span class="line"><span class="meta">$</span> sudo mv ./kubectl /usr/local/bin/kubectl</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> kubectl version</span><br><span class="line">Client Version: version.Info&#123;Major:"1", Minor:"9", GitVersion:"v1.9.1", GitCommit:"3a1c9449a956b6026f075fa3134ff92f7d55f812", GitTreeState:"clean", BuildDate:"2018-01-04T11:52:23Z", GoVersion:"go1.9.2", Compiler:"gc", Platform:"darwin/amd64"&#125;</span><br><span class="line">Server Version: version.Info&#123;Major:"1", Minor:"8", GitVersion:"v1.8.0", GitCommit:"0b9efaeb34a2fc51ff8e4d34ad9bc6375459c4a4", GitTreeState:"clean", BuildDate:"2017-11-29T22:43:34Z", GoVersion:"go1.9.1", Compiler:"gc", Platform:"linux/amd64"&#125;</span><br></pre></td></tr></table></figure><h2 id="安装-Minikube"><a href="#安装-Minikube" class="headerlink" title="安装 Minikube"></a>安装 Minikube</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.24.1/minikube-darwin-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span> minikube version</span><br><span class="line">minikube version: v0.24.1</span><br></pre></td></tr></table></figure><a id="more"></a><p>启动集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> minikube start --vm-driver=xhyve --registry-mirror=https://registry.docker-cn.com</span><br></pre></td></tr></table></figure><p>查看节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> kubectl get node</span><br><span class="line">NAME       STATUS    ROLES     AGE       VERSION</span><br><span class="line">minikube   Ready     &lt;none&gt;    13d       v1.8.0</span><br></pre></td></tr></table></figure><p>进入集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ minikube ssh</span><br></pre></td></tr></table></figure><p>或者使用 Minikube Docker 守护进程：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> eval $(minikube docker-env)</span><br></pre></td></tr></table></figure><p>如果不使用 Minikube，可以通过运行 eval $(minikube docker-env -u) 来撤消此更改。</p><p>确保以下镜像已经预先下载（查源码），可以使用<a href="https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/minikube/pull_minikube_img.sh" target="_blank" rel="noopener">这个脚本</a>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">gcr.io/google_containers/pause-amd64:3.0</span><br><span class="line">gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5</span><br><span class="line"></span><br><span class="line">gcr.io/google-containers/kube-addon-manager:v6.4-beta.2</span><br><span class="line">gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5</span><br><span class="line">gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5</span><br><span class="line">gcr.io/google_containers/kubernetes-dashboard-amd64:v1.8.0</span><br><span class="line">gcr.io/k8s-minikube/storage-provisioner:v1.8.1</span><br><span class="line">quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.9.0-beta.17</span><br><span class="line">gcr.io/google_containers/defaultbackend:1.4</span><br></pre></td></tr></table></figure><p>确认所有服务就绪<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> kubectl get pod --all-namespaces</span><br><span class="line">NAMESPACE     NAME                             READY     STATUS    RESTARTS   AGE</span><br><span class="line">kube-system   default-http-backend-qd455       1/1       Running   1          47m</span><br><span class="line">kube-system   kube-addon-manager-minikube      1/1       Running   3          13d</span><br><span class="line">kube-system   kube-dns-86f6f55dd5-wkbxj        3/3       Running   9          47m</span><br><span class="line">kube-system   kubernetes-dashboard-qn4tw       1/1       Running   3          47m</span><br><span class="line">kube-system   nginx-ingress-controller-jvbtg   1/1       Running   2          47m</span><br><span class="line">kube-system   storage-provisioner              1/1       Running   1          47m</span><br></pre></td></tr></table></figure></p><p>访问 Dashboard</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> minikube dashboard</span><br><span class="line">Opening kubernetes dashboard in default browser...</span><br></pre></td></tr></table></figure><p>切换集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> kubectl config use-context minikube</span><br></pre></td></tr></table></figure><p>查看集群信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> $ kubectl cluster-info</span><br><span class="line">Kubernetes master is running at https://192.168.64.3:8443</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;准备工作&quot;&gt;&lt;a href=&quot;#准备工作&quot; class=&quot;headerlink&quot; title=&quot;准备工作&quot;&gt;&lt;/a&gt;准备工作&lt;/h2&gt;&lt;h3 id=&quot;安装-xhyve-驱动程序。&quot;&gt;&lt;a href=&quot;#安装-xhyve-驱动程序。&quot; class=&quot;headerlink&quot; title=&quot;安装 xhyve 驱动程序。&quot;&gt;&lt;/a&gt;安装 xhyve 驱动程序。&lt;/h3&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; brew install docker-machine-driver-xhyve&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; sudo chown root:wheel $(brew --prefix)/opt/docker-machine-driver-xhyve/bin/docker-machine-driver-xhyve&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; sudo chmod u+s $(brew --prefix)/opt/docker-machine-driver-xhyve/bin/docker-machine-driver-xhyve&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;安装-kubectl&quot;&gt;&lt;a href=&quot;#安装-kubectl&quot; class=&quot;headerlink&quot; title=&quot;安装 kubectl&quot;&gt;&lt;/a&gt;安装 kubectl&lt;/h3&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/darwin/amd64/kubectl&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; chmod +x ./kubectl&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; sudo mv ./kubectl /usr/local/bin/kubectl&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; kubectl version&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Client Version: version.Info&amp;#123;Major:&quot;1&quot;, Minor:&quot;9&quot;, GitVersion:&quot;v1.9.1&quot;, GitCommit:&quot;3a1c9449a956b6026f075fa3134ff92f7d55f812&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-01-04T11:52:23Z&quot;, GoVersion:&quot;go1.9.2&quot;, Compiler:&quot;gc&quot;, Platform:&quot;darwin/amd64&quot;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Server Version: version.Info&amp;#123;Major:&quot;1&quot;, Minor:&quot;8&quot;, GitVersion:&quot;v1.8.0&quot;, GitCommit:&quot;0b9efaeb34a2fc51ff8e4d34ad9bc6375459c4a4&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2017-11-29T22:43:34Z&quot;, GoVersion:&quot;go1.9.1&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;安装-Minikube&quot;&gt;&lt;a href=&quot;#安装-Minikube&quot; class=&quot;headerlink&quot; title=&quot;安装 Minikube&quot;&gt;&lt;/a&gt;安装 Minikube&lt;/h2&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.24.1/minikube-darwin-amd64 &amp;amp;&amp;amp; chmod +x minikube &amp;amp;&amp;amp; sudo mv minikube /usr/local/bin/&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt; minikube version&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;minikube version: v0.24.1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="docker" scheme="http://batizhao.github.io/categories/docker/"/>
    
    
      <category term="mac" scheme="http://batizhao.github.io/tags/mac/"/>
    
      <category term="kubernetes" scheme="http://batizhao.github.io/tags/kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>在 kubernetes 1.8 上部署 ingress</title>
    <link href="http://batizhao.github.io/2017/12/15/install-ingress-to-k8s/"/>
    <id>http://batizhao.github.io/2017/12/15/install-ingress-to-k8s/</id>
    <published>2017-12-15T09:10:41.000Z</published>
    <updated>2017-12-15T09:26:11.884Z</updated>
    
    <content type="html"><![CDATA[<h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>本文所需代码在<a href="https://github.com/batizhao/dockerfile/tree/master/k8s/ingress" target="_blank" rel="noopener">这里</a></p><p>先查看集群状态<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get nodes</span><br><span class="line">NAME         STATUS    ROLES     AGE       VERSION</span><br><span class="line">k8s-master   Ready     master    15d       v1.8.4</span><br><span class="line">k8s-node-1   Ready     &lt;none&gt;    15d       v1.8.4</span><br><span class="line">k8s-node-2   Ready     &lt;none&gt;    15d       v1.8.4</span><br></pre></td></tr></table></figure></p><p>ingress 有多种方式</p><ul><li>deployment 自由调度</li><li>daemonset 全局调度</li></ul><p>官方部署现在是 deployment 方式。在 deployment 自由调度过程中，由于我们需要约束 controller 调度到指定的 node 中，所以需要对 node 进行 label 标签。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl label nodes k8s-node-1 ingress=proxy</span><br><span class="line">node <span class="string">"k8s-node-1"</span> labeled</span><br><span class="line">$ kubectl label nodes k8s-node-2 ingress=proxy</span><br><span class="line">node <span class="string">"k8s-node-2” labeled</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ kubectl get nodes --show-labels</span></span><br><span class="line"><span class="string">NAME         STATUS    ROLES     AGE       VERSION   LABELS</span></span><br><span class="line"><span class="string">k8s-master   Ready     master    15d       v1.8.4    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=k8s-master,node-role.kubernetes.io/master=</span></span><br><span class="line"><span class="string">k8s-node-1   Ready     &lt;none&gt;    15d       v1.8.4    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=k8s-node-1</span></span><br><span class="line"><span class="string">k8s-node-2   Ready     &lt;none&gt;    15d       v1.8.4    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=k8s-node-2</span></span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f namespace.yaml </span><br><span class="line">$ kubectl apply -f default-backend.yaml </span><br><span class="line">$ kubectl apply -f configmap.yaml </span><br><span class="line">$ kubectl apply -f tcp-services-configmap.yaml </span><br><span class="line">$ kubectl apply -f udp-services-configmap.yaml </span><br><span class="line">$ kubectl apply -f rbac.yaml </span><br><span class="line">$ kubectl apply -f with-rbac.yaml</span><br><span class="line"></span><br><span class="line">$ kubectl get pods --all-namespaces -l app=ingress-nginx -o wide</span><br><span class="line">NAMESPACE       NAME                                        READY     STATUS    RESTARTS   AGE       IP              NODE</span><br><span class="line">ingress-nginx   nginx-ingress-controller-64f7567b77-dv7cn   1/1       Running   0          6h        172.31.21.148   k8s-node-2</span><br><span class="line">ingress-nginx   nginx-ingress-controller-64f7567b77-zcvkt   1/1       Running   0          6h        172.31.21.147   k8s-node-1</span><br></pre></td></tr></table></figure><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><h3 id="jenkins"><a href="#jenkins" class="headerlink" title="jenkins"></a>jenkins</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">jenkins-ui</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">  - host:</span> <span class="string">jenkins.idealsoftone.com</span></span><br><span class="line"><span class="attr">    http:</span></span><br><span class="line"><span class="attr">      paths:</span></span><br><span class="line"><span class="attr">      - path:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">        backend:</span></span><br><span class="line"><span class="attr">          serviceName:</span> <span class="string">jenkins-ui</span></span><br><span class="line"><span class="attr">          servicePort:</span> <span class="number">8080</span></span><br></pre></td></tr></table></figure><h3 id="harbor"><a href="#harbor" class="headerlink" title="harbor"></a>harbor</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">harbor-ui</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">  - host:</span> <span class="string">hub.idealsoftone.com</span></span><br><span class="line"><span class="attr">    http:</span></span><br><span class="line"><span class="attr">      paths:</span></span><br><span class="line"><span class="attr">      - path:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">        backend:</span></span><br><span class="line"><span class="attr">          serviceName:</span> <span class="string">harbor-ui</span></span><br><span class="line"><span class="attr">          servicePort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;准备&quot;&gt;&lt;a href=&quot;#准备&quot; class=&quot;headerlink&quot; title=&quot;准备&quot;&gt;&lt;/a&gt;准备&lt;/h2&gt;&lt;p&gt;本文所需代码在&lt;a href=&quot;https://github.com/batizhao/dockerfile/tree/master/k8s/ingress&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;先查看集群状态&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ kubectl get nodes&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;NAME         STATUS    ROLES     AGE       VERSION&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;k8s-master   Ready     master    15d       v1.8.4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;k8s-node-1   Ready     &amp;lt;none&amp;gt;    15d       v1.8.4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;k8s-node-2   Ready     &amp;lt;none&amp;gt;    15d       v1.8.4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;ingress 有多种方式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;deployment 自由调度&lt;/li&gt;
&lt;li&gt;daemonset 全局调度&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;官方部署现在是 deployment 方式。在 deployment 自由调度过程中，由于我们需要约束 controller 调度到指定的 node 中，所以需要对 node 进行 label 标签。&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ kubectl label nodes k8s-node-1 ingress=proxy&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;node &lt;span class=&quot;string&quot;&gt;&quot;k8s-node-1&quot;&lt;/span&gt; labeled&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ kubectl label nodes k8s-node-2 ingress=proxy&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;node &lt;span class=&quot;string&quot;&gt;&quot;k8s-node-2” labeled&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;$ kubectl get nodes --show-labels&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;NAME         STATUS    ROLES     AGE       VERSION   LABELS&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;k8s-master   Ready     master    15d       v1.8.4    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=k8s-master,node-role.kubernetes.io/master=&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;k8s-node-1   Ready     &amp;lt;none&amp;gt;    15d       v1.8.4    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=k8s-node-1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;k8s-node-2   Ready     &amp;lt;none&amp;gt;    15d       v1.8.4    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=k8s-node-2&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="docker" scheme="http://batizhao.github.io/categories/docker/"/>
    
    
      <category term="linux" scheme="http://batizhao.github.io/tags/linux/"/>
    
      <category term="centos" scheme="http://batizhao.github.io/tags/centos/"/>
    
      <category term="kubernetes" scheme="http://batizhao.github.io/tags/kubernetes/"/>
    
      <category term="ingress" scheme="http://batizhao.github.io/tags/ingress/"/>
    
      <category term="nginx" scheme="http://batizhao.github.io/tags/nginx/"/>
    
  </entry>
  
  <entry>
    <title>在 kubernetes 1.8 上部署 Jenkins 动态集群</title>
    <link href="http://batizhao.github.io/2017/12/15/install-jenkins-to-k8s/"/>
    <id>http://batizhao.github.io/2017/12/15/install-jenkins-to-k8s/</id>
    <published>2017-12-15T08:25:42.000Z</published>
    <updated>2017-12-15T09:19:32.950Z</updated>
    
    <content type="html"><![CDATA[<p>本文的目的是通过在 Kubernetes 集群上创建并配置 Jenkins Server ，实现应用开发管理的 CI/CD 流程，并且利用 Kubernetes-Jenkins-Plugin 实现动态的按需扩展 jenkins-slave。</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>本文所需代码在<a href="https://github.com/batizhao/dockerfile/tree/master/k8s/jenkins" target="_blank" rel="noopener">这里</a></p><h3 id="推送-Jenkins-Master-Docker-镜像到-Harbor"><a href="#推送-Jenkins-Master-Docker-镜像到-Harbor" class="headerlink" title="推送 Jenkins Master Docker 镜像到 Harbor"></a>推送 Jenkins Master Docker 镜像到 Harbor</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> master</span><br><span class="line">$ docker build -t 172.31.21.226/ideal/jenkins:lts .</span><br><span class="line">$ docker push 172.31.21.226/ideal/jenkins:lts</span><br></pre></td></tr></table></figure><h3 id="推送-Jenkins-Slave-Docker-镜像到-Harbor"><a href="#推送-Jenkins-Slave-Docker-镜像到-Harbor" class="headerlink" title="推送 Jenkins Slave Docker 镜像到 Harbor"></a>推送 Jenkins Slave Docker 镜像到 Harbor</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> slave</span><br><span class="line">$ docker build -t 172.31.21.226/ideal/jnlp-slave:latest .</span><br><span class="line">$ docker push 172.31.21.226/ideal/jnlp-slave:latest</span><br></pre></td></tr></table></figure><h3 id="安装-Jenkins"><a href="#安装-Jenkins" class="headerlink" title="安装 Jenkins"></a>安装 Jenkins</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/jenkins/service-account.yml</span><br><span class="line">$ kubectl apply -f ./</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="Kubernetes-插件"><a href="#Kubernetes-插件" class="headerlink" title="Kubernetes 插件"></a>Kubernetes 插件</h2><h3 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h3><p>略过。</p><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>系统管理 - 系统设置 - 云 - Kubernetes</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Name: kubernetes</span><br><span class="line">Kubernetes URL: https://kubernetes.default</span><br><span class="line">Jenkins URL: http://jenkins.default:8080</span><br></pre></td></tr></table></figure><p>如果 service account 没有问题，点击 test，应该可以看到 Connection test successful。</p><p><img src="/images/2017-12-15-install-jenkins-to-k8s-3.png" alt=""></p><p>系统管理 - 系统设置 - 云 - Kubernetes - Add Pod Template<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">images - Add Pod Template:</span><br><span class="line">Name: jnlp-slave</span><br><span class="line">Labels: jnlp-slave</span><br><span class="line"></span><br><span class="line">Containers:</span><br><span class="line">Name: jnlp </span><br><span class="line">Docker image: 172.31.21.226/ideal/jnlp-slave:latest</span><br><span class="line">Always pull image: yes</span><br><span class="line">Jenkins slave root directory: /home/jenkins</span><br><span class="line">Host path: /var/run/docker.sock</span><br><span class="line">Mount path: /var/run/docker.sock</span><br></pre></td></tr></table></figure><br><img src="/images/2017-12-15-install-jenkins-to-k8s-4.png" alt=""><br><img src="/images/2017-12-15-install-jenkins-to-k8s-5.png" alt=""></p><h2 id="Jenkins-Job"><a href="#Jenkins-Job" class="headerlink" title="Jenkins Job"></a>Jenkins Job</h2><h3 id="非-pipeline-方式"><a href="#非-pipeline-方式" class="headerlink" title="非 pipeline 方式"></a>非 pipeline 方式</h3><p><img src="/images/2017-12-15-install-jenkins-to-k8s-6.png" alt=""><br><img src="/images/2017-12-15-install-jenkins-to-k8s-7.png" alt=""><br><img src="/images/2017-12-15-install-jenkins-to-k8s-8.png" alt=""></p><h3 id="pipeline-方式"><a href="#pipeline-方式" class="headerlink" title="pipeline 方式"></a>pipeline 方式</h3><p>直接实现 groovy 脚本，可以放到 git 中管理。</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">podTemplate(<span class="string">label:</span> <span class="string">'jnlp-slave'</span>) &#123;</span><br><span class="line">  node(<span class="string">'jnlp-slave'</span>)&#123;</span><br><span class="line">    git <span class="string">branch:</span> <span class="string">'master'</span>, <span class="string">credentialsId:</span> <span class="string">'e242d1e1-58b5-4645-a84e-64f957e32016'</span>, <span class="string">url:</span> <span class="string">'https://gitee.com/idealsoftone/poseidon.git'</span></span><br><span class="line">    sh <span class="string">'sleep 120'</span></span><br><span class="line">    build_tag = sh(<span class="string">returnStdout:</span> <span class="literal">true</span>, <span class="string">script:</span> <span class="string">'git rev-parse --short HEAD'</span>).trim()</span><br><span class="line">    echo build_tag</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里指定 jenkins slave 为插件中配置的 jnlp-slave。</p><p><img src="/images/2017-12-15-install-jenkins-to-k8s-9.png" alt=""><br><img src="/images/2017-12-15-install-jenkins-to-k8s-10.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文的目的是通过在 Kubernetes 集群上创建并配置 Jenkins Server ，实现应用开发管理的 CI/CD 流程，并且利用 Kubernetes-Jenkins-Plugin 实现动态的按需扩展 jenkins-slave。&lt;/p&gt;
&lt;h2 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h2&gt;&lt;p&gt;本文所需代码在&lt;a href=&quot;https://github.com/batizhao/dockerfile/tree/master/k8s/jenkins&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;推送-Jenkins-Master-Docker-镜像到-Harbor&quot;&gt;&lt;a href=&quot;#推送-Jenkins-Master-Docker-镜像到-Harbor&quot; class=&quot;headerlink&quot; title=&quot;推送 Jenkins Master Docker 镜像到 Harbor&quot;&gt;&lt;/a&gt;推送 Jenkins Master Docker 镜像到 Harbor&lt;/h3&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ &lt;span class=&quot;built_in&quot;&gt;cd&lt;/span&gt; master&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ docker build -t 172.31.21.226/ideal/jenkins:lts .&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ docker push 172.31.21.226/ideal/jenkins:lts&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;推送-Jenkins-Slave-Docker-镜像到-Harbor&quot;&gt;&lt;a href=&quot;#推送-Jenkins-Slave-Docker-镜像到-Harbor&quot; class=&quot;headerlink&quot; title=&quot;推送 Jenkins Slave Docker 镜像到 Harbor&quot;&gt;&lt;/a&gt;推送 Jenkins Slave Docker 镜像到 Harbor&lt;/h3&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ &lt;span class=&quot;built_in&quot;&gt;cd&lt;/span&gt; slave&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ docker build -t 172.31.21.226/ideal/jnlp-slave:latest .&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ docker push 172.31.21.226/ideal/jnlp-slave:latest&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;安装-Jenkins&quot;&gt;&lt;a href=&quot;#安装-Jenkins&quot; class=&quot;headerlink&quot; title=&quot;安装 Jenkins&quot;&gt;&lt;/a&gt;安装 Jenkins&lt;/h3&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/jenkins/service-account.yml&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ kubectl apply -f ./&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="docker" scheme="http://batizhao.github.io/categories/docker/"/>
    
    
      <category term="linux" scheme="http://batizhao.github.io/tags/linux/"/>
    
      <category term="centos" scheme="http://batizhao.github.io/tags/centos/"/>
    
      <category term="kubernetes" scheme="http://batizhao.github.io/tags/kubernetes/"/>
    
      <category term="jenkins" scheme="http://batizhao.github.io/tags/jenkins/"/>
    
  </entry>
  
  <entry>
    <title>在 kubernetes 1.8 上安装 Harbor 仓库</title>
    <link href="http://batizhao.github.io/2017/12/15/install-harbor-to-k8s/"/>
    <id>http://batizhao.github.io/2017/12/15/install-harbor-to-k8s/</id>
    <published>2017-12-15T07:42:14.000Z</published>
    <updated>2017-12-19T04:29:24.318Z</updated>
    
    <content type="html"><![CDATA[<h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>当前最新版本是 1.2.2</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ wget http://harbor.orientsoft.cn/harbor-1.2.2/harbor-offline-installer-v1.2.2.tgz</span><br></pre></td></tr></table></figure><h2 id="准备-Docker-镜像"><a href="#准备-Docker-镜像" class="headerlink" title="准备 Docker 镜像"></a>准备 Docker 镜像</h2><p>解压以后要把所有镜像上传到 k8s 工作节点。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ tar -vxf harbor-offline-installer-v1.2.2.tgz</span><br><span class="line">$ <span class="built_in">cd</span> harbor/ </span><br><span class="line">$ scp harbor.v1.2.2.tar.gz k8s-node</span><br><span class="line">$ docker load -i harbor.v1.2.2.tar.gz</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="准备配置文件"><a href="#准备配置文件" class="headerlink" title="准备配置文件"></a>准备配置文件</h2><p>下载源码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/vmware/harbor.git</span><br></pre></td></tr></table></figure></p><p>在以下目录中所有的 rc.yaml 中镜像替换成正确的镜像地址<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make/kubernetes/**/*.rc.yaml</span><br></pre></td></tr></table></figure></p><p>在以下目录文件中设置存储的容量<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make/kubernetes/pv/*.pvc.yaml </span><br></pre></td></tr></table></figure><br>如果你改变了 PVC 的容量，那么你也需要相应的设置 PV 的容量。</p><p>如果想让外部访问，需要修改两个地方</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ vim make/harbor.cfg</span><br><span class="line">hostname = 172.31.21.226</span><br></pre></td></tr></table></figure><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">vim</span> <span class="string">make/kubernetes/nginx/nginx.svc.yaml</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">http</span></span><br><span class="line"><span class="attr">    port:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">nginx-apps</span></span><br><span class="line"><span class="attr">  externalIPs:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="number">172.31</span><span class="number">.21</span><span class="number">.226</span></span><br></pre></td></tr></table></figure><p>如果部署了 ingress，可以不用管上边两步</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">harbor-ui</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">  - host:</span> <span class="string">hub.xxx.com</span></span><br><span class="line"><span class="attr">    http:</span></span><br><span class="line"><span class="attr">      paths:</span></span><br><span class="line"><span class="attr">      - path:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">        backend:</span></span><br><span class="line"><span class="attr">          serviceName:</span> <span class="string">harbor-ui</span></span><br><span class="line"><span class="attr">          servicePort:</span> <span class="number">80</span></span><br><span class="line">        </span><br></pre></td></tr></table></figure>  <p>生成安装脚本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ python make/kubernetes/k8s-prepare</span><br></pre></td></tr></table></figure></p><p>脚本执行完成后会生成下面的一些文件：</p><ul><li>make/kubernetes/jobservice/jobservice.cm.yaml</li><li>make/kubernetes/mysql/mysql.cm.yaml</li><li>make/kubernetes/nginx/nginx.cm.yaml</li><li>make/kubernetes/registry/registry.cm.yaml</li><li>make/kubernetes/ui/ui.cm.yaml</li></ul><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create pv &amp; pvc</span></span><br><span class="line">kubectl apply -f make/kubernetes/pv/log.pv.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/pv/registry.pv.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/pv/storage.pv.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/pv/log.pvc.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/pv/registry.pvc.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/pv/storage.pvc.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># create config map</span></span><br><span class="line">kubectl apply -f make/kubernetes/adminserver/adminserver.cm.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/jobservice/jobservice.cm.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/mysql/mysql.cm.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/registry/registry.cm.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/ui/ui.cm.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/nginx/nginx.cm.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># create service</span></span><br><span class="line">kubectl apply -f make/kubernetes/adminserver/adminserver.svc.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/jobservice/jobservice.svc.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/mysql/mysql.svc.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/registry/registry.svc.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/ui/ui.svc.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/nginx/nginx.svc.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># create k8s rc</span></span><br><span class="line">kubectl apply -f make/kubernetes/registry/registry.rc.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/mysql/mysql.rc.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/jobservice/jobservice.rc.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/ui/ui.rc.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/nginx/nginx.rc.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/adminserver/adminserver.rc.yaml</span><br></pre></td></tr></table></figure><p><img src="/images/2017-12-15-install-harbor-to-k8s.png" alt=""></p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><h3 id="Error-response-from-daemon-Get-https-myregistrydomain-com-v1-users-dial-tcp-myregistrydomain-com-443-getsockopt-connection-refused"><a href="#Error-response-from-daemon-Get-https-myregistrydomain-com-v1-users-dial-tcp-myregistrydomain-com-443-getsockopt-connection-refused" class="headerlink" title="Error response from daemon: Get https://myregistrydomain.com/v1/users/: dial tcp myregistrydomain.com:443 getsockopt: connection refused."></a>Error response from daemon: Get <a href="https://myregistrydomain.com/v1/users/" target="_blank" rel="noopener">https://myregistrydomain.com/v1/users/</a>: dial tcp myregistrydomain.com:443 getsockopt: connection refused.</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ vim /etc/docker/daemon.json</span><br><span class="line"><span class="string">"insecure-registries"</span>: [<span class="string">"172.31.21.226"</span>]</span><br><span class="line"></span><br><span class="line">$ cat /etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"insecure-registries"</span>: [<span class="string">"172.31.21.226"</span>],</span><br><span class="line">  <span class="string">"registry-mirrors"</span>: [<span class="string">"https://xxx.mirror.aliyuncs.com"</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">$ sudo systemctl daemon-reload &amp;&amp; systemctl restart docker</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;下载&quot;&gt;&lt;a href=&quot;#下载&quot; class=&quot;headerlink&quot; title=&quot;下载&quot;&gt;&lt;/a&gt;下载&lt;/h2&gt;&lt;p&gt;当前最新版本是 1.2.2&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ wget http://harbor.orientsoft.cn/harbor-1.2.2/harbor-offline-installer-v1.2.2.tgz&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;准备-Docker-镜像&quot;&gt;&lt;a href=&quot;#准备-Docker-镜像&quot; class=&quot;headerlink&quot; title=&quot;准备 Docker 镜像&quot;&gt;&lt;/a&gt;准备 Docker 镜像&lt;/h2&gt;&lt;p&gt;解压以后要把所有镜像上传到 k8s 工作节点。&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ tar -vxf harbor-offline-installer-v1.2.2.tgz&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ &lt;span class=&quot;built_in&quot;&gt;cd&lt;/span&gt; harbor/ &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ scp harbor.v1.2.2.tar.gz k8s-node&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ docker load -i harbor.v1.2.2.tar.gz&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="docker" scheme="http://batizhao.github.io/categories/docker/"/>
    
    
      <category term="linux" scheme="http://batizhao.github.io/tags/linux/"/>
    
      <category term="centos" scheme="http://batizhao.github.io/tags/centos/"/>
    
      <category term="kubernetes" scheme="http://batizhao.github.io/tags/kubernetes/"/>
    
      <category term="harbor" scheme="http://batizhao.github.io/tags/harbor/"/>
    
  </entry>
  
  <entry>
    <title>在 CentOS 上使用 kubeadm 安装 kubernetes 1.8.4</title>
    <link href="http://batizhao.github.io/2017/12/15/install-kubernetes-1-8-4-use-kubeadm/"/>
    <id>http://batizhao.github.io/2017/12/15/install-kubernetes-1-8-4-use-kubeadm/</id>
    <published>2017-12-15T06:08:21.000Z</published>
    <updated>2017-12-15T07:31:59.469Z</updated>
    
    <content type="html"><![CDATA[<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>在所有主机执行以下工作。</p><h3 id="配置主机"><a href="#配置主机" class="headerlink" title="配置主机"></a>配置主机</h3><h4 id="修改主机名称"><a href="#修改主机名称" class="headerlink" title="修改主机名称"></a>修改主机名称</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ hostnamectl --static <span class="built_in">set</span>-hostname k8s-master</span><br><span class="line">$ hostnamectl --static <span class="built_in">set</span>-hostname k8s-node-1</span><br><span class="line">$ hostnamectl --static <span class="built_in">set</span>-hostname k8s-node-2</span><br></pre></td></tr></table></figure><h4 id="配-hosts"><a href="#配-hosts" class="headerlink" title="配 hosts"></a>配 hosts</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">"172.31.21.226  k8s-master</span></span><br><span class="line"><span class="string">172.31.21.147  k8s-node-1</span></span><br><span class="line"><span class="string">172.31.21.148  k8s-node-2"</span> &gt;&gt; /etc/hosts</span><br></pre></td></tr></table></figure><h4 id="关防火墙和-selinux"><a href="#关防火墙和-selinux" class="headerlink" title="关防火墙和 selinux"></a>关防火墙和 selinux</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl stop firewalld &amp;&amp; systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line">$ iptables -P FORWARD ACCEPT</span><br><span class="line">$ sed -i <span class="string">'s/SELINUX=enforcing/SELINUX=disabled/g'</span> /etc/selinux/config</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">"net.bridge.bridge-nf-call-ip6tables = 1</span></span><br><span class="line"><span class="string">net.bridge.bridge-nf-call-iptables = 1</span></span><br><span class="line"><span class="string">vm.swappiness=0"</span> &gt;&gt; /etc/sysctl.d/k8s.conf</span><br><span class="line">$ sysctl -p /etc/sysctl.d/k8s.conf</span><br></pre></td></tr></table></figure><h4 id="关闭-swap"><a href="#关闭-swap" class="headerlink" title="关闭 swap"></a>关闭 swap</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ swapoff -a</span><br></pre></td></tr></table></figure><p>永久关闭，注释 swap 相关内容<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/fstab</span><br></pre></td></tr></table></figure></p><a id="more"></a><h3 id="下载离线安装包"><a href="#下载离线安装包" class="headerlink" title="下载离线安装包"></a>下载离线安装包</h3><p>k8s 最新的版本需要 FQ 下载。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ wget https://packages.cloud.google.com/yum/pool/aeaad1e283c54876b759a089f152228d7cd4c049f271125c23623995b8e76f96-kubeadm-1.8.4-0.x86_64.rpm</span><br><span class="line">$ wget https://packages.cloud.google.com/yum/pool/a9db28728641ddbf7f025b8b496804d82a396d0ccb178fffd124623fb2f999ea-kubectl-1.8.4-0.x86_64.rpm</span><br><span class="line">$ wget https://packages.cloud.google.com/yum/pool/1acca81eb5cf99453f30466876ff03146112b7f12c625cb48f12508684e02665-kubelet-1.8.4-0.x86_64.rpm</span><br><span class="line">$ wget https://packages.cloud.google.com/yum/pool/79f9ba89dbe7000e7dfeda9b119f711bb626fe2c2d56abeb35141142cda00342-kubernetes-cni-0.5.1-1.x86_64.rpm</span><br></pre></td></tr></table></figure><h2 id="安装-docker"><a href="#安装-docker" class="headerlink" title="安装 docker"></a>安装 docker</h2><p>在所有主机执行以下工作。<br>kubernetes 1.8.4 目前支持 Docker 17.03。</p><h3 id="添加阿里源"><a href="#添加阿里源" class="headerlink" title="添加阿里源"></a>添加阿里源</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum-config-manager --add-repo &lt;http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo&gt;</span><br></pre></td></tr></table></figure><h3 id="安装指定-Docker-版本"><a href="#安装指定-Docker-版本" class="headerlink" title="安装指定 Docker 版本"></a>安装指定 Docker 版本</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ yum install -y --<span class="built_in">setopt</span>=obsoletes=0 \</span><br><span class="line">  docker-ce-17.03.2.ce-1.el7.centos \</span><br><span class="line">  docker-ce-selinux-17.03.2.ce-1.el7.centos</span><br></pre></td></tr></table></figure><h3 id="配置-Docker-加速器"><a href="#配置-Docker-加速器" class="headerlink" title="配置 Docker 加速器"></a>配置 Docker 加速器</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ sudo mkdir -p /etc/docker</span><br><span class="line">$ sudo tee /etc/docker/daemon.json &lt;&lt;-<span class="string">'EOF'</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"registry-mirrors"</span>: [<span class="string">"https://xxx.mirror.aliyuncs.com"</span>]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">$ sudo systemctl daemon-reload &amp;&amp; systemctl <span class="built_in">enable</span> docker &amp;&amp; systemctl restart docker &amp;&amp; systemctl status docker</span><br></pre></td></tr></table></figure><h2 id="安装-k8s"><a href="#安装-k8s" class="headerlink" title="安装 k8s"></a>安装 k8s</h2><p>在所有主机执行以下工作。</p><h3 id="启动-kubelet"><a href="#启动-kubelet" class="headerlink" title="启动 kubelet"></a>启动 kubelet</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ yum -y localinstall *.rpm</span><br><span class="line">$ yum install -y socat</span><br><span class="line">$ sed -i <span class="string">'s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g'</span> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br><span class="line">$ systemctl daemon-reload &amp;&amp; systemctl restart kubelet &amp;&amp; systemctl <span class="built_in">enable</span> kubelet &amp;&amp; systemctl status kubelet</span><br></pre></td></tr></table></figure><p>这时 kubelet 应该还在报错，不用管它。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ journalctl -u kubelet --no-pager</span><br></pre></td></tr></table></figure></p><h3 id="准备-Docker-镜像"><a href="#准备-Docker-镜像" class="headerlink" title="准备 Docker 镜像"></a>准备 Docker 镜像</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">gcr.io/google_containers/kube-apiserver-amd64  v1.8.4</span><br><span class="line">gcr.io/google_containers/kube-controller-manager-amd64  v1.8.4</span><br><span class="line">gcr.io/google_containers/kube-proxy-amd64  v1.8.4</span><br><span class="line">gcr.io/google_containers/kube-scheduler-amd64  v1.8.4</span><br><span class="line">quay.io/coreos/flannel    v0.9.1-amd64</span><br><span class="line">gcr.io/google_containers/k8s-dns-sidecar-amd64  1.14.5</span><br><span class="line">gcr.io/google_containers/k8s-dns-kube-dns-amd64  1.14.5</span><br><span class="line">gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64  1.14.5</span><br><span class="line">gcr.io/google_containers/etcd-amd64  3.0.17</span><br><span class="line">gcr.io/google_containers/pause-amd64  3.0</span><br></pre></td></tr></table></figure><p>可以使用<a href="https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/pull_k8s_img.sh" target="_blank" rel="noopener">这个</a>脚本拉取到本地。</p><h2 id="配置-k8s-集群"><a href="#配置-k8s-集群" class="headerlink" title="配置 k8s 集群"></a>配置 k8s 集群</h2><h3 id="master-初始化"><a href="#master-初始化" class="headerlink" title="master 初始化"></a>master 初始化</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">kubeadm init --apiserver-advertise-address=172.31.21.226 --kubernetes-version=v1.8.4 --pod-network-cidr=10.244.0.0/16</span><br><span class="line">[kubeadm] WARNING: kubeadm is <span class="keyword">in</span> beta, please <span class="keyword">do</span> not use it <span class="keyword">for</span> production clusters.</span><br><span class="line">[init] Using Kubernetes version: v1.8.4</span><br><span class="line">[init] Using Authorization modes: [Node RBAC]</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[kubeadm] WARNING: starting <span class="keyword">in</span> 1.8, tokens expire after 24 hours by default (<span class="keyword">if</span> you require a non-expiring token use --token-ttl 0)</span><br><span class="line">[certificates] Generated ca certificate and key.</span><br><span class="line">[certificates] Generated apiserver certificate and key.</span><br><span class="line">[certificates] apiserver serving cert is signed <span class="keyword">for</span> DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.31.21.226]</span><br><span class="line">[certificates] Generated apiserver-kubelet-client certificate and key.</span><br><span class="line">[certificates] Generated sa key and public key.</span><br><span class="line">[certificates] Generated front-proxy-ca certificate and key.</span><br><span class="line">[certificates] Generated front-proxy-client certificate and key.</span><br><span class="line">[certificates] Valid certificates and keys now exist <span class="keyword">in</span> <span class="string">"/etc/kubernetes/pki"</span></span><br><span class="line">[kubeconfig] Wrote KubeConfig file to disk: <span class="string">"admin.conf"</span></span><br><span class="line">[kubeconfig] Wrote KubeConfig file to disk: <span class="string">"kubelet.conf"</span></span><br><span class="line">[kubeconfig] Wrote KubeConfig file to disk: <span class="string">"controller-manager.conf"</span></span><br><span class="line">[kubeconfig] Wrote KubeConfig file to disk: <span class="string">"scheduler.conf"</span></span><br><span class="line">[controlplane] Wrote Static Pod manifest <span class="keyword">for</span> component kube-apiserver to <span class="string">"/etc/kubernetes/manifests/kube-apiserver.yaml"</span></span><br><span class="line">[controlplane] Wrote Static Pod manifest <span class="keyword">for</span> component kube-controller-manager to <span class="string">"/etc/kubernetes/manifests/kube-controller-manager.yaml"</span></span><br><span class="line">[controlplane] Wrote Static Pod manifest <span class="keyword">for</span> component kube-scheduler to <span class="string">"/etc/kubernetes/manifests/kube-scheduler.yaml"</span></span><br><span class="line">[etcd] Wrote Static Pod manifest <span class="keyword">for</span> a <span class="built_in">local</span> etcd instance to <span class="string">"/etc/kubernetes/manifests/etcd.yaml"</span></span><br><span class="line">[init] Waiting <span class="keyword">for</span> the kubelet to boot up the control plane as Static Pods from directory <span class="string">"/etc/kubernetes/manifests"</span></span><br><span class="line">[init] This often takes around a minute; or longer <span class="keyword">if</span> the control plane images have to be pulled.</span><br><span class="line">[apiclient] All control plane components are healthy after 24.501140 seconds</span><br><span class="line">[uploadconfig] Storing the configuration used <span class="keyword">in</span> ConfigMap <span class="string">"kubeadm-config"</span> <span class="keyword">in</span> the <span class="string">"kube-system"</span> Namespace</span><br><span class="line">[markmaster] Will mark node k8s-master as master by adding a label and a taint</span><br><span class="line">[markmaster] Master k8s-master tainted and labelled with key/value: node-role.kubernetes.io/master=<span class="string">""</span></span><br><span class="line">[bootstraptoken] Using token: d87240.989b8aa6b0039283</span><br><span class="line">[bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs <span class="keyword">in</span> order <span class="keyword">for</span> nodes to get long term certificate credentials</span><br><span class="line">[bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class="line">[bootstraptoken] Configured RBAC rules to allow certificate rotation <span class="keyword">for</span> all node client certificates <span class="keyword">in</span> the cluster</span><br><span class="line">[bootstraptoken] Creating the <span class="string">"cluster-info"</span> ConfigMap <span class="keyword">in</span> the <span class="string">"kube-public"</span> namespace</span><br><span class="line">[addons] Applied essential addon: kube-dns</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes master has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run (as a regular user):</span><br><span class="line"></span><br><span class="line">  mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run <span class="string">"kubectl apply -f [podnetwork].yaml"</span> with one of the options listed at:</span><br><span class="line">http://kubernetes.io/docs/admin/addons/</span><br><span class="line"></span><br><span class="line">You can now join any number of machines by running the following on each node</span><br><span class="line">as root:</span><br><span class="line"></span><br><span class="line">  kubeadm join --token d87240.989b8aa6b0039283 172.31.21.226:6443 --discovery-token-ca-cert-hash sha256:4c2b5469ddc4f49ba15f3146bea5bf9ba8f67f68bdc9ef1ff6cb026d39b94dea</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>配置用户使用 kubectl 访问集群<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir -p <span class="variable">$HOME</span>/.kube &amp;&amp; \</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config &amp;&amp; \</span><br><span class="line">  sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br></pre></td></tr></table></figure></p><p>查看一下集群状态<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod --all-namespaces -o wide</span><br><span class="line">NAMESPACE     NAME                                 READY     STATUS    RESTARTS   AGE       IP              NODE</span><br><span class="line">kube-system   etcd-k8s-master                      1/1       Running   0          1m        172.31.21.226   k8s-master</span><br><span class="line">kube-system   kube-apiserver-k8s-master            1/1       Running   0          1m        172.31.21.226   k8s-master</span><br><span class="line">kube-system   kube-controller-manager-k8s-master   1/1       Running   0          1m        172.31.21.226   k8s-master</span><br><span class="line">kube-system   kube-dns-545bc4bfd4-84pjx            0/3       Pending   0          2m        &lt;none&gt;          &lt;none&gt;</span><br><span class="line">kube-system   kube-proxy-7d2tc                     1/1       Running   0          2m        172.31.21.226   k8s-master</span><br><span class="line">kube-system   kube-scheduler-k8s-master            1/1       Running   0          1m        172.31.21.226   k8s-master</span><br><span class="line"></span><br><span class="line">$ kubectl get cs</span><br><span class="line">NAME                 STATUS    MESSAGE              ERROR</span><br><span class="line">controller-manager   Healthy   ok</span><br><span class="line">scheduler            Healthy   ok</span><br><span class="line">etcd-0               Healthy   &#123;<span class="string">"health"</span>: <span class="string">"true”&#125;</span></span><br></pre></td></tr></table></figure></p><p>安装Pod Network<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/flannel/kube-flannel.yml</span><br></pre></td></tr></table></figure></p><p>这时再执行 kubectl get pod –all-namespaces -o wide 应该可以看到 kube-dns-545bc4bfd4-84pjx 已经变成 Running。如果遇到问题可能使用以下命令查看：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kube-system describe pod kube-dns-545bc4bfd4-84pjx</span><br><span class="line">$ journalctl -u kubelet --no-pager</span><br><span class="line">$ journalctl -u docker --no-pager</span><br></pre></td></tr></table></figure><h3 id="node-加入集群"><a href="#node-加入集群" class="headerlink" title="node 加入集群"></a>node 加入集群</h3><p>在 node 节点分别执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubeadm join --token d87240.989b8aa6b0039283 172.31.21.226:6443 --discovery-token-ca-cert-hash sha256:4c2b5469ddc4f49ba15f3146bea5bf9ba8f67f68bdc9ef1ff6cb026d39b94dea</span><br></pre></td></tr></table></figure><p>如果需要从其它任意主机控制集群<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">$ scp root@172.31.21.226:/etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">$ chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">$ kubectl get nodes</span><br></pre></td></tr></table></figure></p><p>在 master 确认所有节点 ready</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get nodes</span><br><span class="line">NAME         STATUS    ROLES     AGE       VERSION</span><br><span class="line">k8s-master   Ready     master    7m        v1.8.4</span><br><span class="line">k8s-node-1   Ready     &lt;none&gt;    22s       v1.8.4</span><br><span class="line">k8s-node-2   Ready     &lt;none&gt;    15s       v1.8.4</span><br><span class="line"></span><br><span class="line">$ kubectl get pod --all-namespaces -o wide</span><br><span class="line">NAMESPACE     NAME                                 READY     STATUS    RESTARTS   AGE       IP              NODE</span><br><span class="line">kube-system   etcd-k8s-master                      1/1       Running   0          51m       172.31.21.226   k8s-master</span><br><span class="line">kube-system   kube-apiserver-k8s-master            1/1       Running   0          51m       172.31.21.226   k8s-master</span><br><span class="line">kube-system   kube-controller-manager-k8s-master   1/1       Running   0          51m       172.31.21.226   k8s-master</span><br><span class="line">kube-system   kube-dns-545bc4bfd4-84pjx            3/3       Running   0          52m       10.244.0.3      k8s-master</span><br><span class="line">kube-system   kube-flannel-ds-gf2hp                1/1       Running   0          6m        172.31.21.226   k8s-master</span><br><span class="line">kube-system   kube-flannel-ds-k8wc9                1/1       Running   0          24s       172.31.21.147   k8s-node-1</span><br><span class="line">kube-system   kube-flannel-ds-v7jpv                1/1       Running   0          10s       172.31.21.148   k8s-node-2</span><br><span class="line">kube-system   kube-proxy-7d2tc                     1/1       Running   0          52m       172.31.21.226   k8s-master</span><br><span class="line">kube-system   kube-proxy-b9z97                     1/1       Running   0          10s       172.31.21.148   k8s-node-2</span><br><span class="line">kube-system   kube-proxy-ksvwp                     1/1       Running   0          24s       172.31.21.147   k8s-node-1</span><br><span class="line">kube-system   kube-scheduler-k8s-master            1/1       Running   0          51m       172.31.21.226   k8s-master</span><br></pre></td></tr></table></figure><h2 id="安装-dashboard"><a href="#安装-dashboard" class="headerlink" title="安装 dashboard"></a>安装 dashboard</h2><h3 id="准备-Docker-镜像-1"><a href="#准备-Docker-镜像-1" class="headerlink" title="准备 Docker 镜像"></a>准备 Docker 镜像</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gcr.io/google_containers/kubernetes-dashboard-amd64  v1.8.0</span><br></pre></td></tr></table></figure><p>可以使用<a href="https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/kubernetes-dashboard/pull_k8s_dashboard_img.sh" target="_blank" rel="noopener">这个</a>脚本拉取到本地。</p><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/kubernetes-dashboard/kubernetes-dashboard.yaml</span><br><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/kubernetes-dashboard/kubernetes-dashboard-admin.rbac.yaml</span><br></pre></td></tr></table></figure><p>确认 dashboard 状态<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod --all-namespaces -o wide</span><br><span class="line">kube-system   kubernetes-dashboard-7486b894c6-2l4c5   1/1       Running   0          17s       10.244.1.3      k8s-node-1</span><br></pre></td></tr></table></figure></p><h3 id="访问"><a href="#访问" class="headerlink" title="访问"></a>访问</h3><p><a href="https://172.31.21.226:30000" target="_blank" rel="noopener">https://172.31.21.226:30000</a></p><p>或者在任意主机执行（比如我的 Mac）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl proxy</span><br></pre></td></tr></table></figure><p>访问：<a href="http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/" target="_blank" rel="noopener">http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</a></p><h3 id="查看登录-token"><a href="#查看登录-token" class="headerlink" title="查看登录 token"></a>查看登录 token</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kube-system get secret | grep kubernetes-dashboard-admin</span><br><span class="line">kubernetes-dashboard-admin-token-r95kv   kubernetes.io/service-account-token   3         7m</span><br><span class="line">$ kubectl describe -n kube-system secret/kubernetes-dashboard-admin-token-r95kv</span><br><span class="line"></span><br><span class="line">eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbi10b2tlbi1yOTVrdiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjM4MWI4OWQzLWQ0ZDctMTFlNy1hY2U3LTAwMGMyOWMyMTdlNSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcm5l</span><br></pre></td></tr></table></figure><h2 id="安装-heapster"><a href="#安装-heapster" class="headerlink" title="安装 heapster"></a>安装 heapster</h2><h3 id="准备-Docker-镜像-2"><a href="#准备-Docker-镜像-2" class="headerlink" title="准备 Docker 镜像"></a>准备 Docker 镜像</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gcr.io/google_containers/heapster-amd64:v1.4.0</span><br><span class="line">gcr.io/google_containers/heapster-grafana-amd64:v4.4.3</span><br><span class="line">gcr.io/google_containers/heapster-influxdb-amd64:v1.3.3</span><br></pre></td></tr></table></figure><p>可以使用<a href="https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/heapster/pull_k8s_heapster_img.sh" target="_blank" rel="noopener">这个</a>脚本拉取到本地。</p><h3 id="初始化-1"><a href="#初始化-1" class="headerlink" title="初始化"></a>初始化</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/heapster/heapster-rbac.yaml</span><br><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/heapster/grafana.yaml</span><br><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/heapster/heapster.yaml </span><br><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/heapster/influxdb.yaml</span><br></pre></td></tr></table></figure><p><img src="/images/2017-12-15-install-kubernetes-1-8-4-use-kubeadm.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;准备工作&quot;&gt;&lt;a href=&quot;#准备工作&quot; class=&quot;headerlink&quot; title=&quot;准备工作&quot;&gt;&lt;/a&gt;准备工作&lt;/h2&gt;&lt;p&gt;在所有主机执行以下工作。&lt;/p&gt;
&lt;h3 id=&quot;配置主机&quot;&gt;&lt;a href=&quot;#配置主机&quot; class=&quot;headerlink&quot; title=&quot;配置主机&quot;&gt;&lt;/a&gt;配置主机&lt;/h3&gt;&lt;h4 id=&quot;修改主机名称&quot;&gt;&lt;a href=&quot;#修改主机名称&quot; class=&quot;headerlink&quot; title=&quot;修改主机名称&quot;&gt;&lt;/a&gt;修改主机名称&lt;/h4&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ hostnamectl --static &lt;span class=&quot;built_in&quot;&gt;set&lt;/span&gt;-hostname k8s-master&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ hostnamectl --static &lt;span class=&quot;built_in&quot;&gt;set&lt;/span&gt;-hostname k8s-node-1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ hostnamectl --static &lt;span class=&quot;built_in&quot;&gt;set&lt;/span&gt;-hostname k8s-node-2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h4 id=&quot;配-hosts&quot;&gt;&lt;a href=&quot;#配-hosts&quot; class=&quot;headerlink&quot; title=&quot;配 hosts&quot;&gt;&lt;/a&gt;配 hosts&lt;/h4&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ &lt;span class=&quot;built_in&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;172.31.21.226  k8s-master&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;172.31.21.147  k8s-node-1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;172.31.21.148  k8s-node-2&quot;&lt;/span&gt; &amp;gt;&amp;gt; /etc/hosts&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h4 id=&quot;关防火墙和-selinux&quot;&gt;&lt;a href=&quot;#关防火墙和-selinux&quot; class=&quot;headerlink&quot; title=&quot;关防火墙和 selinux&quot;&gt;&lt;/a&gt;关防火墙和 selinux&lt;/h4&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ systemctl stop firewalld &amp;amp;&amp;amp; systemctl &lt;span class=&quot;built_in&quot;&gt;disable&lt;/span&gt; firewalld&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ iptables -P FORWARD ACCEPT&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ sed -i &lt;span class=&quot;string&quot;&gt;&#39;s/SELINUX=enforcing/SELINUX=disabled/g&#39;&lt;/span&gt; /etc/selinux/config&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ &lt;span class=&quot;built_in&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;net.bridge.bridge-nf-call-ip6tables = 1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;net.bridge.bridge-nf-call-iptables = 1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;vm.swappiness=0&quot;&lt;/span&gt; &amp;gt;&amp;gt; /etc/sysctl.d/k8s.conf&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ sysctl -p /etc/sysctl.d/k8s.conf&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h4 id=&quot;关闭-swap&quot;&gt;&lt;a href=&quot;#关闭-swap&quot; class=&quot;headerlink&quot; title=&quot;关闭 swap&quot;&gt;&lt;/a&gt;关闭 swap&lt;/h4&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ swapoff -a&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;永久关闭，注释 swap 相关内容&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;vim /etc/fstab&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="docker" scheme="http://batizhao.github.io/categories/docker/"/>
    
    
      <category term="linux" scheme="http://batizhao.github.io/tags/linux/"/>
    
      <category term="centos" scheme="http://batizhao.github.io/tags/centos/"/>
    
      <category term="kubernetes" scheme="http://batizhao.github.io/tags/kubernetes/"/>
    
      <category term="heapster" scheme="http://batizhao.github.io/tags/heapster/"/>
    
  </entry>
  
  <entry>
    <title>使用 Shasdowsocks + Privoxy 转 HTTP 代理</title>
    <link href="http://batizhao.github.io/2017/12/14/share-shasdowsocks-to-http/"/>
    <id>http://batizhao.github.io/2017/12/14/share-shasdowsocks-to-http/</id>
    <published>2017-12-14T06:30:13.000Z</published>
    <updated>2017-12-15T06:41:53.348Z</updated>
    
    <content type="html"><![CDATA[<p>本文把 Mac 上的 Shadowsocks 转换为 http 代理，分享给其它人使用。</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew install privoxy</span><br></pre></td></tr></table></figure><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">'listen-address 0.0.0.0:8118'</span> &gt;&gt; /usr/<span class="built_in">local</span>/etc/privoxy/config</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">'forward-socks5 / localhost:1080 .'</span> &gt;&gt; /usr/<span class="built_in">local</span>/etc/privoxy/config</span><br></pre></td></tr></table></figure><p>8118 是要监听的 http 端口<br>1082 是我自己本地的 shadowsocks 监听端口。</p><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ usr/<span class="built_in">local</span>/Cellar/privoxy/3.0.26/sbin/privoxy /usr/<span class="built_in">local</span>/etc/privoxy/config</span><br></pre></td></tr></table></figure><h2 id="确认"><a href="#确认" class="headerlink" title="确认"></a>确认</h2><p>查进程<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ps aux  | grep privoxy</span><br></pre></td></tr></table></figure></p><p>查端口<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ netstat -an | grep 8118</span><br></pre></td></tr></table></figure></p><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> http_proxy=http://ip:8118</span><br><span class="line">$ <span class="built_in">export</span> https_proxy=<span class="variable">$http_proxy</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文把 Mac 上的 Shadowsocks 转换为 http 代理，分享给其它人使用。&lt;/p&gt;
&lt;h2 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h2&gt;&lt;figure class=&quot;highli
      
    
    </summary>
    
    
      <category term="shadowsocks" scheme="http://batizhao.github.io/tags/shadowsocks/"/>
    
      <category term="privoxy" scheme="http://batizhao.github.io/tags/privoxy/"/>
    
      <category term="http" scheme="http://batizhao.github.io/tags/http/"/>
    
  </entry>
  
  <entry>
    <title>Docker 实战（六）：使用 Docker Compose 搭建 Web 集群</title>
    <link href="http://batizhao.github.io/2016/12/09/docker-six-docker-compose-cluster/"/>
    <id>http://batizhao.github.io/2016/12/09/docker-six-docker-compose-cluster/</id>
    <published>2016-12-09T01:49:43.000Z</published>
    <updated>2018-08-14T02:10:01.723Z</updated>
    
    <content type="html"><![CDATA[<p>在 <a href="http://batizhao.github.io/2016/12/01/docker-four-docker-compose/">实战（四）</a> 中，使用 HAProxy 和 Tomcat 搭建了一个简单的 Tomcat 集群。<br>这节会去掉 Tomcat，使用 Spring Boot 和 MySQL 组成一个比较典型的负载均衡集群。</p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">compose-haproxy-web</span><br><span class="line">- docker-compose.yml</span><br><span class="line">+ haproxy</span><br><span class="line">  - haproxy.cfg</span><br><span class="line">+ web</span><br><span class="line">  - paper-0.0.1-SNAPSHOT.jar</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="docker-compose-yml"><a href="#docker-compose-yml" class="headerlink" title="docker-compose.yml"></a>docker-compose.yml</h2><h3 id="MySQL-配置"><a href="#MySQL-配置" class="headerlink" title="MySQL 配置"></a>MySQL 配置</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">image:</span> <span class="string">batizhao/mysql</span></span><br><span class="line"><span class="attr">environment:</span></span><br><span class="line"><span class="attr">  MYSQL_ROOT_PASSWORD:</span> <span class="string">password</span></span><br><span class="line"><span class="attr">  MYSQL_DATABASE:</span> <span class="string">paper</span></span><br><span class="line"><span class="attr">  MYSQL_ROOT_HOST:</span> <span class="string">"%"</span></span><br><span class="line"><span class="attr">expose:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">"3306"</span></span><br></pre></td></tr></table></figure><p><strong>MYSQL_DATABASE</strong> 需要在启动时创建 paper 数据库。<br><strong>MYSQL_ROOT_HOST</strong> 在 mysql user 中增加客户端远程访问的权限。</p><p>在 batizhao/mysql 这个镜像中，有一个自定义的 my.cnf 文件，主要是定义了 bind-address ，否则会遇到 CommunicationsException: Communications link failure 的错误，还有一些服务端、客户端 UTF-8 编码的定义。</p><h3 id="Web-配置"><a href="#Web-配置" class="headerlink" title="Web 配置"></a>Web 配置</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">image:</span> <span class="string">batizhao/java:8</span></span><br><span class="line"><span class="attr">command:</span> <span class="string">java</span> <span class="bullet">-jar</span> <span class="string">opt/paper.jar</span></span><br><span class="line"><span class="attr">volumes:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">./web/paper-0.0.1-SNAPSHOT.jar:/opt/paper.jar</span></span><br><span class="line"><span class="attr">depends_on:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">"mysql"</span></span><br><span class="line"><span class="attr">links:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">"mysql:database"</span></span><br><span class="line"><span class="attr">expose:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">"8080"</span></span><br><span class="line"><span class="attr">environment:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">SPRING_DATASOURCE_URL=jdbc:mysql://database:3306/paper?useUnicode=true&amp;useSSL=false</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">SPRING_DATASOURCE_USERNAME=root</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">SPRING_DATASOURCE_PASSWORD=password</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">SPRING_DATASOURCE_SQL-SCRIPT-ENCODING=UTF-8</span></span><br></pre></td></tr></table></figure><p><strong>volumes</strong> 这个 Spring Boot 应用在 <a href="https://github.com/batizhao/paper" target="_blank" rel="noopener">Github</a>，可以自己打包放到 web 目录。<br><strong>SPRING_DATASOURCE_SQL-SCRIPT-ENCODING</strong> 这个配置必须要有，否则初始化脚本会插入乱码到数据库。已经确定和 MySQL 无关，因为在宿主机启动 App 直接连接 MySQL 容器，并且在启动后插入中文数据都没有问题。只是在容器中启动 App 初始化时才会乱码，后来改 Web 容器编码也不起作用，加上这个配置就好了。</p><h3 id="全部的-compose-配置"><a href="#全部的-compose-配置" class="headerlink" title="全部的 compose 配置"></a>全部的 compose 配置</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">'2'</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line"><span class="attr">  mysql:</span></span><br><span class="line"><span class="attr">      image:</span> <span class="string">batizhao/mysql</span></span><br><span class="line"><span class="attr">      environment:</span></span><br><span class="line"><span class="attr">        MYSQL_ROOT_PASSWORD:</span> <span class="string">password</span></span><br><span class="line"><span class="attr">        MYSQL_DATABASE:</span> <span class="string">paper</span></span><br><span class="line"><span class="attr">        MYSQL_ROOT_HOST:</span> <span class="string">"%"</span></span><br><span class="line"><span class="attr">      expose:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"3306"</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  weba:</span></span><br><span class="line"><span class="attr">      image:</span> <span class="string">batizhao/java:8</span></span><br><span class="line"><span class="attr">      command:</span> <span class="string">java</span> <span class="bullet">-jar</span> <span class="string">opt/paper.jar</span></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">./web/paper-0.0.1-SNAPSHOT.jar:/opt/paper.jar</span></span><br><span class="line"><span class="attr">      depends_on:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"mysql"</span></span><br><span class="line"><span class="attr">      links:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"mysql:database"</span></span><br><span class="line"><span class="attr">      expose:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"8080"</span></span><br><span class="line"><span class="attr">      environment:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">SPRING_DATASOURCE_URL=jdbc:mysql://database:3306/paper?useUnicode=true&amp;useSSL=false</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">SPRING_DATASOURCE_USERNAME=root</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">SPRING_DATASOURCE_PASSWORD=password</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">SPRING_DATASOURCE_SQL-SCRIPT-ENCODING=UTF-8</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  webb:</span></span><br><span class="line"><span class="attr">      image:</span> <span class="string">batizhao/java:8</span></span><br><span class="line"><span class="attr">      command:</span> <span class="string">java</span> <span class="bullet">-jar</span> <span class="string">opt/paper.jar</span></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">./web/paper-0.0.1-SNAPSHOT.jar:/opt/paper.jar</span></span><br><span class="line"><span class="attr">      depends_on:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"mysql"</span></span><br><span class="line"><span class="attr">      links:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"mysql:database"</span></span><br><span class="line"><span class="attr">      expose:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"8080"</span></span><br><span class="line"><span class="attr">      environment:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">SPRING_DATASOURCE_URL=jdbc:mysql://database:3306/paper?useUnicode=true&amp;useSSL=false</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">SPRING_DATASOURCE_USERNAME=root</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">SPRING_DATASOURCE_PASSWORD=password</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">SPRING_DATASOURCE_SQL-SCRIPT-ENCODING=UTF-8</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  haproxy:</span></span><br><span class="line"><span class="attr">      image:</span> <span class="string">haproxy</span></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">./haproxy:/haproxy-override</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">./haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro</span></span><br><span class="line"><span class="attr">      links:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">weba</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">webb</span></span><br><span class="line"><span class="attr">      ports:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"80:80"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"70:70"</span></span><br><span class="line"><span class="attr">      expose:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"80"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"70"</span></span><br></pre></td></tr></table></figure><h2 id="haproxy-cfg"><a href="#haproxy-cfg" class="headerlink" title="haproxy.cfg"></a>haproxy.cfg</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">global</span><br><span class="line">  log 127.0.0.1local0</span><br><span class="line">  log 127.0.0.1local1 notice</span><br><span class="line">  maxconn 4096</span><br><span class="line">  daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">  logglobal</span><br><span class="line">  modehttp</span><br><span class="line">  optionhttplog</span><br><span class="line">  optiondontlognull</span><br><span class="line">  retries3</span><br><span class="line">  option redispatch</span><br><span class="line">  maxconn2000</span><br><span class="line">  timeout connect5000</span><br><span class="line">  timeout client50000</span><br><span class="line">  timeout server50000</span><br><span class="line"></span><br><span class="line">frontend balancer</span><br><span class="line">  bind 0.0.0.0:80</span><br><span class="line">  mode http</span><br><span class="line">  default_backend servers</span><br><span class="line"></span><br><span class="line">backend servers</span><br><span class="line">  option httpchk OPTIONS /</span><br><span class="line">  option forwardfor</span><br><span class="line">  cookie JSESSIONID prefix</span><br><span class="line">  server tomcat1 weba:8080 cookie JSESSIONID_SERVER_1 check inter 5000</span><br><span class="line">  server tomcat2 webb:8080 cookie JSESSIONID_SERVER_2 check inter 5000</span><br><span class="line"></span><br><span class="line">listen status</span><br><span class="line">  mode http</span><br><span class="line">  default_backend servers</span><br><span class="line">  bind 0.0.0.0:70</span><br><span class="line">  stats enable</span><br><span class="line">  stats hide-version</span><br><span class="line">  stats uri     /stats</span><br><span class="line">  stats auth    admin:password</span><br><span class="line">  stats admin if TRUE</span><br></pre></td></tr></table></figure><h2 id="启动容器"><a href="#启动容器" class="headerlink" title="启动容器"></a>启动容器</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ docker-compose up</span><br><span class="line">Creating network &quot;composehaproxyweb_default&quot; with the default driver</span><br><span class="line">Creating composehaproxyweb_mysql_1</span><br><span class="line">Creating composehaproxyweb_webb_1</span><br><span class="line">Creating composehaproxyweb_weba_1</span><br><span class="line">Creating composehaproxyweb_haproxy_1</span><br><span class="line">Attaching to composehaproxyweb_mysql_1, composehaproxyweb_webb_1, composehaproxyweb_weba_1, composehaproxyweb_haproxy_1</span><br><span class="line">mysql_1    | Initializing database</span><br><span class="line">haproxy_1  | &lt;7&gt;haproxy-systemd-wrapper: executing /usr/local/sbin/haproxy -p /run/haproxy.pid -f /usr/local/etc/haproxy/haproxy.cfg -Ds</span><br><span class="line">mysql_1    | Database initialized</span><br><span class="line">mysql_1    | MySQL init process in progress...</span><br></pre></td></tr></table></figure><p>访问 <a href="http://localhost" target="_blank" rel="noopener">http://localhost</a><br>使用 admin/123456 可以登录系统。</p><p>访问 <a href="http://localhost:70/stats" target="_blank" rel="noopener">http://localhost:70/stats</a><br>使用 admin/password 可以看到集群状态。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 &lt;a href=&quot;http://batizhao.github.io/2016/12/01/docker-four-docker-compose/&quot;&gt;实战（四）&lt;/a&gt; 中，使用 HAProxy 和 Tomcat 搭建了一个简单的 Tomcat 集群。&lt;br&gt;这节会去掉 Tomcat，使用 Spring Boot 和 MySQL 组成一个比较典型的负载均衡集群。&lt;/p&gt;
&lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;compose-haproxy-web&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;- docker-compose.yml&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;+ haproxy&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  - haproxy.cfg&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;+ web&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  - paper-0.0.1-SNAPSHOT.jar&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="docker" scheme="http://batizhao.github.io/categories/docker/"/>
    
    
      <category term="linux" scheme="http://batizhao.github.io/tags/linux/"/>
    
      <category term="mysql" scheme="http://batizhao.github.io/tags/mysql/"/>
    
      <category term="tomcat" scheme="http://batizhao.github.io/tags/tomcat/"/>
    
      <category term="docker" scheme="http://batizhao.github.io/tags/docker/"/>
    
      <category term="haproxy" scheme="http://batizhao.github.io/tags/haproxy/"/>
    
  </entry>
  
  <entry>
    <title>Docker 实战（五）：Docker Swarm Mode</title>
    <link href="http://batizhao.github.io/2016/12/02/docker-five-swarm-mode/"/>
    <id>http://batizhao.github.io/2016/12/02/docker-five-swarm-mode/</id>
    <published>2016-12-01T23:39:15.000Z</published>
    <updated>2018-08-14T02:10:01.727Z</updated>
    
    <content type="html"><![CDATA[<p>在 <a href="http://batizhao.github.io/2016/12/01/docker-four-docker-compose/">Docker Compose</a> 中，我们可以在单台机器上操作多个相关联的 Docker 容器组成负载均衡集群。<br>那如果我们需要一个分布式的环境中，跨多台主机呢？<br>在 Docker 1.12 以上版本中，有个新的东西叫做 <a href="https://docs.docker.com/engine/swarm/" target="_blank" rel="noopener">Swarm Mode</a>。<br>不同于之前版本的 <a href="https://docs.docker.com/swarm/" target="_blank" rel="noopener">Docker Swarm</a>（还需要 pull swarm），Swarm Mode 已经集成在 Docker Engine 中。</p><a id="more"></a><h2 id="主要特性"><a href="#主要特性" class="headerlink" title="主要特性"></a>主要特性</h2><h3 id="内置于-Docker-Engine-的集群管理"><a href="#内置于-Docker-Engine-的集群管理" class="headerlink" title="内置于 Docker Engine 的集群管理"></a>内置于 Docker Engine 的集群管理</h3><p>可以直接用 Docker Engine CLI 来创建 Swarm 集群，并在该集群上部署服务。你不再需要额外的编排软件来创建或管理 Swarm 集群了。</p><h3 id="去中心化设计"><a href="#去中心化设计" class="headerlink" title="去中心化设计"></a>去中心化设计</h3><p>不同于在部署时就确定节点之间的关系, 新的 Swarm 模式选择在运行时动态地处理这些关系, 你可以用 Docker Engine 部署 manager 和 worker 这两种不同的节点。 这意味着你可以从一个磁盘镜像搭建整个 Swarm 。</p><h3 id="声明式服务模型"><a href="#声明式服务模型" class="headerlink" title="声明式服务模型"></a>声明式服务模型</h3><p>Docker Engine 使用一种声明式方法来定义各种服务的状态。譬如，你可以描述一个由 web 前端服务，消息队列服务和数据库后台组成的应用。</p><h3 id="服务扩缩"><a href="#服务扩缩" class="headerlink" title="服务扩缩"></a>服务扩缩</h3><p>你可以通过 docker service scale 命令轻松地增加或减少某个服务的任务数。</p><h3 id="集群状态维护"><a href="#集群状态维护" class="headerlink" title="集群状态维护"></a>集群状态维护</h3><p>Swarm 管理节点会一直监控集群状态，并依据你给出的期望状态与集群真实状态间的区别来进行调节。譬如，你为一个服务设置了10个任务副本，如果某台运行着该服务两个副本的工作节点停止工作了，管理节点会创建两个新的副本来替掉上述异常终止的副本。 Swarm 管理节点这个新的副本分配到了正常运行的工作节点上。</p><h3 id="跨主机网络"><a href="#跨主机网络" class="headerlink" title="跨主机网络"></a>跨主机网络</h3><p>你可以为你的服务指定一个 overlay 网络。在服务初始化或着更新时，Swarm 管理节点自动的为容器在 overlay 网络上分配地址。</p><h3 id="服务发现"><a href="#服务发现" class="headerlink" title="服务发现"></a>服务发现</h3><p>Swarm 管理节点在集群中自动的为每个服务分配唯一的 DNS name 并为容器配置负载均衡。利用内嵌在 Swarm 中的 DNS 服务器你可以找到每个运行在集群中的容器。</p><h3 id="负载均衡"><a href="#负载均衡" class="headerlink" title="负载均衡"></a>负载均衡</h3><p>你可以把服务的端口暴露给一个集群外部的负载均衡器。 在 Swarm 集群内部你可以决定如何在节点间分发服务的容器。</p><h3 id="默认-TLS-加密"><a href="#默认-TLS-加密" class="headerlink" title="默认 TLS 加密"></a>默认 TLS 加密</h3><p>Swarm 集群中的节点间通信是强制加密的。你可以选择使用自签名的根证书或者来自第三方认证的证书。</p><h3 id="滚动更新"><a href="#滚动更新" class="headerlink" title="滚动更新"></a>滚动更新</h3><p>docker service 允许你自定义更新的间隔时间, 并依次更新你的容器, docker 会按照你设置的更新时间依次更新你的容器, 如果发生了错误, 还可以回滚到之前的状态。</p><blockquote><footer><strong>原文链接</strong><cite><a href="https://dataman.gitbooks.io/crane/content/overview/Introduction-of-Docker-Swarm-Mode.html" target="_blank" rel="noopener">Swarm 模式</a></cite></footer></blockquote><h2 id="准备三台主机"><a href="#准备三台主机" class="headerlink" title="准备三台主机"></a>准备三台主机</h2><p>你可以找三台物理主机，如果在单机环境做测试，可以使用 Docker Machine 创建三台 Docker 主机 manager、worker1、worker2。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker-machine create -d virtualbox manager</span><br><span class="line">$ docker-machine create -d virtualbox worker1</span><br><span class="line">$ docker-machine create -d virtualbox worker2</span><br></pre></td></tr></table></figure><blockquote><p>在 Docker for Mac 中，已经不需要 VirtualBox，而是使用 HyperKit，所以这里需要先安装最新版本的 VirtualBox。</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ docker-machine ls</span><br><span class="line">NAME      ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER    ERRORS</span><br><span class="line">manager   -        virtualbox   Running   tcp://192.168.99.101:2376           v1.12.3</span><br><span class="line">worker1   -        virtualbox   Running   tcp://192.168.99.102:2376           v1.12.3</span><br><span class="line">worker2   *        virtualbox   Running   tcp://192.168.99.103:2376           v1.12.3</span><br></pre></td></tr></table></figure><h2 id="创建-Swarm"><a href="#创建-Swarm" class="headerlink" title="创建 Swarm"></a>创建 Swarm</h2><h3 id="确认-manager-节点的-ip-地址"><a href="#确认-manager-节点的-ip-地址" class="headerlink" title="确认 manager 节点的 ip 地址"></a>确认 manager 节点的 ip 地址</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker-machine ip manager</span><br><span class="line">192.168.99.101</span><br></pre></td></tr></table></figure><h3 id="初始化-swarm"><a href="#初始化-swarm" class="headerlink" title="初始化 swarm"></a>初始化 swarm</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ docker swarm init --advertise-addr 192.168.99.101</span><br><span class="line">Swarm initialized: current node (3pniknvvlt9hb5bjpdnwkp5zr) is now a manager.</span><br><span class="line"></span><br><span class="line">To add a worker to this swarm, run the following <span class="built_in">command</span>:</span><br><span class="line"></span><br><span class="line">    docker swarm join \</span><br><span class="line">    --token SWMTKN-1-3gxjh13dqeabze16assdjhng71gy4x7pvu2lqh8fdj22ttg02d-dnkw6tbxbh6so5f0ng7ukiysb \</span><br><span class="line">    192.168.99.101:2377</span><br><span class="line"></span><br><span class="line">To add a manager to this swarm, run <span class="string">'docker swarm join-token manager'</span> and follow the instructions.</span><br></pre></td></tr></table></figure><h3 id="查看-manager-token"><a href="#查看-manager-token" class="headerlink" title="查看 manager token"></a>查看 manager token</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ docker swarm join-token manager</span><br><span class="line">To add a manager to this swarm, run the following <span class="built_in">command</span>:</span><br><span class="line"></span><br><span class="line">    docker swarm join \</span><br><span class="line">    --token SWMTKN-1-3gxjh13dqeabze16assdjhng71gy4x7pvu2lqh8fdj22ttg02d-dnkw6tbxbh6so5f0ng7ukiysb \</span><br><span class="line">    192.168.99.101:2377</span><br></pre></td></tr></table></figure><h3 id="查看-worker-token"><a href="#查看-worker-token" class="headerlink" title="查看 worker token"></a>查看 worker token</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ docker swarm join-token worker</span><br><span class="line">To add a worker to this swarm, run the following <span class="built_in">command</span>:</span><br><span class="line"></span><br><span class="line">    docker swarm join \</span><br><span class="line">    --token SWMTKN-1-3gxjh13dqeabze16assdjhng71gy4x7pvu2lqh8fdj22ttg02d-buqlcaugygyzizgewblrfdqr3 \</span><br><span class="line">    192.168.99.101:2377</span><br></pre></td></tr></table></figure><blockquote><p>这里可以自由选择 manager 还是 worker 类型的节点，示例中以 worker 为例。</p></blockquote><h3 id="查看节点"><a href="#查看节点" class="headerlink" title="查看节点"></a>查看节点</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker node ls</span><br><span class="line">ID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS</span><br><span class="line">3pniknvvlt9hb5bjpdnwkp5zr *  manager   Ready   Active        Leader</span><br></pre></td></tr></table></figure><h2 id="进入节点"><a href="#进入节点" class="headerlink" title="进入节点"></a>进入节点</h2><p>这里有 2 种办法进入 worker1，先新开一个终端窗口</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">eval</span> $(docker-machine env worker1)</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker-machine ssh worker1</span><br></pre></td></tr></table></figure><blockquote><p>这里建议打开 3 个终端窗口，分别执行 $ eval $(docker-machine env <machine-name>) 进入 manager、worker1、worker2 三台主机。</machine-name></p></blockquote><p>进入 worker1 后执行 <strong>查看 worker token</strong> 下的那段命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker swarm join \</span><br><span class="line">  --token SWMTKN-1-3gxjh13dqeabze16assdjhng71gy4x7pvu2lqh8fdj22ttg02d-buqlcaugygyzizgewblrfdqr3 \</span><br><span class="line">  192.168.99.101:2377  </span><br></pre></td></tr></table></figure></p><p>同样的方式进入 worker2 执行上述相同的命令。</p><p>回到 manager 查看节点<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ docker node ls</span><br><span class="line">ID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS</span><br><span class="line">1s6uw7ew22xhvy5trv2ejnncd    worker2   Ready   Active</span><br><span class="line">3pniknvvlt9hb5bjpdnwkp5zr *  manager   Ready   Active        Leader</span><br><span class="line">beh2b7riuqv7oz5nhbhuvmr0t    worker1   Ready   Active</span><br></pre></td></tr></table></figure></p><h2 id="部署服务"><a href="#部署服务" class="headerlink" title="部署服务"></a>部署服务</h2><p>在创建 3 个节点完成后，现在可以在上边部署服务。<br>现在还是回到 manager 节点</p><p>创建服务<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker service create --replicas 1 --name helloworld alpine ping docker.com</span><br><span class="line">doq2uzfwm3c5fukhksv50ewsf</span><br></pre></td></tr></table></figure></p><blockquote><p><strong>name</strong> 指定容器名字<br><strong>replicas</strong> 只复制一个实例<br><strong>alpine</strong> ping docker.com 定义一个 Alpine Linux container 并执行 ping docker.com 命令。</p></blockquote><p>查看服务实例<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker service ls</span><br><span class="line">ID            NAME        REPLICAS  IMAGE        COMMAND</span><br><span class="line">doq2uzfwm3c5  helloworld  1/1       alpine       ping docker.com</span><br></pre></td></tr></table></figure></p><h2 id="检查服务"><a href="#检查服务" class="headerlink" title="检查服务"></a>检查服务</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ docker service inspect --pretty helloworld</span><br><span class="line">ID:doq2uzfwm3c5fukhksv50ewsf</span><br><span class="line">Name:helloworld</span><br><span class="line">Mode:Replicated</span><br><span class="line"> Replicas:1</span><br><span class="line">Placement:</span><br><span class="line">UpdateConfig:</span><br><span class="line"> Parallelism:1</span><br><span class="line"> On failure:pause</span><br><span class="line">ContainerSpec:</span><br><span class="line"> Image:alpine</span><br><span class="line"> Args:ping docker.com</span><br><span class="line">Resources:</span><br></pre></td></tr></table></figure><p>查看服务运行在哪个节点<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker service ps helloworld</span><br><span class="line">ID                         NAME          IMAGE   NODE     DESIRED STATE  CURRENT STATE          ERROR</span><br><span class="line">c81lt2t8ride9ioduqv86i0v7  helloworld.1  alpine  manager  Running        Running 7 minutes ago</span><br></pre></td></tr></table></figure></p><blockquote><p>这里要关注 DESIRED STATE 和 LAST STATE 两个状态。</p></blockquote><p>在服务所在节点查看进程，这里是 manager<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES</span><br><span class="line">458df9db3e9a        alpine:latest       <span class="string">"ping docker.com"</span>        10 minutes ago      Up 10 minutes                           helloworld.1.c81lt2t8ride9ioduqv86i0v7</span><br></pre></td></tr></table></figure></p><h2 id="服务扩展"><a href="#服务扩展" class="headerlink" title="服务扩展"></a>服务扩展</h2><p>还在是 manager 节点，通过命令，我们可以改变集群中的节点实例。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker service scale &lt;SERVICE-ID&gt;=&lt;NUMBER-OF-TASKS&gt;</span><br></pre></td></tr></table></figure><p>在这里，我们把 helloworld 实例扩展到 5 个<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker service scale helloworld=5</span><br><span class="line">helloworld scaled to 5</span><br></pre></td></tr></table></figure></p><p>查看服务实例运行节点分布<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ docker service ps helloworld</span><br><span class="line">ID                         NAME          IMAGE   NODE     DESIRED STATE  CURRENT STATE           ERROR</span><br><span class="line">c81lt2t8ride9ioduqv86i0v7  helloworld.1  alpine  manager  Running        Running 20 minutes ago</span><br><span class="line">b98r4ypet6b9doiof1bb0aq68  helloworld.2  alpine  worker1  Running        Running 5 seconds ago</span><br><span class="line">20hkvi1i3ihur0x17a989qvpy  helloworld.3  alpine  manager  Running        Running 8 seconds ago</span><br><span class="line">472p0ykqnwtjvmo10bx1kutz8  helloworld.4  alpine  worker2  Running        Running 2 seconds ago</span><br><span class="line">dfmx6i4aunk27m8xstavncwxa  helloworld.5  alpine  worker1  Running        Running 2 seconds ago</span><br></pre></td></tr></table></figure></p><p>可以看到，manager 和 worker1 有 2 个实例，worker2 有 1 个实例。</p><p>到 manager 上执行<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED              STATUS              PORTS               NAMES</span><br><span class="line">e90f1e77d7aa        alpine:latest       <span class="string">"ping docker.com"</span>        About a minute ago   Up About a minute                       helloworld.3.20hkvi1i3ihur0x17a989qvpy</span><br><span class="line">458df9db3e9a        alpine:latest       <span class="string">"ping docker.com"</span>        22 minutes ago       Up 22 minutes                           helloworld.1.c81lt2t8ride9ioduqv86i0v7</span><br></pre></td></tr></table></figure></p><p>到 worker1 上执行<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES</span><br><span class="line">806eb838fd86        alpine:latest       <span class="string">"ping docker.com"</span>        2 minutes ago       Up 2 minutes                            helloworld.5.dfmx6i4aunk27m8xstavncwxa</span><br><span class="line">faa01a019dcf        alpine:latest       <span class="string">"ping docker.com"</span>        2 minutes ago       Up 2 minutes                            helloworld.2.b98r4ypet6b9doiof1bb0aq68</span><br></pre></td></tr></table></figure></p><p>到 worker2 上执行<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES</span><br><span class="line">f020134c72bb        alpine:latest       <span class="string">"ping docker.com"</span>        2 minutes ago       Up 2 minutes                            helloworld.4.472p0ykqnwtjvmo10bx1kutz8</span><br></pre></td></tr></table></figure></p><h2 id="删除服务"><a href="#删除服务" class="headerlink" title="删除服务"></a>删除服务</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker service rm helloworld</span><br></pre></td></tr></table></figure><p>确认是否删除<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker service inspect helloworld</span><br><span class="line">[]</span><br><span class="line">Error: no such service: helloworld</span><br></pre></td></tr></table></figure></p><h2 id="滚动更新-1"><a href="#滚动更新-1" class="headerlink" title="滚动更新"></a>滚动更新</h2><p>这里我们会先部署 3 个 Redis 3.0.6 实例到 swarm 节点，然后再更新到 3.0.7。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ docker service create \</span><br><span class="line">  --replicas 3 \</span><br><span class="line">  --name redis \</span><br><span class="line">  --update-delay 10s \</span><br><span class="line">  redis:3.0.6</span><br><span class="line">0knduq4z4vae02wvc33vz5b0u</span><br></pre></td></tr></table></figure><blockquote><p><strong>update-delay</strong> 实例之间的更新延时时间. 可以使用秒 s、分钟 m 或者 小时 h。例如 10m30s 就是延时 10分30秒。<br>默认情况同一时间更新一个实例。可以通过 <strong>–update-parallelism</strong> 配置同时更新的个数。</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ docker service ps redis</span><br><span class="line">ID                         NAME     IMAGE        NODE     DESIRED STATE  CURRENT STATE             ERROR</span><br><span class="line">27mlfg8pqlvz9w4yky1q9fxm7  redis.1  redis:3.0.6  worker1  Running        Running 2 seconds ago</span><br><span class="line">ejlctv6j92caxapd7g2bll0bo  redis.2  redis:3.0.6  manager  Running        Preparing 16 seconds ago</span><br><span class="line">3l0nu4zt99kecwig1sfie1km9  redis.3  redis:3.0.6  worker2  Running        Preparing 16 seconds ago</span><br></pre></td></tr></table></figure><blockquote><p>这里要关注 <strong>CURRENT STATE</strong>，上边的状态说明 worker1 实例已经 Running，但 manager 和 worker2 还在 Preparing。<br>到各自主机上用 <strong>docker ps</strong> 可以证明这一点。等待片刻，所有实例的 <strong>CURRENT STATE</strong> 都变成 Running。</p></blockquote><p>看最后的 Running 时间，第二个节点晚了 7 分钟，第三个节点晚了 16 分钟，三个实例全部启动成功。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ docker service ps redis</span><br><span class="line">ID                         NAME     IMAGE        NODE     DESIRED STATE  CURRENT STATE           ERROR</span><br><span class="line">27mlfg8pqlvz9w4yky1q9fxm7  redis.1  redis:3.0.6  worker1  Running        Running 18 minutes ago</span><br><span class="line">ejlctv6j92caxapd7g2bll0bo  redis.2  redis:3.0.6  manager  Running        Running 2 minutes ago</span><br><span class="line">3l0nu4zt99kecwig1sfie1km9  redis.3  redis:3.0.6  worker2  Running        Running 11 minutes ago</span><br></pre></td></tr></table></figure></p><blockquote><p>如果想看各个 redis 实例的启动日志，你可以 <strong>docker ps</strong> 拿到容器 ID，然后 <strong>docker logs CONTAINER_ID</strong> 看到 redis 的启动日志。<br>如果想知道在最后一个实例启动之前 16 分钟内三个实例发生了什么事情，你需要 <strong>docker-machine ssh NODE_NAME</strong> ，看 /var/log/docker.log 中的内容。</p></blockquote><p>接下来，我们更新 redis 实例到 3.0.7<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker service update --image redis:3.0.7 redis</span><br><span class="line">redis</span><br></pre></td></tr></table></figure></p><p>因为之前已经在各个节点更新过 3.0.7 镜像，省略了下载新镜像的过程，所以这次更新在 2 分钟之内全部完成。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ docker service ps redis</span><br><span class="line">ID                         NAME         IMAGE        NODE     DESIRED STATE  CURRENT STATE                ERROR</span><br><span class="line">cf3stbcfz3zdmw5mgogya6amd  redis.1      redis:3.0.7  manager  Running        Running 2 minutes ago</span><br><span class="line">27mlfg8pqlvz9w4yky1q9fxm7   \_ redis.1  redis:3.0.6  worker1  Shutdown       Shutdown 2 minutes ago</span><br><span class="line">ewv52c73p0klbx6hmofxm72ti  redis.2      redis:3.0.7  worker2  Running        Running about a minute ago</span><br><span class="line">ejlctv6j92caxapd7g2bll0bo   \_ redis.2  redis:3.0.6  manager  Shutdown       Shutdown about a minute ago</span><br><span class="line">4rcaexit4kupwcjrxdnjftgln  redis.3      redis:3.0.7  worker1  Running        Running about a minute ago</span><br><span class="line">3l0nu4zt99kecwig1sfie1km9   \_ redis.3  redis:3.0.6  worker2  Shutdown       Shutdown about a minute ago</span><br></pre></td></tr></table></figure></p><h2 id="拉掉节点"><a href="#拉掉节点" class="headerlink" title="拉掉节点"></a>拉掉节点</h2><p>有时，比如计划维护时间，您需要将节点设置为不可用。 <strong>DRAIN</strong> 可用性防止节点从 swarm 管理器接收新任务。<br>它还意味着管理器停止在节点上运行的任务，并在具有 <strong>ACTIVE</strong> 可用性的节点上启动副本任务。</p><p>拉掉 worker1 节点<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker node update --availability drain worker1</span><br><span class="line">worker1</span><br></pre></td></tr></table></figure></p><p>这时看到 worker1 节点已经 Shutdown，并且在 worker2 上启动了一个新的 redis 实例。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ docker service ps redis</span><br><span class="line">ID                         NAME         IMAGE        NODE     DESIRED STATE  CURRENT STATE            ERROR</span><br><span class="line">cf3stbcfz3zdmw5mgogya6amd  redis.1      redis:3.0.7  manager  Running        Running 13 minutes ago</span><br><span class="line">27mlfg8pqlvz9w4yky1q9fxm7   \_ redis.1  redis:3.0.6  worker1  Shutdown       Shutdown 13 minutes ago</span><br><span class="line">ewv52c73p0klbx6hmofxm72ti  redis.2      redis:3.0.7  worker2  Running        Running 12 minutes ago</span><br><span class="line">ejlctv6j92caxapd7g2bll0bo   \_ redis.2  redis:3.0.6  manager  Shutdown       Shutdown 12 minutes ago</span><br><span class="line">80zf0ykluvhhmydro7egm04iu  redis.3      redis:3.0.7  worker2  Running        Running 17 seconds ago</span><br><span class="line">4rcaexit4kupwcjrxdnjftgln   \_ redis.3  redis:3.0.7  worker1  Shutdown       Shutdown 35 seconds ago</span><br><span class="line">3l0nu4zt99kecwig1sfie1km9   \_ redis.3  redis:3.0.6  worker2  Shutdown       Shutdown 13 minutes ago</span><br></pre></td></tr></table></figure></p><p>恢复 worker1 节点<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker node update --availability active worker1</span><br><span class="line">worker1</span><br></pre></td></tr></table></figure></p><p>查看 worker1 节点状态已经 Active<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ docker node inspect --pretty worker1</span><br><span class="line">ID:beh2b7riuqv7oz5nhbhuvmr0t</span><br><span class="line">Hostname:worker1</span><br><span class="line">Joined at:2016-11-30 09:29:10.681020915 +0000 utc</span><br><span class="line">Status:</span><br><span class="line"> State:Ready</span><br><span class="line"> Availability:Active</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><p>现在 worker1 可以接收新的任务了。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker service scale redis=5</span><br><span class="line">redis scaled to 5</span><br></pre></td></tr></table></figure></p><p>可以看到在 worker1 上启动了 2 个新的 redis 实例<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ docker service ps redis</span><br><span class="line">ID                         NAME         IMAGE        NODE     DESIRED STATE  CURRENT STATE            ERROR</span><br><span class="line">cf3stbcfz3zdmw5mgogya6amd  redis.1      redis:3.0.7  manager  Running        Running 20 minutes ago</span><br><span class="line">27mlfg8pqlvz9w4yky1q9fxm7   \_ redis.1  redis:3.0.6  worker1  Shutdown       Shutdown 20 minutes ago</span><br><span class="line">ewv52c73p0klbx6hmofxm72ti  redis.2      redis:3.0.7  worker2  Running        Running 19 minutes ago</span><br><span class="line">ejlctv6j92caxapd7g2bll0bo   \_ redis.2  redis:3.0.6  manager  Shutdown       Shutdown 19 minutes ago</span><br><span class="line">80zf0ykluvhhmydro7egm04iu  redis.3      redis:3.0.7  worker2  Running        Running 7 minutes ago</span><br><span class="line">4rcaexit4kupwcjrxdnjftgln   \_ redis.3  redis:3.0.7  worker1  Shutdown       Shutdown 7 minutes ago</span><br><span class="line">3l0nu4zt99kecwig1sfie1km9   \_ redis.3  redis:3.0.6  worker2  Shutdown       Shutdown 20 minutes ago</span><br><span class="line">79dffeo1l7etwc731b5lgrgac  redis.4      redis:3.0.7  worker1  Running        Running 12 seconds ago</span><br><span class="line">5eyb4lhp16m9n2ob2bkifpeqw  redis.5      redis:3.0.7  worker1  Running        Running 12 seconds ago</span><br></pre></td></tr></table></figure></p><h2 id="路由网络-routing-mesh"><a href="#路由网络-routing-mesh" class="headerlink" title="路由网络 routing mesh"></a>路由网络 routing mesh</h2><p>Docker Swarm Mode 可以发布服务端口，使其可用于群外的资源。<br>所有节点都加入路由网络，路由网络使得每个节点都能够接受已发布端口上的连接，即使节点上没有任何服务正在运行。<br>路由网络将所有传入的请求路由到正在运行服务的节点上。</p><p>为了在群中使用入口网络，您需要在群集节点之间打开以下端口：</p><ul><li>Port 7946 TCP/UDP</li><li>Port 4789 UDP</li></ul><h3 id="发布一个对外端口"><a href="#发布一个对外端口" class="headerlink" title="发布一个对外端口"></a>发布一个对外端口</h3><p>发布端口使用以下命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ docker service create \</span><br><span class="line">  --name &lt;SERVICE-NAME&gt; \</span><br><span class="line">  --publish &lt;PUBLISHED-PORT&gt;:&lt;TARGET-PORT&gt; \</span><br><span class="line">  &lt;IMAGE&gt;</span><br></pre></td></tr></table></figure></p><p>这里以 nginx 为例，为了演示，这里只用了 2 个 nginx 实例<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ docker service create \</span><br><span class="line">  --name my-web \</span><br><span class="line">  --publish 8080:80 \</span><br><span class="line">  --replicas 2 \</span><br><span class="line">  nginx</span><br><span class="line">d3192apcq7hharl4kpzl0eqqg</span><br></pre></td></tr></table></figure></p><p>确认服务已经启动<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ docker service ps my-web</span><br><span class="line">ID                         NAME      IMAGE  NODE     DESIRED STATE  CURRENT STATE               ERROR</span><br><span class="line">a90s5hnb58dck8cxny4k2xgdk  my-web.1  nginx  worker1  Running        Running 2 minutes ago</span><br><span class="line">2148yi9va70eu27xl6nfi9une  my-web.2  nginx  worker2  Running        Running about a minute ago</span><br></pre></td></tr></table></figure></p><p>这时我们分别访问三个节点，都可以看到 nginx 首页。</p><ul><li><a href="http://192.168.99.101:8080" target="_blank" rel="noopener">http://192.168.99.101:8080</a></li><li><a href="http://192.168.99.102:8080" target="_blank" rel="noopener">http://192.168.99.102:8080</a></li><li><a href="http://192.168.99.103:8080" target="_blank" rel="noopener">http://192.168.99.103:8080</a></li></ul><p>架构如下（官方图）<br><img src="/images/ingress-routing-mesh.png" alt=""></p><p>如果你要发布一个新的端口<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker service update \</span><br><span class="line">  --publish-add &lt;PUBLISHED-PORT&gt;:&lt;TARGET-PORT&gt; \</span><br><span class="line">  my-web</span><br></pre></td></tr></table></figure></p><h3 id="配置一个负载均衡器"><a href="#配置一个负载均衡器" class="headerlink" title="配置一个负载均衡器"></a>配置一个负载均衡器</h3><p>前边实现了节点中 service 的负载均衡。<br>我们可以在 Swarm Load Balance 之前再加一层负载均衡器，实现节点之间的负载均衡。<br>具体的实现这里就不再描述，可以自己编写 haproxy.cfg 和 Dockerfile 构建 Docker HAProxy 镜像。</p><p>架构如下（官方图）<br><img src="/images/ingress-lb.png" alt=""></p><p>-EOF-</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 &lt;a href=&quot;http://batizhao.github.io/2016/12/01/docker-four-docker-compose/&quot;&gt;Docker Compose&lt;/a&gt; 中，我们可以在单台机器上操作多个相关联的 Docker 容器组成负载均衡集群。&lt;br&gt;那如果我们需要一个分布式的环境中，跨多台主机呢？&lt;br&gt;在 Docker 1.12 以上版本中，有个新的东西叫做 &lt;a href=&quot;https://docs.docker.com/engine/swarm/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Swarm Mode&lt;/a&gt;。&lt;br&gt;不同于之前版本的 &lt;a href=&quot;https://docs.docker.com/swarm/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Docker Swarm&lt;/a&gt;（还需要 pull swarm），Swarm Mode 已经集成在 Docker Engine 中。&lt;/p&gt;
    
    </summary>
    
      <category term="docker" scheme="http://batizhao.github.io/categories/docker/"/>
    
    
      <category term="linux" scheme="http://batizhao.github.io/tags/linux/"/>
    
      <category term="docker" scheme="http://batizhao.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>Docker 实战（四）：Docker Compose</title>
    <link href="http://batizhao.github.io/2016/12/01/docker-four-docker-compose/"/>
    <id>http://batizhao.github.io/2016/12/01/docker-four-docker-compose/</id>
    <published>2016-12-01T05:31:47.000Z</published>
    <updated>2016-12-05T02:49:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>之前都是单个 Docker 容器，现在，基于 Docker Compose，你可以同时控制多个相关联的 Docker 容器。<br>比如典型的 Nginx + Tomcat + MySQL 的 Web 架构，只需要几个简单的配置，敲击几个命令，原来可能需要好几个小时的工作，现在几分钟就可以搞定。</p><a id="more"></a><p>使用 Compose 主要由以下三步组成：</p><ul><li>定义 Dockerfile；</li><li>定义 docker-compose.yml；</li><li>运行 docker-compose up 启动所有容器。</li></ul><p>这里会构造一个 HAProxy + 两个 Tomcat 负载均衡的架构。</p><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">compose-haproxy-web</span><br><span class="line">- docker-compose.yml</span><br><span class="line">+ haproxy</span><br><span class="line">  - haproxy.cfg</span><br><span class="line">+ web</span><br><span class="line">  - Dockerfile</span><br><span class="line">  - index.html</span><br></pre></td></tr></table></figure><h2 id="docker-compose-yml"><a href="#docker-compose-yml" class="headerlink" title="docker-compose.yml"></a>docker-compose.yml</h2><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">weba:</span></span><br><span class="line"><span class="attr">    build:</span> <span class="string">./web</span></span><br><span class="line"><span class="attr">    expose:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="number">8080</span></span><br><span class="line"></span><br><span class="line"><span class="attr">webb:</span></span><br><span class="line"><span class="attr">    build:</span> <span class="string">./web</span></span><br><span class="line"><span class="attr">    expose:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="number">8080</span></span><br><span class="line"></span><br><span class="line"><span class="attr">haproxy:</span></span><br><span class="line"><span class="attr">    image:</span> <span class="attr">haproxy:latest</span></span><br><span class="line"><span class="attr">    volumes:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">./haproxy:/haproxy-override</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">./haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro</span></span><br><span class="line"><span class="attr">    links:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">weba</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">webb</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"80:80"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"70:70"</span></span><br><span class="line"><span class="attr">    expose:</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"80"</span></span><br><span class="line"><span class="bullet">        -</span> <span class="string">"70"</span></span><br></pre></td></tr></table></figure><h2 id="haproxy-cfg"><a href="#haproxy-cfg" class="headerlink" title="haproxy.cfg"></a>haproxy.cfg</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">global</span><br><span class="line">  log 127.0.0.1local0</span><br><span class="line">  log 127.0.0.1local1 notice</span><br><span class="line">  maxconn 4096</span><br><span class="line">  daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">  logglobal</span><br><span class="line">  modehttp</span><br><span class="line">  optionhttplog</span><br><span class="line">  optiondontlognull</span><br><span class="line">  retries3</span><br><span class="line">  option redispatch</span><br><span class="line">  maxconn2000</span><br><span class="line">  timeout connect5000</span><br><span class="line">  timeout client50000</span><br><span class="line">  timeout server50000</span><br><span class="line"></span><br><span class="line">frontend balancer</span><br><span class="line">  bind 0.0.0.0:80</span><br><span class="line">  mode http</span><br><span class="line">  default_backend servers</span><br><span class="line"></span><br><span class="line">backend servers</span><br><span class="line">  option httpchk OPTIONS /</span><br><span class="line">  option forwardfor</span><br><span class="line">  cookie JSESSIONID prefix</span><br><span class="line">  server tomcat1 weba:8080 cookie JSESSIONID_SERVER_1 check inter 5000</span><br><span class="line">  server tomcat2 webb:8080 cookie JSESSIONID_SERVER_2 check inter 5000</span><br><span class="line"></span><br><span class="line">listen status</span><br><span class="line">  mode http</span><br><span class="line">  default_backend servers</span><br><span class="line">  bind 0.0.0.0:70</span><br><span class="line">  stats enable</span><br><span class="line">  stats hide-version</span><br><span class="line">  stats uri     /stats</span><br><span class="line">  stats auth    admin:password</span><br><span class="line">  stats admin if TRUE</span><br></pre></td></tr></table></figure><h2 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">FROM batizhao/java:8</span><br><span class="line"></span><br><span class="line"># 创建者信息</span><br><span class="line">MAINTAINER batizhao &lt;zhaobati@gmail.com&gt;</span><br><span class="line"></span><br><span class="line"># Install dependencies</span><br><span class="line">RUN apt-get update &amp;&amp; \</span><br><span class="line">    apt-get install -y wget tar &amp;&amp; \</span><br><span class="line">    apt-get clean &amp;&amp; \</span><br><span class="line">    rm -rf /var/lib/apt/lists/*</span><br><span class="line"></span><br><span class="line"># Define commonly used JAVA_HOME variable</span><br><span class="line">ENV TOMCAT_VERSION 8.5.8</span><br><span class="line"></span><br><span class="line"># Get Tomcat</span><br><span class="line">RUN wget --no-cookies http://ftp.jaist.ac.jp/pub/apache/tomcat/tomcat-8/v$&#123;TOMCAT_VERSION&#125;/bin/apache-tomcat-$&#123;TOMCAT_VERSION&#125;.tar.gz -O /tmp/tomcat.tgz &amp;&amp; \</span><br><span class="line">    tar xzvf /tmp/tomcat.tgz -C /opt &amp;&amp; \</span><br><span class="line">    mv /opt/apache-tomcat-$&#123;TOMCAT_VERSION&#125; /opt/tomcat &amp;&amp; \</span><br><span class="line">    rm /tmp/tomcat.tgz &amp;&amp; \</span><br><span class="line">    rm -rf /opt/tomcat/webapps/examples &amp;&amp; \</span><br><span class="line">    rm -rf /opt/tomcat/webapps/docs &amp;&amp; \</span><br><span class="line">    rm -rf /opt/tomcat/webapps/manager &amp;&amp; \</span><br><span class="line">    rm -rf /opt/tomcat/webapps/host-manager &amp;&amp; \</span><br><span class="line">    rm -rf /opt/tomcat/webapps/ROOT/*</span><br><span class="line"></span><br><span class="line">ADD index.html /opt/tomcat/webapps/ROOT</span><br><span class="line"></span><br><span class="line">ENV CATALINA_HOME /opt/tomcat</span><br><span class="line">ENV PATH $PATH:$CATALINA_HOME/bin</span><br><span class="line"></span><br><span class="line">EXPOSE 8080</span><br><span class="line">WORKDIR /opt/tomcat</span><br><span class="line"></span><br><span class="line"># Launch Tomcat</span><br><span class="line">CMD [&quot;/opt/tomcat/bin/catalina.sh&quot;, &quot;run&quot;]</span><br></pre></td></tr></table></figure><h2 id="index-html"><a href="#index-html" class="headerlink" title="index.html"></a>index.html</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hello, Docker.</span><br></pre></td></tr></table></figure><h2 id="启动容器"><a href="#启动容器" class="headerlink" title="启动容器"></a>启动容器</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ docker-compose up</span><br><span class="line">Creating composehaproxyweb_webb_1</span><br><span class="line">Creating composehaproxyweb_weba_1</span><br><span class="line">Creating composehaproxyweb_haproxy_1</span><br><span class="line">Attaching to composehaproxyweb_weba_1, composehaproxyweb_webb_1, composehaproxyweb_haproxy_1</span><br><span class="line">haproxy_1  | &lt;7&gt;haproxy-systemd-wrapper: executing /usr/local/sbin/haproxy -p /run/haproxy.pid -f /usr/local/etc/haproxy/haproxy.cfg -Ds</span><br><span class="line">webb_1     | 30-Nov-2016 12:50:21.524 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version:        Apache Tomcat/8.5.8</span><br><span class="line">weba_1     | 30-Nov-2016 12:50:21.531 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version:        Apache Tomcat/8.5.8</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>访问 <a href="http://localhost" target="_blank" rel="noopener">http://localhost</a><br>可以看到 Hello, Docker.</p><p>访问 <a href="http://localhost:70/stats" target="_blank" rel="noopener">http://localhost:70/stats</a><br><img src="/images/2016-12-01-docker-four-docker-compose-haproxy.png" alt=""></p><h2 id="后台运行"><a href="#后台运行" class="headerlink" title="后台运行"></a>后台运行</h2><p>增加 -d 参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ docker-compose up -d</span><br><span class="line">Starting composehaproxyweb_webb_1</span><br><span class="line">Starting composehaproxyweb_weba_1</span><br><span class="line">Starting composehaproxyweb_haproxy_1</span><br></pre></td></tr></table></figure><p>查看 compose 进程<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ docker-compose ps</span><br><span class="line">           Name                          Command               State                   Ports</span><br><span class="line">-------------------------------------------------------------------------------------------------------------</span><br><span class="line">composehaproxyweb_haproxy_1   /docker-entrypoint.sh hapr ...   Up      0.0.0.0:70-&gt;70/tcp, 0.0.0.0:80-&gt;80/tcp</span><br><span class="line">composehaproxyweb_weba_1      /opt/tomcat/bin/catalina.s ...   Up      8080/tcp</span><br><span class="line">composehaproxyweb_webb_1      /opt/tomcat/bin/catalina.s ...   Up      8080/tcp</span><br></pre></td></tr></table></figure></p><p>在 weba 上执行命令，比如 env</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">$ docker-compose run weba env</span><br><span class="line">PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/tomcat/bin</span><br><span class="line">HOSTNAME=4fd738ffc9b2</span><br><span class="line">TERM=xterm</span><br><span class="line">COMPOSEHAPROXYWEB_WEBA_1_PORT=tcp://172.17.0.2:8080</span><br><span class="line">COMPOSEHAPROXYWEB_WEBA_1_PORT_8080_TCP=tcp://172.17.0.2:8080</span><br><span class="line">COMPOSEHAPROXYWEB_WEBA_1_PORT_8080_TCP_ADDR=172.17.0.2</span><br><span class="line">COMPOSEHAPROXYWEB_WEBA_1_PORT_8080_TCP_PORT=8080</span><br><span class="line">COMPOSEHAPROXYWEB_WEBA_1_PORT_8080_TCP_PROTO=tcp</span><br><span class="line">COMPOSEHAPROXYWEB_WEBA_1_NAME=/composehaproxyweb_weba_run_1/composehaproxyweb_weba_1</span><br><span class="line">COMPOSEHAPROXYWEB_WEBA_1_ENV_TOMCAT_VERSION=8.5.8</span><br><span class="line">COMPOSEHAPROXYWEB_WEBA_1_ENV_CATALINA_HOME=/opt/tomcat</span><br><span class="line">WEBA_PORT=tcp://172.17.0.2:8080</span><br><span class="line">WEBA_PORT_8080_TCP=tcp://172.17.0.2:8080</span><br><span class="line">WEBA_PORT_8080_TCP_ADDR=172.17.0.2</span><br><span class="line">WEBA_PORT_8080_TCP_PORT=8080</span><br><span class="line">WEBA_PORT_8080_TCP_PROTO=tcp</span><br><span class="line">WEBA_NAME=/composehaproxyweb_weba_run_1/weba</span><br><span class="line">WEBA_ENV_TOMCAT_VERSION=8.5.8</span><br><span class="line">WEBA_ENV_CATALINA_HOME=/opt/tomcat</span><br><span class="line">WEBA_1_PORT=tcp://172.17.0.2:8080</span><br><span class="line">WEBA_1_PORT_8080_TCP=tcp://172.17.0.2:8080</span><br><span class="line">WEBA_1_PORT_8080_TCP_ADDR=172.17.0.2</span><br><span class="line">WEBA_1_PORT_8080_TCP_PORT=8080</span><br><span class="line">WEBA_1_PORT_8080_TCP_PROTO=tcp</span><br><span class="line">WEBA_1_NAME=/composehaproxyweb_weba_run_1/weba_1</span><br><span class="line">WEBA_1_ENV_TOMCAT_VERSION=8.5.8</span><br><span class="line">WEBA_1_ENV_CATALINA_HOME=/opt/tomcat</span><br><span class="line">TOMCAT_VERSION=8.5.8</span><br><span class="line">CATALINA_HOME=/opt/tomcat</span><br><span class="line">HOME=/root</span><br></pre></td></tr></table></figure><p>停止后台进程</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ docker-compose stop</span><br><span class="line">Stopping composehaproxyweb_haproxy_1 ... done</span><br><span class="line">Stopping composehaproxyweb_webb_1 ... done</span><br><span class="line">Stopping composehaproxyweb_weba_1 ... done</span><br></pre></td></tr></table></figure><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><h3 id="No-such-image"><a href="#No-such-image" class="headerlink" title="No such image"></a>No such image</h3><p>这是缓存造成的一个问题，可能由于你直接手动删除了 compose 生成的 image。<br>你可以通过 <strong>docker-compose ps</strong> 查看，也通过 <strong>docker-compose rm</strong> 直接清除。<br>想要避免这个问题，你需要 <strong>docker-compose down</strong>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前都是单个 Docker 容器，现在，基于 Docker Compose，你可以同时控制多个相关联的 Docker 容器。&lt;br&gt;比如典型的 Nginx + Tomcat + MySQL 的 Web 架构，只需要几个简单的配置，敲击几个命令，原来可能需要好几个小时的工作，现在几分钟就可以搞定。&lt;/p&gt;
    
    </summary>
    
      <category term="docker" scheme="http://batizhao.github.io/categories/docker/"/>
    
    
      <category term="linux" scheme="http://batizhao.github.io/tags/linux/"/>
    
      <category term="docker" scheme="http://batizhao.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>Docker 实战（三）：Docker 常用命令</title>
    <link href="http://batizhao.github.io/2016/11/24/docker-three-command/"/>
    <id>http://batizhao.github.io/2016/11/24/docker-three-command/</id>
    <published>2016-11-24T08:52:03.000Z</published>
    <updated>2017-08-03T04:51:37.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="images-查看镜像"><a href="#images-查看镜像" class="headerlink" title="images 查看镜像"></a>images 查看镜像</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ docker images</span><br><span class="line"></span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">tomcat              latest              c6cfe59eb987        6 days ago          357 MB</span><br><span class="line">batizhao/ubuntu     latest              a15b93298276        9 days ago          297.2 MB</span><br><span class="line">ubuntu              16.04               f753707788c5        5 weeks ago         127.2 MB</span><br><span class="line">ubuntu              latest              f753707788c5        5 weeks ago         127.2 MB</span><br><span class="line">swarm               latest              942fd5fd357e        3 months ago        19.47 MB</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="ps-查看容器"><a href="#ps-查看容器" class="headerlink" title="ps 查看容器"></a>ps 查看容器</h2><p>运行中的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps</span><br><span class="line"></span><br><span class="line">CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                     NAMES</span><br><span class="line">e6eef58b3544        tomcat              <span class="string">"catalina.sh run"</span>   16 minutes ago      Up 14 minutes       0.0.0.0:32770-&gt;8080/tcp   evil_wozniak</span><br></pre></td></tr></table></figure><p>所有的</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps -a</span><br><span class="line"></span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                        PORTS                     NAMES</span><br><span class="line">e6eef58b3544        tomcat              <span class="string">"catalina.sh run"</span>        17 minutes ago      Up 15 minutes                 0.0.0.0:32770-&gt;8080/tcp   evil_wozniak</span><br><span class="line">d4faf2e9badf        tomcat              <span class="string">"catalina.sh run"</span>        35 minutes ago      Exited (130) 26 minutes ago                             ecstatic_bohr</span><br><span class="line">47d97451e8a1        tomcat              <span class="string">"catalina.sh run"</span>        36 minutes ago      Exited (130) 35 minutes ago                             berserk_mclean</span><br><span class="line">54c65a764bf0        batizhao/ubuntu     <span class="string">"/usr/bin/supervisord"</span>   7 days ago          Exited (0) 7 days ago                                   jolly_allen</span><br><span class="line">9246f78c4b8f        batizhao/ubuntu     <span class="string">"/usr/bin/supervisord"</span>   7 days ago          Exited (0) 7 days ago                                   stupefied_gates</span><br><span class="line">d49845097aa0        batizhao/ubuntu     <span class="string">"/usr/bin/supervisord"</span>   7 days ago          Exited (0) 7 days ago                                   sad_stallman</span><br><span class="line">311ba7bb9ffd        swarm               <span class="string">"/swarm --help"</span>          7 days ago          Exited (0) 7 days ago                                   naughty_bhaskara</span><br></pre></td></tr></table></figure><h2 id="run-运行容器"><a href="#run-运行容器" class="headerlink" title="run 运行容器"></a>run 运行容器</h2><p>启动 MySQL Docker 容器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ docker run -itd -e MYSQL_ROOT_PASSWORD=passw0rd -e MYSQL_ROOT_HOST=% -e MYSQL_DATABASE=iwamp2 -p 3306:3306 --name=mysql batizhao/mysql:latest</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Tomcat Docker 容器连接宿主机 MySQL</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ docker run -itd -p 9000:8080 --name iwamp -v /Users/batizhao/Downloads/web:/opt/tomcat/webapps --add-host=<span class="string">'iwamp2.dev:172.17.0.1'</span> batizhao/tomcat:8-jre8</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Tomcat Docker 容器连接 MySQL Docker 容器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ docker run -it -p 9000:8080 --name iwamp2 -v /Users/batizhao/Downloads/web:/opt/tomcat/webapps --link mysql:iwamp2.dev batizhao/tomcat:8-jre8</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Tomcat Docker 容器连接物理 Oracle 数据库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ docker run -itd -p 9000:8080 --name iwamp -v /Users/batizhao/Downloads/web:/opt/tomcat/webapps --add-host=<span class="string">'iwamp.dev:172.31.21.216'</span> batizhao/tomcat:8-jre8</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="rm-删除容器"><a href="#rm-删除容器" class="headerlink" title="rm 删除容器"></a>rm 删除容器</h2><p>删除单个容器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker rm 47d97451e8a1</span><br><span class="line"></span><br><span class="line">47d97451e8a1</span><br></pre></td></tr></table></figure><p>删除所有停止运行的容器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker rm $(docker ps -a -q)</span><br></pre></td></tr></table></figure><h2 id="rmi-删除镜像"><a href="#rmi-删除镜像" class="headerlink" title="rmi 删除镜像"></a>rmi 删除镜像</h2><p>删除 REPOSITORY 为 swarm，TAG 为 latest 的镜像</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker rmi swarm</span><br><span class="line"></span><br><span class="line">Error response from daemon: conflict: unable to remove repository reference <span class="string">"swarm"</span> (must force) - container 311ba7bb9ffd is using its referenced image 942fd5fd357e</span><br></pre></td></tr></table></figure><p>如果提示以上错误，可以先删除所关联容器，或者直接使用 -f 参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ docker rmi -f swarm</span><br><span class="line"></span><br><span class="line">Untagged: swarm:latest</span><br><span class="line">Untagged: swarm@sha256:c9e1b4d4e399946c0542accf30f9a73500d6b0b075e152ed1c792214d3509d70</span><br><span class="line">Deleted: sha256:942fd5fd357e2fe2fcecbaf3dd77c313f22ce18a84a5a4d288c0df407a61e623</span><br></pre></td></tr></table></figure><p>批量删除 tag 为 none 的镜像</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ docker images|grep none|awk <span class="string">'&#123;print $3&#125;'</span>|xargs docker rmi -f</span><br><span class="line"></span><br><span class="line">Deleted: sha256:d43fcd0c191c0fa7ae1df73ea59ed374a5e9b5c25788ddc4183800257cc8a38f</span><br><span class="line">Deleted: sha256:59e448d53f303721ca12a40513c3dbc8651cc046311186825b9d7eec0805baac</span><br></pre></td></tr></table></figure><h2 id="port-查看端口映射"><a href="#port-查看端口映射" class="headerlink" title="port 查看端口映射"></a>port 查看端口映射</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker port e6eef58b3544 8080</span><br><span class="line">0.0.0.0:32770</span><br></pre></td></tr></table></figure><h2 id="exec-进入运行中的容器"><a href="#exec-进入运行中的容器" class="headerlink" title="exec 进入运行中的容器"></a>exec 进入运行中的容器</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker <span class="built_in">exec</span> -it e6eef58b3544 bash</span><br><span class="line"></span><br><span class="line">root@e6eef58b3544:/usr/<span class="built_in">local</span>/tomcat<span class="comment">#</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;images-查看镜像&quot;&gt;&lt;a href=&quot;#images-查看镜像&quot; class=&quot;headerlink&quot; title=&quot;images 查看镜像&quot;&gt;&lt;/a&gt;images 查看镜像&lt;/h2&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ docker images&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;tomcat              latest              c6cfe59eb987        6 days ago          357 MB&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;batizhao/ubuntu     latest              a15b93298276        9 days ago          297.2 MB&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;ubuntu              16.04               f753707788c5        5 weeks ago         127.2 MB&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;ubuntu              latest              f753707788c5        5 weeks ago         127.2 MB&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;swarm               latest              942fd5fd357e        3 months ago        19.47 MB&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="docker" scheme="http://batizhao.github.io/categories/docker/"/>
    
    
      <category term="linux" scheme="http://batizhao.github.io/tags/linux/"/>
    
      <category term="docker" scheme="http://batizhao.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>Docker 实战（二）：基于 Dockerfile 构建基础镜像</title>
    <link href="http://batizhao.github.io/2016/11/15/dokcer-two-dockerfile/"/>
    <id>http://batizhao.github.io/2016/11/15/dokcer-two-dockerfile/</id>
    <published>2016-11-15T04:10:16.000Z</published>
    <updated>2016-11-15T07:37:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>这里使用 Dockerfile 构建一个 Ubuntu 的基础镜像，并且安装了 OpenSSH、Supervisor 等基础服务。使用 Supervisor 可以更好的同时控制多个我们希望执行的程序。</p><p>最新的 Docker 版本建议安装 <a href="https://www.docker.com" target="_blank" rel="noopener">Docker for Mac</a>，原先的 Docker Toolbox 方式已经不建议使用。</p><a id="more"></a><h2 id="创建-Dockerfile-脚本"><a href="#创建-Dockerfile-脚本" class="headerlink" title="创建 Dockerfile 脚本"></a>创建 Dockerfile 脚本</h2><figure class="highlight bash"><figcaption><span>Dockerfile</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Docker file for ubuntu 16</span></span><br><span class="line">FROM ubuntu:16.04</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建者信息</span></span><br><span class="line">MAINTAINER batizhao &lt;zhaobati@gmail.com&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用阿里云镜像</span></span><br><span class="line">RUN sed -i <span class="string">'s/archive.ubuntu.com/mirrors.aliyun.com/g'</span> /etc/apt/sources.list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装基础工具包</span></span><br><span class="line">RUN \</span><br><span class="line">  apt-get update &amp;&amp; \</span><br><span class="line">  apt-get -y upgrade &amp;&amp; \</span><br><span class="line">  apt-get install -y vim openssh-server supervisor &amp;&amp; \</span><br><span class="line">  rm -rf /var/lib/apt/lists/*</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 ssh 和 supervisor 目录</span></span><br><span class="line">RUN mkdir -p /var/run/sshd</span><br><span class="line">RUN mkdir -p /var/<span class="built_in">log</span>/supervisor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 root ssh 远程登录密码为 password</span></span><br><span class="line">RUN <span class="built_in">echo</span> <span class="string">"root:password"</span> | chpasswd</span><br><span class="line">RUN sed -i <span class="string">'s/PermitRootLogin prohibit-password/PermitRootLogin yes/'</span> /etc/ssh/sshd_config</span><br><span class="line">RUN sed <span class="string">'s@session\s*required\s*pam_loginuid.so@session optional pam_loginuid.so@g'</span> -i /etc/pam.d/sshd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加 supervisor 的配置文件</span></span><br><span class="line">COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开放 SSH 22 端口</span></span><br><span class="line">EXPOSE 22</span><br><span class="line"></span><br><span class="line"><span class="comment"># 容器启动命令</span></span><br><span class="line">CMD [<span class="string">"/usr/bin/supervisord"</span>]</span><br></pre></td></tr></table></figure><h2 id="创建-Supervisor-脚本"><a href="#创建-Supervisor-脚本" class="headerlink" title="创建 Supervisor 脚本"></a>创建 Supervisor 脚本</h2><figure class="highlight plain"><figcaption><span>在 Dockerfile 文件所在目录创建 supervisord.conf</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[supervisord]</span><br><span class="line">nodaemon=true</span><br><span class="line"></span><br><span class="line">[program:sshd]</span><br><span class="line">command=/usr/sbin/sshd -D</span><br></pre></td></tr></table></figure><blockquote><p>在执行以下步骤之前，需要先启动 Docker。</p></blockquote><h2 id="使用-Dockerfile-构建镜像并运行"><a href="#使用-Dockerfile-构建镜像并运行" class="headerlink" title="使用 Dockerfile 构建镜像并运行"></a>使用 Dockerfile 构建镜像并运行</h2><figure class="highlight bash"><figcaption><span>在 Dockerfile 文件所在目录执行命令</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker build -t batizhao/ubuntu .</span><br></pre></td></tr></table></figure><figure class="highlight bash"><figcaption><span>启动 Docker 容器</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -p 2200:22 -it batizhao/ubuntu</span><br></pre></td></tr></table></figure><figure class="highlight bash"><figcaption><span>宿主机做为 SSH 客户端</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh root@localhost -p 2200</span><br></pre></td></tr></table></figure><h2 id="提交到仓库"><a href="#提交到仓库" class="headerlink" title="提交到仓库"></a>提交到仓库</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker push batizhao/ubuntu</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这里使用 Dockerfile 构建一个 Ubuntu 的基础镜像，并且安装了 OpenSSH、Supervisor 等基础服务。使用 Supervisor 可以更好的同时控制多个我们希望执行的程序。&lt;/p&gt;
&lt;p&gt;最新的 Docker 版本建议安装 &lt;a href=&quot;https://www.docker.com&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Docker for Mac&lt;/a&gt;，原先的 Docker Toolbox 方式已经不建议使用。&lt;/p&gt;
    
    </summary>
    
      <category term="docker" scheme="http://batizhao.github.io/categories/docker/"/>
    
    
      <category term="linux" scheme="http://batizhao.github.io/tags/linux/"/>
    
      <category term="docker" scheme="http://batizhao.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>Docker 实战（一）：在 Mac 上安装配置 Docker</title>
    <link href="http://batizhao.github.io/2016/11/14/docker-one-install/"/>
    <id>http://batizhao.github.io/2016/11/14/docker-one-install/</id>
    <published>2016-11-14T09:19:58.000Z</published>
    <updated>2016-11-15T07:37:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="在安装之前"><a href="#在安装之前" class="headerlink" title="在安装之前"></a>在安装之前</h2><p>最新的 Docker 版本建议安装 <a href="https://www.docker.com" target="_blank" rel="noopener">Docker for Mac</a>，原先的 Docker Toolbox 方式已经不建议使用。</p><blockquote><p>If you already have an installation of Docker Toolbox, please read these topics first to learn how Docker for Mac and Docker Toolbox differ, and how they can coexist.</p><footer><strong>新老两种方法对比以及 Toolbox 的卸载</strong><cite><a href="https://docs.docker.com/docker-for-mac/docker-toolbox/" target="_blank" rel="noopener">Docker for Mac vs. Docker Toolbox</a></cite></footer></blockquote><a id="more"></a><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><blockquote><p>Welcome to Docker for Mac!</p><p>Please read through these topics on how to get started. To give us feedback on your experience with the app and report bugs or problems, log in to our Docker for Mac forum.</p><footer><strong>基本的安装步骤</strong><cite><a href="https://docs.docker.com/docker-for-mac/" target="_blank" rel="noopener">Get started with Docker for Mac</a></cite></footer></blockquote><h2 id="使用镜像库加速"><a href="#使用镜像库加速" class="headerlink" title="使用镜像库加速"></a>使用镜像库加速</h2><p>国内访问 Docker 仓库会比较慢，可以使用 2 个加速地址：</p><ul><li><a href="https://cr.console.aliyun.com" target="_blank" rel="noopener">阿里云</a></li><li><a href="https://www.daocloud.io/mirror" target="_blank" rel="noopener">DaoCloud</a></li></ul><p>注册并登录以上地址可以获得专属加速地址。</p><blockquote><p>在 Mac 上，点击桌面顶栏的 Docker 图标，选择 Preferences ，在 Advanced 标签下的 Registry mirrors 列表中加入专属地址。<br>点击 Apply &amp; Restart 按钮使设置生效。</p></blockquote><h2 id="第一个镜像"><a href="#第一个镜像" class="headerlink" title="第一个镜像"></a>第一个镜像</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ docker pull ubuntu</span><br><span class="line">Using default tag: latest</span><br><span class="line">latest: Pulling from library/ubuntu</span><br><span class="line">Digest: sha256:2d44ae143feeb36f4c898d32ed2ab2dffeb3a573d2d8928646dfc9cb7deb1315</span><br><span class="line">Status: Image is up to date <span class="keyword">for</span> ubuntu:latest</span><br></pre></td></tr></table></figure><p>启动 ubuntu 容器</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -it ubuntu</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;在安装之前&quot;&gt;&lt;a href=&quot;#在安装之前&quot; class=&quot;headerlink&quot; title=&quot;在安装之前&quot;&gt;&lt;/a&gt;在安装之前&lt;/h2&gt;&lt;p&gt;最新的 Docker 版本建议安装 &lt;a href=&quot;https://www.docker.com&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Docker for Mac&lt;/a&gt;，原先的 Docker Toolbox 方式已经不建议使用。&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;If you already have an installation of Docker Toolbox, please read these topics first to learn how Docker for Mac and Docker Toolbox differ, and how they can coexist.&lt;/p&gt;
&lt;footer&gt;&lt;strong&gt;新老两种方法对比以及 Toolbox 的卸载&lt;/strong&gt;&lt;cite&gt;&lt;a href=&quot;https://docs.docker.com/docker-for-mac/docker-toolbox/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Docker for Mac vs. Docker Toolbox&lt;/a&gt;&lt;/cite&gt;&lt;/footer&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="docker" scheme="http://batizhao.github.io/categories/docker/"/>
    
    
      <category term="linux" scheme="http://batizhao.github.io/tags/linux/"/>
    
      <category term="docker" scheme="http://batizhao.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>edX 学习四：制作课程</title>
    <link href="http://batizhao.github.io/2014/04/18/building-a-course/"/>
    <id>http://batizhao.github.io/2014/04/18/building-a-course/</id>
    <published>2014-04-17T16:00:00.000Z</published>
    <updated>2016-11-04T09:11:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>前边提到，主要有两种方式制作课程：Studio 和 LMS(+Github)。这里对这两种方式做简单的说明。</p><h2 id="Studio"><a href="#Studio" class="headerlink" title="Studio"></a>Studio</h2><p>如果取得制作课程的权限，第一次登录的时候是这样的</p><p><img src="/images/2014-04-18-building-a-course-edx4edx_lite-Studio-1.png" alt=""></p><p>填入课程具体名称、组织、代码（因为需要根据这些信息生成 URL，所以要注意长度和不能包含特殊字符、空格等）</p><p><img src="/images/2014-04-18-building-a-course-edx4edx_lite-Studio-2.png" alt=""></p><a id="more"></a><p>只能看到自己通过 Studio 制作的课程</p><p><img src="/images/2014-04-18-building-a-course-edx4edx_lite-Studio-4.png" alt=""></p><p>edX 的工作人员可以看到所有通过 Studio 制作的课程</p><p><img src="/images/2014-04-18-building-a-course-edx4edx_lite-Studio-3.png" alt=""></p><p>创建成功后进入 Course Outline 页面，可以使用 Checklists 完成课程的制作。</p><p><img src="/images/2014-04-18-building-a-course-edx4edx_lite-Studio-6.png" alt=""></p><p>Add Course Team Members 团队管理员可以添加或删除团队成员，或授予管理员权限给其他团队成员。<br>其他团队成员可以编辑课程。</p><p><img src="/images/2014-04-18-building-a-course-edx4edx_lite-Studio-7.png" alt=""></p><p>Course Outline 对课程内容进行制作</p><p><img src="/images/2014-04-18-building-a-course-edx4edx_lite-Studio-9.png" alt=""></p><p>其它的功能还有：</p><ul><li>课程导入导出</li><li>学生视图（设置课程概览模板、图片、视频、开始结束日期、学生登记日期）</li><li>设置功课类型、分数范围、级别和打分策略</li></ul><p><img src="/images/2014-04-18-building-a-course-edx4edx_lite-Studio-10.png" alt=""></p><p>这些都可以查看 Checklists 中的 Explore edX’s Support Tools 章节。</p><h2 id="LMS-Github"><a href="#LMS-Github" class="headerlink" title="LMS(+Github)"></a>LMS(+Github)</h2><p>环境中的示例课程 <a href="https://github.com/mitocw/edx4edx_lite" target="_blank" rel="noopener">edx4edx_lite</a> 的所有内容通过 Github 管理，下边是此课程的目录结构。</p><p><img src="/images/2014-04-18-building-a-course-edx4edx_lite.png" alt=""></p><p>首先是 course.xml，内容相当于点击“New Course”填写的内容</p><pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;&lt;course url_name=&quot;edx4edx&quot; org=&quot;MITx&quot; course=&quot;edx4edx&quot;/&gt;</code></pre><p>course 目录下的课程同名 edx4edx.xml 文件相当于 Course Outline 的顶级目录</p><pre><code>&lt;course graceperiod=&quot;1 day 5 hours 59 minutes 59 seconds&quot; org=&quot;MITx&quot; course=&quot;edx4edx&quot; ispublic=&quot;True&quot; semester=&quot;edx4edx&quot;&gt;  &lt;chapter url_name=&quot;Introduction_chapter&quot;/&gt;  &lt;chapter url_name=&quot;Assessment_Problems_chapter&quot;/&gt;  &lt;chapter url_name=&quot;Author_tools_chapter&quot;/&gt;&lt;/course&gt;</code></pre><p>Introduction_chapter.xml</p><pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;&lt;chapter display_name=&quot;Introduction&quot;&gt;  &lt;sequential url_name=&quot;edx4edx_Course_sequential&quot;/&gt;&lt;/chapter&gt;</code></pre><p>Assessment_Problems_chapter.xml</p><pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;&lt;chapter display_name=&quot;Assessment Problems&quot;&gt;  &lt;sequential url_name=&quot;Sample_Problems_sequential&quot;/&gt;  &lt;sequential url_name=&quot;Advanced_Problems_Custom_Response_and_Randomization_sequential&quot;/&gt;  &lt;sequential url_name=&quot;Advanced_Problems_Hints_sequential&quot;/&gt;  &lt;sequential url_name=&quot;Advanced_Problems_Scripts_and_Javascript_sequential&quot;/&gt;  &lt;sequential url_name=&quot;Advanced_Problems_Code_Grading_sequential&quot;/&gt;  &lt;sequential url_name=&quot;Rich_Interface_Examples&quot;/&gt;&lt;/chapter&gt;    </code></pre><p>edx4edx_Course_sequential.xml</p><pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;&lt;sequential format=&quot;&quot; Due=&quot;Dec 12-25&quot; display_name=&quot;edx4edx Course&quot;&gt;  &lt;vertical url_name=&quot;edx4edx_Course_vertical&quot;/&gt;&lt;/sequential&gt;</code></pre><p>edx4edx_Course_vertical.xml</p><pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;&lt;vertical display_name=&quot;edx4edx_Course_vertical&quot;&gt;  &lt;html url_name=&quot;edx4edx_Course_html&quot;/&gt;&lt;/vertical&gt;        </code></pre><p>整个层级关系是 course - chapter - sequential - problem(or vertical) - html</p><p><img src="/images/2014-04-18-building-a-course-edx4edx_lite-Studio-11.png" alt="">    </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前边提到，主要有两种方式制作课程：Studio 和 LMS(+Github)。这里对这两种方式做简单的说明。&lt;/p&gt;
&lt;h2 id=&quot;Studio&quot;&gt;&lt;a href=&quot;#Studio&quot; class=&quot;headerlink&quot; title=&quot;Studio&quot;&gt;&lt;/a&gt;Studio&lt;/h2&gt;&lt;p&gt;如果取得制作课程的权限，第一次登录的时候是这样的&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/2014-04-18-building-a-course-edx4edx_lite-Studio-1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;填入课程具体名称、组织、代码（因为需要根据这些信息生成 URL，所以要注意长度和不能包含特殊字符、空格等）&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/2014-04-18-building-a-course-edx4edx_lite-Studio-2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="mooc" scheme="http://batizhao.github.io/categories/mooc/"/>
    
    
      <category term="mooc" scheme="http://batizhao.github.io/tags/mooc/"/>
    
      <category term="edx" scheme="http://batizhao.github.io/tags/edx/"/>
    
  </entry>
  
  <entry>
    <title>edX 学习三：开始课程制作之前</title>
    <link href="http://batizhao.github.io/2014/04/15/edx-course-management/"/>
    <id>http://batizhao.github.io/2014/04/15/edx-course-management/</id>
    <published>2014-04-14T16:00:00.000Z</published>
    <updated>2016-11-15T07:25:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>在制作课程之前，先要知道下 <a href="https://www.edx.org/" target="_blank" rel="noopener">edX.org</a> 和 <a href="https://edge.edx.org" target="_blank" rel="noopener">edX Edge</a> 的关系。</p><h2 id="edX-org-和-edX-Edge"><a href="#edX-org-和-edX-Edge" class="headerlink" title="edX.org 和 edX Edge"></a>edX.org 和 edX Edge</h2><p>从外观来看，这两个站点几乎是一样的，但内容和目的不同。这是两个完全独立的站点，包括所有用户信息、课程数据、数据库、服务器都是独立的。现在有很多的课程都即将在 Edge 毕业，在将来成为 edX 上的正式公开课程。但也有很多课程不需要经过 Edge，而直接在 edX 上开课。官方建议在 edX 上正式开课前，先在 Edge 上做一些测试和学习。</p><a id="more"></a><h2 id="edX-org"><a href="#edX-org" class="headerlink" title="edX.org"></a>edX.org</h2><p>edX.org 的在线课程来自 edX 合作伙伴。在和 edX 签定协议之后，可以在 edX.org 上发布公开课程，并开放给来自世界各地的学生。课程通过 studio.edx.org 管理。</p><p>edX.org</p><p><img src="/images/2014-04-15-edx-course-management-edX-org.png" alt=""></p><p>studio.edx.org</p><p><img src="/images/2014-04-15-edx-course-management-studio-edx-org.png" alt=""></p><h2 id="edX-Edge"><a href="#edX-Edge" class="headerlink" title="edX Edge"></a>edX Edge</h2><p>Edge 是一个私有内容站点。在这里课程不是公开的，没有课程目录，也不能被搜索引擎索引，只有受到明确邀请并且知道具体的课程 URL 才可以访问课程。课程通过 studio.edge.edx.org 管理。  </p><p>没有任何公开课程，需要先注册登录</p><p><img src="/images/2014-04-15-edx-course-management-edX-Edge.png" alt=""></p><p>注册激活，登录以后看不到任何课程内容，没有课程目录，也没有 “Find Courses” 按钮</p><p><img src="/images/2014-04-15-edx-course-management-edX-Edge-home.png" alt=""></p><p>只能通过某个 URL，例如官方提供的 <a href="https://edge.edx.org/courses/edX/edX101/How_to_Create_an_edX_Course/about" target="_blank" rel="noopener">edX101</a> 注册课程</p><p><img src="/images/2014-04-15-edx-course-management-edX-Edge-home-101.png" alt=""></p><p>studio.edge.edx.org，和 edx.org 不同的是这里不需要给 edx 发邮件，只需要点击“Request the Ability to Create Courses”等待审核通过（需要提供大学或者课程相关的名字）。</p><p><img src="/images/2014-04-15-edx-course-management-studio-edge-edx-org.png" alt=""></p><h2 id="制作课程"><a href="#制作课程" class="headerlink" title="制作课程"></a>制作课程</h2><ul><li>LMS（+ Github） – 这种方式结合 Github ，可以直接编辑课程相关 XML 文件，实现课程的版本控制，保留课程修改纪录。通过一个 webhook 实现课程的自动更新。上篇提到的 <a href="https://github.com/mitocw/edx4edx_lite" target="_blank" rel="noopener">edx4edx_lite</a> 就是这种方式。</li><li>Studio – Studio 是用于构建课程的 edX 工具。这是一种可视化、所见即所得的编辑方式，基于 Web 界面。可以使用 Studio 为学生来创建课程内容、问题、视频和其他资源。使用 Studio，可以管理日程安排和课程团队、设置分级策略、发布课程，等等。可以直接通过浏览器使用的 Studio，不需要任何额外的软件。但只能单人编辑工作、没有课程修改纪录。课程内容数据存储在 MongoDB。</li></ul><h2 id="使用-LaTeX-制作课程"><a href="#使用-LaTeX-制作课程" class="headerlink" title="使用 LaTeX 制作课程"></a>使用 LaTeX 制作课程</h2><p>LaTeX 是一个强大的排版系统，广泛应用于数学、科技、工程等学术领域（本篇 Markdown 格式内容转换成 PDF 时也需要使用到 LaTeX 模板），非常适合于制作一些科学图表、数学公式，很多学术论文都使用了此系统。 MIT 开源了 <a href="https://people.csail.mit.edu/ichuang/edx/latex2edx.php" target="_blank" rel="noopener">latex2edx</a>，使用 LaTeX 来制作整个或部分 edX 课程。</p><p>latex2edx 已被用来生产许多 MITX 上 edX 课程，包括 8.01x、8.02x、16.101x 等。它可以通过一个 TEX 模板，同时生成一个在线课程，以及一个 PDF 文件。这种其实是第一种方式。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在制作课程之前，先要知道下 &lt;a href=&quot;https://www.edx.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;edX.org&lt;/a&gt; 和 &lt;a href=&quot;https://edge.edx.org&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;edX Edge&lt;/a&gt; 的关系。&lt;/p&gt;
&lt;h2 id=&quot;edX-org-和-edX-Edge&quot;&gt;&lt;a href=&quot;#edX-org-和-edX-Edge&quot; class=&quot;headerlink&quot; title=&quot;edX.org 和 edX Edge&quot;&gt;&lt;/a&gt;edX.org 和 edX Edge&lt;/h2&gt;&lt;p&gt;从外观来看，这两个站点几乎是一样的，但内容和目的不同。这是两个完全独立的站点，包括所有用户信息、课程数据、数据库、服务器都是独立的。现在有很多的课程都即将在 Edge 毕业，在将来成为 edX 上的正式公开课程。但也有很多课程不需要经过 Edge，而直接在 edX 上开课。官方建议在 edX 上正式开课前，先在 Edge 上做一些测试和学习。&lt;/p&gt;
    
    </summary>
    
      <category term="mooc" scheme="http://batizhao.github.io/categories/mooc/"/>
    
    
      <category term="mooc" scheme="http://batizhao.github.io/tags/mooc/"/>
    
      <category term="edx" scheme="http://batizhao.github.io/tags/edx/"/>
    
  </entry>
  
  <entry>
    <title>edX 学习二：搭建 edX 平台</title>
    <link href="http://batizhao.github.io/2014/04/14/quick-start-to-working-with-the-edx-platform/"/>
    <id>http://batizhao.github.io/2014/04/14/quick-start-to-working-with-the-edx-platform/</id>
    <published>2014-04-13T16:00:00.000Z</published>
    <updated>2016-11-04T09:10:54.000Z</updated>
    
    <content type="html"><![CDATA[<p>折腾了好几天，试过官方的 <a href="https://github.com/edx/configuration/wiki/edX-Developer-Stack" target="_blank" rel="noopener">edX Developer Stack</a> 和 <a href="https://github.com/edx/configuration/wiki/edX-Ubuntu-12.04-Installation" target="_blank" rel="noopener">edX Ubuntu 12.04 installation</a> 两种方法没成功之后，终于按照 <a href="https://people.csail.mit.edu/ichuang/edx" target="_blank" rel="noopener">Quick Start to working with the edX Platform</a> 这篇文章搞定，缺点就是不是最新的 edX 环境。</p><p>按照官方文档，在安装时主要会卡在两个地方（主要还是网络问题）：</p><ul><li>安装 MongoDB（这个地方时间长点还可以执行下去，大概 10 几个小时）</li></ul><p><img src="/images/2014-04-14-quick-start-to-working-with-the-edx-platform-1.png" alt=""></p><ul><li>安装 ORA（花了一天一夜也没执行下去）</li></ul><p><img src="/images/2014-04-14-quick-start-to-working-with-the-edx-platform-2.png" alt=""></p><p>最后这两种方法都放弃，使用了直接下载 box 的方式。就是 box 文件下载需要点时间，我家里 20M 光纤用了 4 个小时左右。</p><a id="more"></a><h1 id="使用-MITxVM-distribution-安装"><a href="#使用-MITxVM-distribution-安装" class="headerlink" title="使用 MITxVM distribution 安装"></a>使用 MITxVM distribution 安装</h1><p>The MITxVM box is built on a base Ubuntu 12.04LTS distribution. The edX platform runs using django/python, and is served via gunicorn and nginx. Virtual Box is used to provide a host-only network, 192.168.42.*. The four edX services listen on eth0 on four separate IP addresses. The system uses mysql for the main database, and mongo for Studio.        </p><p>Installed repos include edx-platform, xqueue, xserver, latex2edx, edx-ora, ease.</p><h2 id="1-安装-Vagrant-和-VirtualBox"><a href="#1-安装-Vagrant-和-VirtualBox" class="headerlink" title="1. 安装 Vagrant 和 VirtualBox"></a>1. 安装 Vagrant 和 VirtualBox</h2><ul><li><a href="http://www.vagrantup.com/" target="_blank" rel="noopener">Vagrant</a></li><li><a href="https://www.virtualbox.org/" target="_blank" rel="noopener">VirtualBox</a></li></ul><h2 id="2-下载示例课程-edx4edx-lite-到-data-目录"><a href="#2-下载示例课程-edx4edx-lite-到-data-目录" class="headerlink" title="2. 下载示例课程 edx4edx_lite 到 data 目录"></a>2. 下载示例课程 edx4edx_lite 到 data 目录</h2><pre><code># mkdir mitx-vagrant# cd mitx-vagrant# mkdir data# cd data# git clone https://github.com/mitocw/edx4edx_lite.git</code></pre><h2 id="3-在-mitx-vagrant-启动虚拟机。"><a href="#3-在-mitx-vagrant-启动虚拟机。" class="headerlink" title="3. 在 mitx-vagrant 启动虚拟机。"></a>3. 在 mitx-vagrant 启动虚拟机。</h2><p>下载 <a href="https://people.csail.mit.edu/ichuang/edx/download.php?file=mitxvm-edx-platform-02sep13a.box" target="_blank" rel="noopener">mitxvm-edx-platform-02sep13a.box</a> (large file: 3.8 GB) </p><blockquote><p>md5sum: 7d3671a92f8ba4f8e6b54db91a90bc91</p></blockquote><pre><code># vagrant init mitxvm mitxvm-edx-platform-02sep13a.box # vagrant up...==&gt; default: Configuring and enabling network interfaces...==&gt; default: Mounting shared folders...    default: /vagrant =&gt; /opt/mitx-vagrant==&gt; default: VM already provisioned. Run `vagrant provision` or use `--provision` to force it    </code></pre><h2 id="4-访问站点"><a href="#4-访问站点" class="headerlink" title="4. 访问站点"></a>4. 访问站点</h2><ul><li><a href="http://192.168.42.2" target="_blank" rel="noopener">http://192.168.42.2</a> – LMS（www.edx.org，这个环境没有找到 studio.edx.org 对应的站点）</li><li><a href="http://192.168.42.2/admin" target="_blank" rel="noopener">http://192.168.42.2/admin</a> – LMS 后台管理，需要通过下边的 setstaff 命令激活访问账号</li><li><a href="http://192.168.42.5" target="_blank" rel="noopener">http://192.168.42.5</a> – Edge（edge.edx.org）</li><li><a href="http://192.168.42.3" target="_blank" rel="noopener">http://192.168.42.3</a> – Edge Studio (studio.edge.edx.org) </li><li><a href="http://192.168.42.4" target="_blank" rel="noopener">http://192.168.42.4</a> – Preview    </li></ul><p>登录账号: </p><ul><li>email “xadmin@mitxvm.local”，password “xadmin”。</li><li>如果要创建用户，激活用户需要使用 “xmanage” 命令，否则无法收到激活邮件。</li><li>登录以后，可以看到 edx4edx_lite 这个示例课程。</li></ul><h2 id="5-虚拟机管理"><a href="#5-虚拟机管理" class="headerlink" title="5. 虚拟机管理"></a>5. 虚拟机管理</h2><p>MITx virtual machine Vagrant box 使用 <a href="https://github.com/mitocw/xmanage" target="_blank" rel="noopener">xmanage</a>：</p><pre><code># vagrant ssh -- xmanage helpommands available:restart-lms      - restart the LMS (for vagrant boxes, running at http://192.168.42.2)                   This will force re-loading of course datarestart-cms      - restart the CMS (aka the Studio system)restart-edge     - restart the Edge server (part of the Studio system)restart-preview  - restart the Preview server (part of the Studio system)restart-xqueue   - restart the xqueue main systemrestart-consumer - restart the xqueue consumerrestart-xserver  - restart the xserver (python code grader)logs &lt;appname&gt;   - view last 100 lines of log file for &lt;appname&gt;                   appname should be one of lms, cms, edge, preview, xserver, xqueueactivate &lt;user&gt;  - activate user specified by username &lt;user&gt;setstaff &lt;user&gt;  - make user (specified by username &lt;user&gt;) into staffupdate-mitx      - update mitx system code (use with care!)update           - update this management script (from central repo)help             - print out this message, as well as local NOTES.txt file</code></pre><h2 id="5-问题"><a href="#5-问题" class="headerlink" title="5. 问题"></a>5. 问题</h2><ul><li><p>Vagrant error : Failed to mount folders in Linux guest</p><pre><code># vagrant ssh# sudo ln -s /opt/VBoxGuestAdditions-4.3.10/lib/VBoxGuestAdditions /usr/lib/VBoxGuestAdditions# vagrant reload</code></pre></li><li><p>Unknown command: ‘activate_user’        </p><p>  这里发现 xmanage activate 命令不可用，其它命令正常。在 <a href="https://groups.google.com/forum/#!topic/edx-code/mmFqv6688GQ" target="_blank" rel="noopener">Google Groups</a> 上看到这个问题，但没解决。如果遇到以下错误，这里可以通过 admin 登录来激活。</p><pre><code>$ xmanage activate batizhaoactivating user batizhaoUnknown command: &apos;activate_user&apos;Type &apos;django-admin.py help&apos; for usage.To complete the activation, please logout then log back in</code></pre></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;折腾了好几天，试过官方的 &lt;a href=&quot;https://github.com/edx/configuration/wiki/edX-Developer-Stack&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;edX Developer Stack&lt;/a&gt; 和 &lt;a href=&quot;https://github.com/edx/configuration/wiki/edX-Ubuntu-12.04-Installation&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;edX Ubuntu 12.04 installation&lt;/a&gt; 两种方法没成功之后，终于按照 &lt;a href=&quot;https://people.csail.mit.edu/ichuang/edx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Quick Start to working with the edX Platform&lt;/a&gt; 这篇文章搞定，缺点就是不是最新的 edX 环境。&lt;/p&gt;
&lt;p&gt;按照官方文档，在安装时主要会卡在两个地方（主要还是网络问题）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;安装 MongoDB（这个地方时间长点还可以执行下去，大概 10 几个小时）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/images/2014-04-14-quick-start-to-working-with-the-edx-platform-1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;安装 ORA（花了一天一夜也没执行下去）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/images/2014-04-14-quick-start-to-working-with-the-edx-platform-2.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;最后这两种方法都放弃，使用了直接下载 box 的方式。就是 box 文件下载需要点时间，我家里 20M 光纤用了 4 个小时左右。&lt;/p&gt;
    
    </summary>
    
      <category term="mooc" scheme="http://batizhao.github.io/categories/mooc/"/>
    
    
      <category term="mooc" scheme="http://batizhao.github.io/tags/mooc/"/>
    
      <category term="edx" scheme="http://batizhao.github.io/tags/edx/"/>
    
  </entry>
  
</feed>

<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[使用 Gluster FS 做 Kubernetes 持久化存储]]></title>
    <url>%2F2018%2F02%2F08%2Finstalling-glusterfs%2F</url>
    <content type="text"><![CDATA[安装 Gluster FS1234567891011121314151617181920# 先安装 gluster 源$ yum install centos-release-gluster -y# 安装 glusterfs 组件$ yum install -y glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma glusterfs-geo-replication glusterfs-devel# 创建 glusterfs 目录$ mkdir /opt/glusterd# 修改 glusterd 目录$ sed -i 's/var\/lib/opt/g' /etc/glusterfs/glusterd.vol# 启动 glusterfs$ systemctl start glusterd.service# 设置开机启动$ systemctl enable glusterd.service#查看状态$ systemctl status glusterd.service 配置 Gluster FS1234567891011121314151617181920212223242526272829303132333435# 配置 hosts$ vi /etc/hosts172.31.21.208 gfs01.isoftone.com 172.31.21.209 gfs02.isoftone.com 172.31.21.210 gfs03.isoftone.com172.31.21.211 gfs04.isoftone.com# 开放端口$ iptables -I INPUT -p tcp --dport 24007 -j ACCEPT# 创建存储目录$ mkdir /opt/gfs_data# 添加节点到 集群# 执行操作的本机不需要probe 本机[root@sz-pg-oam-docker-test-001 ~]#gluster peer probe gfs02.isoftone.comgluster peer probe gfs03.isoftone.comgluster peer probe gfs04.isoftone.com# 查看集群状态$ gluster peer statusNumber of Peers: 3Hostname: gfs02.isoftone.comUuid: 0697cfd7-4c88-4d29-b907-59894b158df7State: Peer in Cluster (Connected)Hostname: gfs03.isoftone.comUuid: 7754d301-00ac-4c28-ac22-4d5f89d2d1d3State: Peer in Cluster (Connected)Hostname: gfs04.isoftone.comUuid: 7b33777c-0d3b-45f9-acbd-4f8d09da6431State: Peer in Cluster (Connected) 配置 Gluster FS VolumeGlusterFS中的volume的模式有很多中，包括以下几种： 分布卷（默认模式）：即DHT, 也叫 分布卷: 将文件已hash算法随机分布到 一台服务器节点中存储。 复制模式：即AFR, 创建volume 时带 replica x 数量: 将文件复制到 replica x 个节点中。 条带模式：即Striped, 创建volume 时带 stripe x 数量： 将文件切割成数据块，分别存储到 stripe x 个节点中 ( 类似raid 0 )。 分布式条带模式：最少需要4台服务器才能创建。 创建volume 时 stripe 2 server = 4 个节点： 是DHT 与 Striped 的组合型。 分布式复制模式：最少需要4台服务器才能创建。 创建volume 时 replica 2 server = 4 个节点：是DHT 与 AFR 的组合型。 条带复制卷模式：最少需要4台服务器才能创建。 创建volume 时 stripe 2 replica 2 server = 4 个节点： 是 Striped 与 AFR 的组合型。 三种模式混合： 至少需要8台 服务器才能创建。 stripe 2 replica 2 , 每4个节点 组成一个 组。 这几种模式的示例图参考：CentOS7安装GlusterFS。 因为我们只有四台主机，在此我们使用默认的分布式复制模式。 123456789101112131415161718192021222324# 创建分布卷$ gluster volume create k8s-volume replica 2 transport tcp gfs01.isoftone.com:/opt/gfs_data gfs02.isoftone.com:/opt/gfs_data gfs03.isoftone.com:/opt/gfs_data gfs04.isoftone.com:/opt/gfs_data force# 查看volume状态$ gluster volume infoVolume Name: k8s-volumeType: Distributed-ReplicateVolume ID: f5438a2b-22b3-4608-9a45-ed9fa0028d38Status: CreatedSnapshot Count: 0Number of Bricks: 2 x 2 = 4Transport-type: tcpBricks:Brick1: gfs01.isoftone.com:/opt/gfs_dataBrick2: gfs02.isoftone.com:/opt/gfs_dataBrick3: gfs03.isoftone.com:/opt/gfs_dataBrick4: gfs04.isoftone.com:/opt/gfs_dataOptions Reconfigured:transport.address-family: inetnfs.disable: onperformance.client-io-threads: off# 启动 k8s-volume 卷$ gluster volume start k8s-volume 调优 Gluster FS1234567891011121314151617# 开启 指定 volume 的配额$ gluster volume quota k8s-volume enable# 限制 指定 volume 的配额$ gluster volume quota k8s-volume limit-usage / 1TB# 设置 cache 大小, 默认32MB$ gluster volume set k8s-volume performance.cache-size 4GB# 设置 io 线程, 太大会导致进程崩溃$ gluster volume set k8s-volume performance.io-thread-count 16# 设置 网络检测时间, 默认42s$ gluster volume set k8s-volume network.ping-timeout 10# 设置 写缓冲区的大小, 默认1M$ gluster volume set k8s-volume performance.write-behind-window-size 1024MB 在 Kubernetes 使用 Gluster FS安装 Gluster FS 客户端123456789# 在所有 k8s node 中安装 glusterfs 客户端$ yum install -y glusterfs glusterfs-fuse# 配置 hosts$ vi /etc/hosts172.31.21.208 gfs01.isoftone.com 172.31.21.209 gfs02.isoftone.com 172.31.21.210 gfs03.isoftone.com172.31.21.211 gfs04.isoftone.com 创建 endpoints1234567$ curl -O https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/glusterfs/glusterfs-endpoints.yaml# 导入 glusterfs-pod.json$ kubectl apply -f glusterfs-endpoints.yaml$ kubectl get epNAME ENDPOINTS AGEglusterfs-cluster 172.31.21.208:1,172.31.21.209:1,172.31.21.210:1 + 1 more... 59m 创建 PersistentVolume12345$ curl -O https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/glusterfs/glusterfs-pv.yaml$ kubectl apply -f glusterfs-pvc.yaml$ kubectl get pv | grep glusglusterfs-pv 8Gi RWX Retain Bound default/glusterfs-pvc 55m 创建 PersistentVolumeClaim12345$ curl -O https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/glusterfs/glusterfs-pvc.yaml$ kubectl apply -f glusterfs-pv.yaml$ kubectl get pvc | grep glusglusterfs-pvc Bound glusterfs-pv 8Gi RWX 56m 创建测试 nginx 挂载 volume123456789101112131415161718192021222324252627282930313233$ curl -O https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/glusterfs/nginx-deployment.yaml$ kubectl apply -f nginx-deployment.yaml$ kubectl get deploy,podNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEdeploy/nginx-dm 2 2 2 2 54mNAME READY STATUS RESTARTS AGEpo/nginx-dm-5d49ccb7f5-h62dc 1/1 Running 0 54mpo/nginx-dm-5d49ccb7f5-szsfg 1/1 Running 0 54m# 登陆 k8s node 物理机，使用 df 可查看挂载目录$ df -h | grep glusterfs172.31.21.208:k8s-volume 560G 8.7G 552G 2% /var/lib/kubelet/pods/c403020c-0ca8-11e8-ace7-000c29c217e5/volumes/kubernetes.io~glusterfs/glusterfs-pv# 查看挂载$ kubectl exec -it nginx-dm-5d49ccb7f5-h62dc -- df -h|grep k8s-volume172.31.21.208:k8s-volume 560G 8.7G 552G 2% /usr/share/nginx/html# 创建文件 测试$ kubectl exec -it nginx-dm-5d49ccb7f5-h62dc -- touch /usr/share/nginx/html/index.html$ kubectl exec -it nginx-dm-5d49ccb7f5-h62dc -- ls -lt /usr/share/nginx/html/index.html-rw-r--r-- 1 root root 0 Feb 8 09:24 /usr/share/nginx/html/index.html# 验证 glusterfs# 因为我们使用分布复制卷 replica 2，所以可以看到有 2 个节点中有文件[root@glusterfs-01 gfs_data]# lsindex.html[root@glusterfs-02 gfs_data]# lsindex.html[root@glusterfs-03 gfs_data]# ls[root@glusterfs-04 gfs_data]# ls 参考 使用glusterfs做持久化存储]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>kubernetes</tag>
        <tag>glusterfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 Mac 上通过 Minikube 安装本地 K8s 集群]]></title>
    <url>%2F2018%2F01%2F18%2FRunning-Kubernetes-Locally-via-Minikube%2F</url>
    <content type="text"><![CDATA[准备工作安装 xhyve 驱动程序。123$ brew install docker-machine-driver-xhyve$ sudo chown root:wheel $(brew --prefix)/opt/docker-machine-driver-xhyve/bin/docker-machine-driver-xhyve$ sudo chmod u+s $(brew --prefix)/opt/docker-machine-driver-xhyve/bin/docker-machine-driver-xhyve 安装 kubectl12345678$ curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/darwin/amd64/kubectl$ chmod +x ./kubectl$ sudo mv ./kubectl /usr/local/bin/kubectl$ kubectl versionClient Version: version.Info&#123;Major:"1", Minor:"9", GitVersion:"v1.9.1", GitCommit:"3a1c9449a956b6026f075fa3134ff92f7d55f812", GitTreeState:"clean", BuildDate:"2018-01-04T11:52:23Z", GoVersion:"go1.9.2", Compiler:"gc", Platform:"darwin/amd64"&#125;Server Version: version.Info&#123;Major:"1", Minor:"8", GitVersion:"v1.8.0", GitCommit:"0b9efaeb34a2fc51ff8e4d34ad9bc6375459c4a4", GitTreeState:"clean", BuildDate:"2017-11-29T22:43:34Z", GoVersion:"go1.9.1", Compiler:"gc", Platform:"linux/amd64"&#125; 安装 Minikube1234$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.24.1/minikube-darwin-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/$ minikube versionminikube version: v0.24.1 启动集群 1$ minikube start --vm-driver=xhyve --registry-mirror=https://registry.docker-cn.com 查看节点 123$ kubectl get nodeNAME STATUS ROLES AGE VERSIONminikube Ready &lt;none&gt; 13d v1.8.0 进入集群 1$ minikube ssh 或者使用 Minikube Docker 守护进程： 1$ eval $(minikube docker-env) 如果不使用 Minikube，可以通过运行 eval $(minikube docker-env -u) 来撤消此更改。 确保以下镜像已经预先下载（查源码），可以使用这个脚本。 12345678910gcr.io/google_containers/pause-amd64:3.0gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5gcr.io/google-containers/kube-addon-manager:v6.4-beta.2gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5gcr.io/google_containers/kubernetes-dashboard-amd64:v1.8.0gcr.io/k8s-minikube/storage-provisioner:v1.8.1quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.9.0-beta.17gcr.io/google_containers/defaultbackend:1.4 确认所有服务就绪12345678$ kubectl get pod --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system default-http-backend-qd455 1/1 Running 1 47mkube-system kube-addon-manager-minikube 1/1 Running 3 13dkube-system kube-dns-86f6f55dd5-wkbxj 3/3 Running 9 47mkube-system kubernetes-dashboard-qn4tw 1/1 Running 3 47mkube-system nginx-ingress-controller-jvbtg 1/1 Running 2 47mkube-system storage-provisioner 1/1 Running 1 47m 访问 Dashboard 12$ minikube dashboardOpening kubernetes dashboard in default browser... 切换集群 1$ kubectl config use-context minikube 查看集群信息 12$ $ kubectl cluster-infoKubernetes master is running at https://192.168.64.3:8443]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>mac</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 kubernetes 1.8 上部署 ingress]]></title>
    <url>%2F2017%2F12%2F15%2Finstall-ingress-to-k8s%2F</url>
    <content type="text"><![CDATA[准备本文所需代码在这里 先查看集群状态12345$ kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-master Ready master 15d v1.8.4k8s-node-1 Ready &lt;none&gt; 15d v1.8.4k8s-node-2 Ready &lt;none&gt; 15d v1.8.4 ingress 有多种方式 deployment 自由调度 daemonset 全局调度 官方部署现在是 deployment 方式。在 deployment 自由调度过程中，由于我们需要约束 controller 调度到指定的 node 中，所以需要对 node 进行 label 标签。 12345678910$ kubectl label nodes k8s-node-1 ingress=proxynode "k8s-node-1" labeled$ kubectl label nodes k8s-node-2 ingress=proxynode "k8s-node-2” labeled$ kubectl get nodes --show-labelsNAME STATUS ROLES AGE VERSION LABELSk8s-master Ready master 15d v1.8.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=k8s-master,node-role.kubernetes.io/master=k8s-node-1 Ready &lt;none&gt; 15d v1.8.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=k8s-node-1k8s-node-2 Ready &lt;none&gt; 15d v1.8.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=k8s-node-2 安装123456789101112$ kubectl apply -f namespace.yaml $ kubectl apply -f default-backend.yaml $ kubectl apply -f configmap.yaml $ kubectl apply -f tcp-services-configmap.yaml $ kubectl apply -f udp-services-configmap.yaml $ kubectl apply -f rbac.yaml $ kubectl apply -f with-rbac.yaml$ kubectl get pods --all-namespaces -l app=ingress-nginx -o wideNAMESPACE NAME READY STATUS RESTARTS AGE IP NODEingress-nginx nginx-ingress-controller-64f7567b77-dv7cn 1/1 Running 0 6h 172.31.21.148 k8s-node-2ingress-nginx nginx-ingress-controller-64f7567b77-zcvkt 1/1 Running 0 6h 172.31.21.147 k8s-node-1 使用jenkins12345678910111213apiVersion: extensions/v1beta1kind: Ingressmetadata: name: jenkins-uispec: rules: - host: jenkins.idealsoftone.com http: paths: - path: / backend: serviceName: jenkins-ui servicePort: 8080 harbor12345678910111213apiVersion: extensions/v1beta1kind: Ingressmetadata: name: harbor-uispec: rules: - host: hub.idealsoftone.com http: paths: - path: / backend: serviceName: harbor-ui servicePort: 80]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
        <tag>kubernetes</tag>
        <tag>ingress</tag>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 kubernetes 1.8 上部署 Jenkins 动态集群]]></title>
    <url>%2F2017%2F12%2F15%2Finstall-jenkins-to-k8s%2F</url>
    <content type="text"><![CDATA[本文的目的是通过在 Kubernetes 集群上创建并配置 Jenkins Server ，实现应用开发管理的 CI/CD 流程，并且利用 Kubernetes-Jenkins-Plugin 实现动态的按需扩展 jenkins-slave。 安装本文所需代码在这里 推送 Jenkins Master Docker 镜像到 Harbor123$ cd master$ docker build -t 172.31.21.226/ideal/jenkins:lts .$ docker push 172.31.21.226/ideal/jenkins:lts 推送 Jenkins Slave Docker 镜像到 Harbor123$ cd slave$ docker build -t 172.31.21.226/ideal/jnlp-slave:latest .$ docker push 172.31.21.226/ideal/jnlp-slave:latest 安装 Jenkins12$ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/jenkins/service-account.yml$ kubectl apply -f ./ Kubernetes 插件安装略过。 配置系统管理 - 系统设置 - 云 - Kubernetes 123Name: kubernetesKubernetes URL: https://kubernetes.defaultJenkins URL: http://jenkins.default:8080 如果 service account 没有问题，点击 test，应该可以看到 Connection test successful。 系统管理 - 系统设置 - 云 - Kubernetes - Add Pod Template1234567891011images - Add Pod Template:Name: jnlp-slaveLabels: jnlp-slaveContainers:Name: jnlp Docker image: 172.31.21.226/ideal/jnlp-slave:latestAlways pull image: yesJenkins slave root directory: /home/jenkinsHost path: /var/run/docker.sockMount path: /var/run/docker.sock Jenkins Job非 pipeline 方式 pipeline 方式直接实现 groovy 脚本，可以放到 git 中管理。 12345678podTemplate(label: 'jnlp-slave') &#123; node('jnlp-slave')&#123; git branch: 'master', credentialsId: 'e242d1e1-58b5-4645-a84e-64f957e32016', url: 'https://gitee.com/idealsoftone/poseidon.git' sh 'sleep 120' build_tag = sh(returnStdout: true, script: 'git rev-parse --short HEAD').trim() echo build_tag &#125;&#125; 这里指定 jenkins slave 为插件中配置的 jnlp-slave。]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
        <tag>kubernetes</tag>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 kubernetes 1.8 上安装 Harbor 仓库]]></title>
    <url>%2F2017%2F12%2F15%2Finstall-harbor-to-k8s%2F</url>
    <content type="text"><![CDATA[下载当前最新版本是 1.2.2 1$ wget http://harbor.orientsoft.cn/harbor-1.2.2/harbor-offline-installer-v1.2.2.tgz 准备 Docker 镜像解压以后要把所有镜像上传到 k8s 工作节点。 1234$ tar -vxf harbor-offline-installer-v1.2.2.tgz$ cd harbor/ $ scp harbor.v1.2.2.tar.gz k8s-node$ docker load -i harbor.v1.2.2.tar.gz 准备配置文件下载源码1$ git clone https://github.com/vmware/harbor.git 在以下目录中所有的 rc.yaml 中镜像替换成正确的镜像地址1make/kubernetes/**/*.rc.yaml 在以下目录文件中设置存储的容量1make/kubernetes/pv/*.pvc.yaml 如果你改变了 PVC 的容量，那么你也需要相应的设置 PV 的容量。 如果想让外部访问，需要修改两个地方 12$ vim make/harbor.cfghostname = 172.31.21.226 123456789101112$ vim make/kubernetes/nginx/nginx.svc.yaml...metadata: name: nginxspec: ports: - name: http port: 80 selector: name: nginx-apps externalIPs: - 172.31.21.226 如果部署了 ingress，可以不用管上边两步 1234567891011121314apiVersion: extensions/v1beta1kind: Ingressmetadata: name: harbor-uispec: rules: - host: hub.xxx.com http: paths: - path: / backend: serviceName: harbor-ui servicePort: 80 生成安装脚本1$ python make/kubernetes/k8s-prepare 脚本执行完成后会生成下面的一些文件： make/kubernetes/jobservice/jobservice.cm.yaml make/kubernetes/mysql/mysql.cm.yaml make/kubernetes/nginx/nginx.cm.yaml make/kubernetes/registry/registry.cm.yaml make/kubernetes/ui/ui.cm.yaml 安装12345678910111213141516171819202122232425262728293031# create pv &amp; pvckubectl apply -f make/kubernetes/pv/log.pv.yaml &amp;&amp;\kubectl apply -f make/kubernetes/pv/registry.pv.yaml &amp;&amp;\kubectl apply -f make/kubernetes/pv/storage.pv.yaml &amp;&amp;\kubectl apply -f make/kubernetes/pv/log.pvc.yaml &amp;&amp;\kubectl apply -f make/kubernetes/pv/registry.pvc.yaml &amp;&amp;\kubectl apply -f make/kubernetes/pv/storage.pvc.yaml# create config mapkubectl apply -f make/kubernetes/adminserver/adminserver.cm.yaml &amp;&amp;\kubectl apply -f make/kubernetes/jobservice/jobservice.cm.yaml &amp;&amp;\kubectl apply -f make/kubernetes/mysql/mysql.cm.yaml &amp;&amp;\kubectl apply -f make/kubernetes/registry/registry.cm.yaml &amp;&amp;\kubectl apply -f make/kubernetes/ui/ui.cm.yaml &amp;&amp;\kubectl apply -f make/kubernetes/nginx/nginx.cm.yaml# create servicekubectl apply -f make/kubernetes/adminserver/adminserver.svc.yaml &amp;&amp;\kubectl apply -f make/kubernetes/jobservice/jobservice.svc.yaml &amp;&amp;\kubectl apply -f make/kubernetes/mysql/mysql.svc.yaml &amp;&amp;\kubectl apply -f make/kubernetes/registry/registry.svc.yaml &amp;&amp;\kubectl apply -f make/kubernetes/ui/ui.svc.yaml &amp;&amp;\kubectl apply -f make/kubernetes/nginx/nginx.svc.yaml# create k8s rckubectl apply -f make/kubernetes/registry/registry.rc.yaml &amp;&amp;\kubectl apply -f make/kubernetes/mysql/mysql.rc.yaml &amp;&amp;\kubectl apply -f make/kubernetes/jobservice/jobservice.rc.yaml &amp;&amp;\kubectl apply -f make/kubernetes/ui/ui.rc.yaml &amp;&amp;\kubectl apply -f make/kubernetes/nginx/nginx.rc.yaml &amp;&amp;\kubectl apply -f make/kubernetes/adminserver/adminserver.rc.yaml 问题Error response from daemon: Get https://myregistrydomain.com/v1/users/: dial tcp myregistrydomain.com:443 getsockopt: connection refused.12345678910$ vim /etc/docker/daemon.json"insecure-registries": ["172.31.21.226"]$ cat /etc/docker/daemon.json&#123; "insecure-registries": ["172.31.21.226"], "registry-mirrors": ["https://xxx.mirror.aliyuncs.com"]&#125;$ sudo systemctl daemon-reload &amp;&amp; systemctl restart docker]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
        <tag>kubernetes</tag>
        <tag>harbor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 CentOS 上使用 kubeadm 安装 kubernetes 1.8.4]]></title>
    <url>%2F2017%2F12%2F15%2Finstall-kubernetes-1-8-4-use-kubeadm%2F</url>
    <content type="text"><![CDATA[准备工作在所有主机执行以下工作。 配置主机修改主机名称123$ hostnamectl --static set-hostname k8s-master$ hostnamectl --static set-hostname k8s-node-1$ hostnamectl --static set-hostname k8s-node-2 配 hosts123$ echo "172.31.21.226 k8s-master172.31.21.147 k8s-node-1172.31.21.148 k8s-node-2" &gt;&gt; /etc/hosts 关防火墙和 selinux123$ systemctl stop firewalld &amp;&amp; systemctl disable firewalld$ iptables -P FORWARD ACCEPT$ sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config 1234$ echo "net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1vm.swappiness=0" &gt;&gt; /etc/sysctl.d/k8s.conf$ sysctl -p /etc/sysctl.d/k8s.conf 关闭 swap1$ swapoff -a 永久关闭，注释 swap 相关内容1vim /etc/fstab 下载离线安装包k8s 最新的版本需要 FQ 下载。 1234$ wget https://packages.cloud.google.com/yum/pool/aeaad1e283c54876b759a089f152228d7cd4c049f271125c23623995b8e76f96-kubeadm-1.8.4-0.x86_64.rpm$ wget https://packages.cloud.google.com/yum/pool/a9db28728641ddbf7f025b8b496804d82a396d0ccb178fffd124623fb2f999ea-kubectl-1.8.4-0.x86_64.rpm$ wget https://packages.cloud.google.com/yum/pool/1acca81eb5cf99453f30466876ff03146112b7f12c625cb48f12508684e02665-kubelet-1.8.4-0.x86_64.rpm$ wget https://packages.cloud.google.com/yum/pool/79f9ba89dbe7000e7dfeda9b119f711bb626fe2c2d56abeb35141142cda00342-kubernetes-cni-0.5.1-1.x86_64.rpm 安装 docker在所有主机执行以下工作。kubernetes 1.8.4 目前支持 Docker 17.03。 添加阿里源1$ yum-config-manager --add-repo &lt;http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo&gt; 安装指定 Docker 版本123$ yum install -y --setopt=obsoletes=0 \ docker-ce-17.03.2.ce-1.el7.centos \ docker-ce-selinux-17.03.2.ce-1.el7.centos 配置 Docker 加速器1234567$ sudo mkdir -p /etc/docker$ sudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'&#123; "registry-mirrors": ["https://xxx.mirror.aliyuncs.com"]&#125;EOF$ sudo systemctl daemon-reload &amp;&amp; systemctl enable docker &amp;&amp; systemctl restart docker &amp;&amp; systemctl status docker 安装 k8s在所有主机执行以下工作。 启动 kubelet1234$ yum -y localinstall *.rpm$ yum install -y socat$ sed -i 's/cgroup-driver=systemd/cgroup-driver=cgroupfs/g' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf$ systemctl daemon-reload &amp;&amp; systemctl restart kubelet &amp;&amp; systemctl enable kubelet &amp;&amp; systemctl status kubelet 这时 kubelet 应该还在报错，不用管它。1$ journalctl -u kubelet --no-pager 准备 Docker 镜像12345678910gcr.io/google_containers/kube-apiserver-amd64 v1.8.4gcr.io/google_containers/kube-controller-manager-amd64 v1.8.4gcr.io/google_containers/kube-proxy-amd64 v1.8.4gcr.io/google_containers/kube-scheduler-amd64 v1.8.4quay.io/coreos/flannel v0.9.1-amd64gcr.io/google_containers/k8s-dns-sidecar-amd64 1.14.5gcr.io/google_containers/k8s-dns-kube-dns-amd64 1.14.5gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64 1.14.5gcr.io/google_containers/etcd-amd64 3.0.17gcr.io/google_containers/pause-amd64 3.0 可以使用这个脚本拉取到本地。 配置 k8s 集群master 初始化1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253kubeadm init --apiserver-advertise-address=172.31.21.226 --kubernetes-version=v1.8.4 --pod-network-cidr=10.244.0.0/16[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.[init] Using Kubernetes version: v1.8.4[init] Using Authorization modes: [Node RBAC][preflight] Running pre-flight checks[kubeadm] WARNING: starting in 1.8, tokens expire after 24 hours by default (if you require a non-expiring token use --token-ttl 0)[certificates] Generated ca certificate and key.[certificates] Generated apiserver certificate and key.[certificates] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.31.21.226][certificates] Generated apiserver-kubelet-client certificate and key.[certificates] Generated sa key and public key.[certificates] Generated front-proxy-ca certificate and key.[certificates] Generated front-proxy-client certificate and key.[certificates] Valid certificates and keys now exist in "/etc/kubernetes/pki"[kubeconfig] Wrote KubeConfig file to disk: "admin.conf"[kubeconfig] Wrote KubeConfig file to disk: "kubelet.conf"[kubeconfig] Wrote KubeConfig file to disk: "controller-manager.conf"[kubeconfig] Wrote KubeConfig file to disk: "scheduler.conf"[controlplane] Wrote Static Pod manifest for component kube-apiserver to "/etc/kubernetes/manifests/kube-apiserver.yaml"[controlplane] Wrote Static Pod manifest for component kube-controller-manager to "/etc/kubernetes/manifests/kube-controller-manager.yaml"[controlplane] Wrote Static Pod manifest for component kube-scheduler to "/etc/kubernetes/manifests/kube-scheduler.yaml"[etcd] Wrote Static Pod manifest for a local etcd instance to "/etc/kubernetes/manifests/etcd.yaml"[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory "/etc/kubernetes/manifests"[init] This often takes around a minute; or longer if the control plane images have to be pulled.[apiclient] All control plane components are healthy after 24.501140 seconds[uploadconfig] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace[markmaster] Will mark node k8s-master as master by adding a label and a taint[markmaster] Master k8s-master tainted and labelled with key/value: node-role.kubernetes.io/master=""[bootstraptoken] Using token: d87240.989b8aa6b0039283[bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstraptoken] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[bootstraptoken] Creating the "cluster-info" ConfigMap in the "kube-public" namespace[addons] Applied essential addon: kube-dns[addons] Applied essential addon: kube-proxyYour Kubernetes master has initialized successfully!To start using your cluster, you need to run (as a regular user): mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configYou should now deploy a pod network to the cluster.Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:http://kubernetes.io/docs/admin/addons/You can now join any number of machines by running the following on each nodeas root: kubeadm join --token d87240.989b8aa6b0039283 172.31.21.226:6443 --discovery-token-ca-cert-hash sha256:4c2b5469ddc4f49ba15f3146bea5bf9ba8f67f68bdc9ef1ff6cb026d39b94dea 配置用户使用 kubectl 访问集群123$ mkdir -p $HOME/.kube &amp;&amp; \ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config &amp;&amp; \ sudo chown $(id -u):$(id -g) $HOME/.kube/config 查看一下集群状态1234567891011121314$ kubectl get pod --all-namespaces -o wideNAMESPACE NAME READY STATUS RESTARTS AGE IP NODEkube-system etcd-k8s-master 1/1 Running 0 1m 172.31.21.226 k8s-masterkube-system kube-apiserver-k8s-master 1/1 Running 0 1m 172.31.21.226 k8s-masterkube-system kube-controller-manager-k8s-master 1/1 Running 0 1m 172.31.21.226 k8s-masterkube-system kube-dns-545bc4bfd4-84pjx 0/3 Pending 0 2m &lt;none&gt; &lt;none&gt;kube-system kube-proxy-7d2tc 1/1 Running 0 2m 172.31.21.226 k8s-masterkube-system kube-scheduler-k8s-master 1/1 Running 0 1m 172.31.21.226 k8s-master$ kubectl get csNAME STATUS MESSAGE ERRORcontroller-manager Healthy okscheduler Healthy oketcd-0 Healthy &#123;"health": "true”&#125; 安装Pod Network1$ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/flannel/kube-flannel.yml 这时再执行 kubectl get pod –all-namespaces -o wide 应该可以看到 kube-dns-545bc4bfd4-84pjx 已经变成 Running。如果遇到问题可能使用以下命令查看： 123$ kubectl -n kube-system describe pod kube-dns-545bc4bfd4-84pjx$ journalctl -u kubelet --no-pager$ journalctl -u docker --no-pager node 加入集群在 node 节点分别执行 1$ kubeadm join --token d87240.989b8aa6b0039283 172.31.21.226:6443 --discovery-token-ca-cert-hash sha256:4c2b5469ddc4f49ba15f3146bea5bf9ba8f67f68bdc9ef1ff6cb026d39b94dea 如果需要从其它任意主机控制集群1234$ mkdir -p $HOME/.kube$ scp root@172.31.21.226:/etc/kubernetes/admin.conf $HOME/.kube/config$ chown $(id -u):$(id -g) $HOME/.kube/config$ kubectl get nodes 在 master 确认所有节点 ready 12345678910111213141516171819$ kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-master Ready master 7m v1.8.4k8s-node-1 Ready &lt;none&gt; 22s v1.8.4k8s-node-2 Ready &lt;none&gt; 15s v1.8.4$ kubectl get pod --all-namespaces -o wideNAMESPACE NAME READY STATUS RESTARTS AGE IP NODEkube-system etcd-k8s-master 1/1 Running 0 51m 172.31.21.226 k8s-masterkube-system kube-apiserver-k8s-master 1/1 Running 0 51m 172.31.21.226 k8s-masterkube-system kube-controller-manager-k8s-master 1/1 Running 0 51m 172.31.21.226 k8s-masterkube-system kube-dns-545bc4bfd4-84pjx 3/3 Running 0 52m 10.244.0.3 k8s-masterkube-system kube-flannel-ds-gf2hp 1/1 Running 0 6m 172.31.21.226 k8s-masterkube-system kube-flannel-ds-k8wc9 1/1 Running 0 24s 172.31.21.147 k8s-node-1kube-system kube-flannel-ds-v7jpv 1/1 Running 0 10s 172.31.21.148 k8s-node-2kube-system kube-proxy-7d2tc 1/1 Running 0 52m 172.31.21.226 k8s-masterkube-system kube-proxy-b9z97 1/1 Running 0 10s 172.31.21.148 k8s-node-2kube-system kube-proxy-ksvwp 1/1 Running 0 24s 172.31.21.147 k8s-node-1kube-system kube-scheduler-k8s-master 1/1 Running 0 51m 172.31.21.226 k8s-master 安装 dashboard准备 Docker 镜像1gcr.io/google_containers/kubernetes-dashboard-amd64 v1.8.0 可以使用这个脚本拉取到本地。 初始化12$ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/kubernetes-dashboard/kubernetes-dashboard.yaml$ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/kubernetes-dashboard/kubernetes-dashboard-admin.rbac.yaml 确认 dashboard 状态12$ kubectl get pod --all-namespaces -o widekube-system kubernetes-dashboard-7486b894c6-2l4c5 1/1 Running 0 17s 10.244.1.3 k8s-node-1 访问https://172.31.21.226:30000 或者在任意主机执行（比如我的 Mac） 1$ kubectl proxy 访问：http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ 查看登录 token12345$ kubectl -n kube-system get secret | grep kubernetes-dashboard-adminkubernetes-dashboard-admin-token-r95kv kubernetes.io/service-account-token 3 7m$ kubectl describe -n kube-system secret/kubernetes-dashboard-admin-token-r95kveyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbi10b2tlbi1yOTVrdiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjM4MWI4OWQzLWQ0ZDctMTFlNy1hY2U3LTAwMGMyOWMyMTdlNSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcm5l 安装 heapster准备 Docker 镜像123gcr.io/google_containers/heapster-amd64:v1.4.0gcr.io/google_containers/heapster-grafana-amd64:v4.4.3gcr.io/google_containers/heapster-influxdb-amd64:v1.3.3 可以使用这个脚本拉取到本地。 初始化1234$ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/heapster/heapster-rbac.yaml$ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/heapster/grafana.yaml$ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/heapster/heapster.yaml $ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/heapster/influxdb.yaml]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
        <tag>kubernetes</tag>
        <tag>heapster</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Shasdowsocks + Privoxy 转 HTTP 代理]]></title>
    <url>%2F2017%2F12%2F14%2Fshare-shasdowsocks-to-http%2F</url>
    <content type="text"><![CDATA[本文把 Mac 上的 Shadowsocks 转换为 http 代理，分享给其它人使用。 安装1$ brew install privoxy 配置12$ echo 'listen-address 0.0.0.0:8118' &gt;&gt; /usr/local/etc/privoxy/config$ echo 'forward-socks5 / localhost:1080 .' &gt;&gt; /usr/local/etc/privoxy/config 8118 是要监听的 http 端口1082 是我自己本地的 shadowsocks 监听端口。 启动1$ usr/local/Cellar/privoxy/3.0.26/sbin/privoxy /usr/local/etc/privoxy/config 确认查进程1$ ps aux | grep privoxy 查端口1$ netstat -an | grep 8118 使用12$ export http_proxy=http://ip:8118$ export https_proxy=$http_proxy]]></content>
      <tags>
        <tag>shadowsocks</tag>
        <tag>privoxy</tag>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 实战（六）：使用 Docker Compose 搭建 Web 集群]]></title>
    <url>%2F2016%2F12%2F09%2Fdocker-six-docker-compose-cluster%2F</url>
    <content type="text"><![CDATA[在 实战（四） 中，使用 HAProxy 和 Tomcat 搭建了一个简单的 Tomcat 集群。这节会去掉 Tomcat，使用 Spring Boot 和 MySQL 组成一个比较典型的负载均衡集群。 目录123456compose-haproxy-web- docker-compose.yml+ haproxy - haproxy.cfg+ web - paper-0.0.1-SNAPSHOT.jar docker-compose.ymlMySQL 配置1234567image: batizhao/mysqlenvironment: MYSQL_ROOT_PASSWORD: password MYSQL_DATABASE: paper MYSQL_ROOT_HOST: "%"expose: - "3306" MYSQL_DATABASE 需要在启动时创建 paper 数据库。MYSQL_ROOT_HOST 在 mysql user 中增加客户端远程访问的权限。 在 batizhao/mysql 这个镜像中，有一个自定义的 my.cnf 文件，主要是定义了 bind-address ，否则会遇到 CommunicationsException: Communications link failure 的错误，还有一些服务端、客户端 UTF-8 编码的定义。 Web 配置123456789101112131415image: batizhao/java:8command: java -jar opt/paper.jarvolumes: - ./web/paper-0.0.1-SNAPSHOT.jar:/opt/paper.jardepends_on: - "mysql"links: - "mysql:database"expose: - "8080"environment: - SPRING_DATASOURCE_URL=jdbc:mysql://database:3306/paper?useUnicode=true&amp;useSSL=false - SPRING_DATASOURCE_USERNAME=root - SPRING_DATASOURCE_PASSWORD=password - SPRING_DATASOURCE_SQL-SCRIPT-ENCODING=UTF-8 volumes 这个 Spring Boot 应用在 Github，可以自己打包放到 web 目录。SPRING_DATASOURCE_SQL-SCRIPT-ENCODING 这个配置必须要有，否则初始化脚本会插入乱码到数据库。已经确定和 MySQL 无关，因为在宿主机启动 App 直接连接 MySQL 容器，并且在启动后插入中文数据都没有问题。只是在容器中启动 App 初始化时才会乱码，后来改 Web 容器编码也不起作用，加上这个配置就好了。 全部的 compose 配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859version: '2'services: mysql: image: batizhao/mysql environment: MYSQL_ROOT_PASSWORD: password MYSQL_DATABASE: paper MYSQL_ROOT_HOST: "%" expose: - "3306" weba: image: batizhao/java:8 command: java -jar opt/paper.jar volumes: - ./web/paper-0.0.1-SNAPSHOT.jar:/opt/paper.jar depends_on: - "mysql" links: - "mysql:database" expose: - "8080" environment: - SPRING_DATASOURCE_URL=jdbc:mysql://database:3306/paper?useUnicode=true&amp;useSSL=false - SPRING_DATASOURCE_USERNAME=root - SPRING_DATASOURCE_PASSWORD=password - SPRING_DATASOURCE_SQL-SCRIPT-ENCODING=UTF-8 webb: image: batizhao/java:8 command: java -jar opt/paper.jar volumes: - ./web/paper-0.0.1-SNAPSHOT.jar:/opt/paper.jar depends_on: - "mysql" links: - "mysql:database" expose: - "8080" environment: - SPRING_DATASOURCE_URL=jdbc:mysql://database:3306/paper?useUnicode=true&amp;useSSL=false - SPRING_DATASOURCE_USERNAME=root - SPRING_DATASOURCE_PASSWORD=password - SPRING_DATASOURCE_SQL-SCRIPT-ENCODING=UTF-8 haproxy: image: haproxy volumes: - ./haproxy:/haproxy-override - ./haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro links: - weba - webb ports: - "80:80" - "70:70" expose: - "80" - "70" haproxy.cfg123456789101112131415161718192021222324252627282930313233343536373839global log 127.0.0.1 local0 log 127.0.0.1 local1 notice maxconn 4096 daemondefaults log global mode http option httplog option dontlognull retries 3 option redispatch maxconn 2000 timeout connect 5000 timeout client 50000 timeout server 50000frontend balancer bind 0.0.0.0:80 mode http default_backend serversbackend servers option httpchk OPTIONS / option forwardfor cookie JSESSIONID prefix server tomcat1 weba:8080 cookie JSESSIONID_SERVER_1 check inter 5000 server tomcat2 webb:8080 cookie JSESSIONID_SERVER_2 check inter 5000listen status mode http default_backend servers bind 0.0.0.0:70 stats enable stats hide-version stats uri /stats stats auth admin:password stats admin if TRUE 启动容器1234567891011$ docker-compose upCreating network &quot;composehaproxyweb_default&quot; with the default driverCreating composehaproxyweb_mysql_1Creating composehaproxyweb_webb_1Creating composehaproxyweb_weba_1Creating composehaproxyweb_haproxy_1Attaching to composehaproxyweb_mysql_1, composehaproxyweb_webb_1, composehaproxyweb_weba_1, composehaproxyweb_haproxy_1mysql_1 | Initializing databasehaproxy_1 | &lt;7&gt;haproxy-systemd-wrapper: executing /usr/local/sbin/haproxy -p /run/haproxy.pid -f /usr/local/etc/haproxy/haproxy.cfg -Dsmysql_1 | Database initializedmysql_1 | MySQL init process in progress... 访问 http://localhost使用 admin/123456 可以登录系统。 访问 http://localhost:70/stats使用 admin/password 可以看到集群状态。]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>mysql</tag>
        <tag>tomcat</tag>
        <tag>docker</tag>
        <tag>haproxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 实战（五）：Docker Swarm Mode]]></title>
    <url>%2F2016%2F12%2F02%2Fdocker-five-swarm-mode%2F</url>
    <content type="text"><![CDATA[在 Docker Compose 中，我们可以在单台机器上操作多个相关联的 Docker 容器组成负载均衡集群。那如果我们需要一个分布式的环境中，跨多台主机呢？在 Docker 1.12 以上版本中，有个新的东西叫做 Swarm Mode。不同于之前版本的 Docker Swarm（还需要 pull swarm），Swarm Mode 已经集成在 Docker Engine 中。 主要特性内置于 Docker Engine 的集群管理可以直接用 Docker Engine CLI 来创建 Swarm 集群，并在该集群上部署服务。你不再需要额外的编排软件来创建或管理 Swarm 集群了。 去中心化设计不同于在部署时就确定节点之间的关系, 新的 Swarm 模式选择在运行时动态地处理这些关系, 你可以用 Docker Engine 部署 manager 和 worker 这两种不同的节点。 这意味着你可以从一个磁盘镜像搭建整个 Swarm 。 声明式服务模型Docker Engine 使用一种声明式方法来定义各种服务的状态。譬如，你可以描述一个由 web 前端服务，消息队列服务和数据库后台组成的应用。 服务扩缩你可以通过 docker service scale 命令轻松地增加或减少某个服务的任务数。 集群状态维护Swarm 管理节点会一直监控集群状态，并依据你给出的期望状态与集群真实状态间的区别来进行调节。譬如，你为一个服务设置了10个任务副本，如果某台运行着该服务两个副本的工作节点停止工作了，管理节点会创建两个新的副本来替掉上述异常终止的副本。 Swarm 管理节点这个新的副本分配到了正常运行的工作节点上。 跨主机网络你可以为你的服务指定一个 overlay 网络。在服务初始化或着更新时，Swarm 管理节点自动的为容器在 overlay 网络上分配地址。 服务发现Swarm 管理节点在集群中自动的为每个服务分配唯一的 DNS name 并为容器配置负载均衡。利用内嵌在 Swarm 中的 DNS 服务器你可以找到每个运行在集群中的容器。 负载均衡你可以把服务的端口暴露给一个集群外部的负载均衡器。 在 Swarm 集群内部你可以决定如何在节点间分发服务的容器。 默认 TLS 加密Swarm 集群中的节点间通信是强制加密的。你可以选择使用自签名的根证书或者来自第三方认证的证书。 滚动更新docker service 允许你自定义更新的间隔时间, 并依次更新你的容器, docker 会按照你设置的更新时间依次更新你的容器, 如果发生了错误, 还可以回滚到之前的状态。 原文链接Swarm 模式 准备三台主机你可以找三台物理主机，如果在单机环境做测试，可以使用 Docker Machine 创建三台 Docker 主机 manager、worker1、worker2。 123$ docker-machine create -d virtualbox manager$ docker-machine create -d virtualbox worker1$ docker-machine create -d virtualbox worker2 在 Docker for Mac 中，已经不需要 VirtualBox，而是使用 HyperKit，所以这里需要先安装最新版本的 VirtualBox。 12345$ docker-machine lsNAME ACTIVE DRIVER STATE URL SWARM DOCKER ERRORSmanager - virtualbox Running tcp://192.168.99.101:2376 v1.12.3worker1 - virtualbox Running tcp://192.168.99.102:2376 v1.12.3worker2 * virtualbox Running tcp://192.168.99.103:2376 v1.12.3 创建 Swarm确认 manager 节点的 ip 地址12$ docker-machine ip manager192.168.99.101 初始化 swarm12345678910$ docker swarm init --advertise-addr 192.168.99.101Swarm initialized: current node (3pniknvvlt9hb5bjpdnwkp5zr) is now a manager.To add a worker to this swarm, run the following command: docker swarm join \ --token SWMTKN-1-3gxjh13dqeabze16assdjhng71gy4x7pvu2lqh8fdj22ttg02d-dnkw6tbxbh6so5f0ng7ukiysb \ 192.168.99.101:2377To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. 查看 manager token123456$ docker swarm join-token managerTo add a manager to this swarm, run the following command: docker swarm join \ --token SWMTKN-1-3gxjh13dqeabze16assdjhng71gy4x7pvu2lqh8fdj22ttg02d-dnkw6tbxbh6so5f0ng7ukiysb \ 192.168.99.101:2377 查看 worker token123456$ docker swarm join-token workerTo add a worker to this swarm, run the following command: docker swarm join \ --token SWMTKN-1-3gxjh13dqeabze16assdjhng71gy4x7pvu2lqh8fdj22ttg02d-buqlcaugygyzizgewblrfdqr3 \ 192.168.99.101:2377 这里可以自由选择 manager 还是 worker 类型的节点，示例中以 worker 为例。 查看节点123$ docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS3pniknvvlt9hb5bjpdnwkp5zr * manager Ready Active Leader 进入节点这里有 2 种办法进入 worker1，先新开一个终端窗口 1$ eval $(docker-machine env worker1) 或者 1$ docker-machine ssh worker1 这里建议打开 3 个终端窗口，分别执行 $ eval $(docker-machine env ) 进入 manager、worker1、worker2 三台主机。 进入 worker1 后执行 查看 worker token 下的那段命令123$ docker swarm join \ --token SWMTKN-1-3gxjh13dqeabze16assdjhng71gy4x7pvu2lqh8fdj22ttg02d-buqlcaugygyzizgewblrfdqr3 \ 192.168.99.101:2377 同样的方式进入 worker2 执行上述相同的命令。 回到 manager 查看节点12345$ docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS1s6uw7ew22xhvy5trv2ejnncd worker2 Ready Active3pniknvvlt9hb5bjpdnwkp5zr * manager Ready Active Leaderbeh2b7riuqv7oz5nhbhuvmr0t worker1 Ready Active 部署服务在创建 3 个节点完成后，现在可以在上边部署服务。现在还是回到 manager 节点 创建服务12$ docker service create --replicas 1 --name helloworld alpine ping docker.comdoq2uzfwm3c5fukhksv50ewsf name 指定容器名字replicas 只复制一个实例alpine ping docker.com 定义一个 Alpine Linux container 并执行 ping docker.com 命令。 查看服务实例123$ docker service lsID NAME REPLICAS IMAGE COMMANDdoq2uzfwm3c5 helloworld 1/1 alpine ping docker.com 检查服务12345678910111213$ docker service inspect --pretty helloworldID: doq2uzfwm3c5fukhksv50ewsfName: helloworldMode: Replicated Replicas: 1Placement:UpdateConfig: Parallelism: 1 On failure: pauseContainerSpec: Image: alpine Args: ping docker.comResources: 查看服务运行在哪个节点123$ docker service ps helloworldID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERRORc81lt2t8ride9ioduqv86i0v7 helloworld.1 alpine manager Running Running 7 minutes ago 这里要关注 DESIRED STATE 和 LAST STATE 两个状态。 在服务所在节点查看进程，这里是 manager123$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES458df9db3e9a alpine:latest "ping docker.com" 10 minutes ago Up 10 minutes helloworld.1.c81lt2t8ride9ioduqv86i0v7 服务扩展还在是 manager 节点，通过命令，我们可以改变集群中的节点实例。 1$ docker service scale &lt;SERVICE-ID&gt;=&lt;NUMBER-OF-TASKS&gt; 在这里，我们把 helloworld 实例扩展到 5 个12$ docker service scale helloworld=5helloworld scaled to 5 查看服务实例运行节点分布1234567$ docker service ps helloworldID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERRORc81lt2t8ride9ioduqv86i0v7 helloworld.1 alpine manager Running Running 20 minutes agob98r4ypet6b9doiof1bb0aq68 helloworld.2 alpine worker1 Running Running 5 seconds ago20hkvi1i3ihur0x17a989qvpy helloworld.3 alpine manager Running Running 8 seconds ago472p0ykqnwtjvmo10bx1kutz8 helloworld.4 alpine worker2 Running Running 2 seconds agodfmx6i4aunk27m8xstavncwxa helloworld.5 alpine worker1 Running Running 2 seconds ago 可以看到，manager 和 worker1 有 2 个实例，worker2 有 1 个实例。 到 manager 上执行1234$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe90f1e77d7aa alpine:latest "ping docker.com" About a minute ago Up About a minute helloworld.3.20hkvi1i3ihur0x17a989qvpy458df9db3e9a alpine:latest "ping docker.com" 22 minutes ago Up 22 minutes helloworld.1.c81lt2t8ride9ioduqv86i0v7 到 worker1 上执行1234$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES806eb838fd86 alpine:latest "ping docker.com" 2 minutes ago Up 2 minutes helloworld.5.dfmx6i4aunk27m8xstavncwxafaa01a019dcf alpine:latest "ping docker.com" 2 minutes ago Up 2 minutes helloworld.2.b98r4ypet6b9doiof1bb0aq68 到 worker2 上执行123$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf020134c72bb alpine:latest "ping docker.com" 2 minutes ago Up 2 minutes helloworld.4.472p0ykqnwtjvmo10bx1kutz8 删除服务1$ docker service rm helloworld 确认是否删除123$ docker service inspect helloworld[]Error: no such service: helloworld 滚动更新这里我们会先部署 3 个 Redis 3.0.6 实例到 swarm 节点，然后再更新到 3.0.7。 123456$ docker service create \ --replicas 3 \ --name redis \ --update-delay 10s \ redis:3.0.60knduq4z4vae02wvc33vz5b0u update-delay 实例之间的更新延时时间. 可以使用秒 s、分钟 m 或者 小时 h。例如 10m30s 就是延时 10分30秒。默认情况同一时间更新一个实例。可以通过 –update-parallelism 配置同时更新的个数。 12345$ docker service ps redisID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR27mlfg8pqlvz9w4yky1q9fxm7 redis.1 redis:3.0.6 worker1 Running Running 2 seconds agoejlctv6j92caxapd7g2bll0bo redis.2 redis:3.0.6 manager Running Preparing 16 seconds ago3l0nu4zt99kecwig1sfie1km9 redis.3 redis:3.0.6 worker2 Running Preparing 16 seconds ago 这里要关注 CURRENT STATE，上边的状态说明 worker1 实例已经 Running，但 manager 和 worker2 还在 Preparing。到各自主机上用 docker ps 可以证明这一点。等待片刻，所有实例的 CURRENT STATE 都变成 Running。 看最后的 Running 时间，第二个节点晚了 7 分钟，第三个节点晚了 16 分钟，三个实例全部启动成功。12345$ docker service ps redisID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR27mlfg8pqlvz9w4yky1q9fxm7 redis.1 redis:3.0.6 worker1 Running Running 18 minutes agoejlctv6j92caxapd7g2bll0bo redis.2 redis:3.0.6 manager Running Running 2 minutes ago3l0nu4zt99kecwig1sfie1km9 redis.3 redis:3.0.6 worker2 Running Running 11 minutes ago 如果想看各个 redis 实例的启动日志，你可以 docker ps 拿到容器 ID，然后 docker logs CONTAINER_ID 看到 redis 的启动日志。如果想知道在最后一个实例启动之前 16 分钟内三个实例发生了什么事情，你需要 docker-machine ssh NODE_NAME ，看 /var/log/docker.log 中的内容。 接下来，我们更新 redis 实例到 3.0.712$ docker service update --image redis:3.0.7 redisredis 因为之前已经在各个节点更新过 3.0.7 镜像，省略了下载新镜像的过程，所以这次更新在 2 分钟之内全部完成。12345678$ docker service ps redisID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERRORcf3stbcfz3zdmw5mgogya6amd redis.1 redis:3.0.7 manager Running Running 2 minutes ago27mlfg8pqlvz9w4yky1q9fxm7 \_ redis.1 redis:3.0.6 worker1 Shutdown Shutdown 2 minutes agoewv52c73p0klbx6hmofxm72ti redis.2 redis:3.0.7 worker2 Running Running about a minute agoejlctv6j92caxapd7g2bll0bo \_ redis.2 redis:3.0.6 manager Shutdown Shutdown about a minute ago4rcaexit4kupwcjrxdnjftgln redis.3 redis:3.0.7 worker1 Running Running about a minute ago3l0nu4zt99kecwig1sfie1km9 \_ redis.3 redis:3.0.6 worker2 Shutdown Shutdown about a minute ago 拉掉节点有时，比如计划维护时间，您需要将节点设置为不可用。 DRAIN 可用性防止节点从 swarm 管理器接收新任务。它还意味着管理器停止在节点上运行的任务，并在具有 ACTIVE 可用性的节点上启动副本任务。 拉掉 worker1 节点12$ docker node update --availability drain worker1worker1 这时看到 worker1 节点已经 Shutdown，并且在 worker2 上启动了一个新的 redis 实例。123456789$ docker service ps redisID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERRORcf3stbcfz3zdmw5mgogya6amd redis.1 redis:3.0.7 manager Running Running 13 minutes ago27mlfg8pqlvz9w4yky1q9fxm7 \_ redis.1 redis:3.0.6 worker1 Shutdown Shutdown 13 minutes agoewv52c73p0klbx6hmofxm72ti redis.2 redis:3.0.7 worker2 Running Running 12 minutes agoejlctv6j92caxapd7g2bll0bo \_ redis.2 redis:3.0.6 manager Shutdown Shutdown 12 minutes ago80zf0ykluvhhmydro7egm04iu redis.3 redis:3.0.7 worker2 Running Running 17 seconds ago4rcaexit4kupwcjrxdnjftgln \_ redis.3 redis:3.0.7 worker1 Shutdown Shutdown 35 seconds ago3l0nu4zt99kecwig1sfie1km9 \_ redis.3 redis:3.0.6 worker2 Shutdown Shutdown 13 minutes ago 恢复 worker1 节点12$ docker node update --availability active worker1worker1 查看 worker1 节点状态已经 Active12345678$ docker node inspect --pretty worker1ID: beh2b7riuqv7oz5nhbhuvmr0tHostname: worker1Joined at: 2016-11-30 09:29:10.681020915 +0000 utcStatus: State: Ready Availability: Active... 现在 worker1 可以接收新的任务了。12$ docker service scale redis=5redis scaled to 5 可以看到在 worker1 上启动了 2 个新的 redis 实例1234567891011$ docker service ps redisID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERRORcf3stbcfz3zdmw5mgogya6amd redis.1 redis:3.0.7 manager Running Running 20 minutes ago27mlfg8pqlvz9w4yky1q9fxm7 \_ redis.1 redis:3.0.6 worker1 Shutdown Shutdown 20 minutes agoewv52c73p0klbx6hmofxm72ti redis.2 redis:3.0.7 worker2 Running Running 19 minutes agoejlctv6j92caxapd7g2bll0bo \_ redis.2 redis:3.0.6 manager Shutdown Shutdown 19 minutes ago80zf0ykluvhhmydro7egm04iu redis.3 redis:3.0.7 worker2 Running Running 7 minutes ago4rcaexit4kupwcjrxdnjftgln \_ redis.3 redis:3.0.7 worker1 Shutdown Shutdown 7 minutes ago3l0nu4zt99kecwig1sfie1km9 \_ redis.3 redis:3.0.6 worker2 Shutdown Shutdown 20 minutes ago79dffeo1l7etwc731b5lgrgac redis.4 redis:3.0.7 worker1 Running Running 12 seconds ago5eyb4lhp16m9n2ob2bkifpeqw redis.5 redis:3.0.7 worker1 Running Running 12 seconds ago 路由网络 routing meshDocker Swarm Mode 可以发布服务端口，使其可用于群外的资源。所有节点都加入路由网络，路由网络使得每个节点都能够接受已发布端口上的连接，即使节点上没有任何服务正在运行。路由网络将所有传入的请求路由到正在运行服务的节点上。 为了在群中使用入口网络，您需要在群集节点之间打开以下端口： Port 7946 TCP/UDP Port 4789 UDP 发布一个对外端口发布端口使用以下命令1234$ docker service create \ --name &lt;SERVICE-NAME&gt; \ --publish &lt;PUBLISHED-PORT&gt;:&lt;TARGET-PORT&gt; \ &lt;IMAGE&gt; 这里以 nginx 为例，为了演示，这里只用了 2 个 nginx 实例123456$ docker service create \ --name my-web \ --publish 8080:80 \ --replicas 2 \ nginxd3192apcq7hharl4kpzl0eqqg 确认服务已经启动1234$ docker service ps my-webID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERRORa90s5hnb58dck8cxny4k2xgdk my-web.1 nginx worker1 Running Running 2 minutes ago2148yi9va70eu27xl6nfi9une my-web.2 nginx worker2 Running Running about a minute ago 这时我们分别访问三个节点，都可以看到 nginx 首页。 http://192.168.99.101:8080 http://192.168.99.102:8080 http://192.168.99.103:8080 架构如下（官方图） 如果你要发布一个新的端口123$ docker service update \ --publish-add &lt;PUBLISHED-PORT&gt;:&lt;TARGET-PORT&gt; \ my-web 配置一个负载均衡器前边实现了节点中 service 的负载均衡。我们可以在 Swarm Load Balance 之前再加一层负载均衡器，实现节点之间的负载均衡。具体的实现这里就不再描述，可以自己编写 haproxy.cfg 和 Dockerfile 构建 Docker HAProxy 镜像。 架构如下（官方图） -EOF-]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 实战（四）：Docker Compose]]></title>
    <url>%2F2016%2F12%2F01%2Fdocker-four-docker-compose%2F</url>
    <content type="text"><![CDATA[之前都是单个 Docker 容器，现在，基于 Docker Compose，你可以同时控制多个相关联的 Docker 容器。比如典型的 Nginx + Tomcat + MySQL 的 Web 架构，只需要几个简单的配置，敲击几个命令，原来可能需要好几个小时的工作，现在几分钟就可以搞定。 使用 Compose 主要由以下三步组成： 定义 Dockerfile； 定义 docker-compose.yml； 运行 docker-compose up 启动所有容器。 这里会构造一个 HAProxy + 两个 Tomcat 负载均衡的架构。 目录1234567compose-haproxy-web- docker-compose.yml+ haproxy - haproxy.cfg+ web - Dockerfile - index.html docker-compose.yml123456789101112131415161718192021222324weba: build: ./web expose: - 8080webb: build: ./web expose: - 8080haproxy: image: haproxy:latest volumes: - ./haproxy:/haproxy-override - ./haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro links: - weba - webb ports: - "80:80" - "70:70" expose: - "80" - "70" haproxy.cfg123456789101112131415161718192021222324252627282930313233343536373839global log 127.0.0.1 local0 log 127.0.0.1 local1 notice maxconn 4096 daemondefaults log global mode http option httplog option dontlognull retries 3 option redispatch maxconn 2000 timeout connect 5000 timeout client 50000 timeout server 50000frontend balancer bind 0.0.0.0:80 mode http default_backend serversbackend servers option httpchk OPTIONS / option forwardfor cookie JSESSIONID prefix server tomcat1 weba:8080 cookie JSESSIONID_SERVER_1 check inter 5000 server tomcat2 webb:8080 cookie JSESSIONID_SERVER_2 check inter 5000listen status mode http default_backend servers bind 0.0.0.0:70 stats enable stats hide-version stats uri /stats stats auth admin:password stats admin if TRUE Dockerfile1234567891011121314151617181920212223242526272829303132333435FROM batizhao/java:8# 创建者信息MAINTAINER batizhao &lt;zhaobati@gmail.com&gt;# Install dependenciesRUN apt-get update &amp;&amp; \ apt-get install -y wget tar &amp;&amp; \ apt-get clean &amp;&amp; \ rm -rf /var/lib/apt/lists/*# Define commonly used JAVA_HOME variableENV TOMCAT_VERSION 8.5.8# Get TomcatRUN wget --no-cookies http://ftp.jaist.ac.jp/pub/apache/tomcat/tomcat-8/v$&#123;TOMCAT_VERSION&#125;/bin/apache-tomcat-$&#123;TOMCAT_VERSION&#125;.tar.gz -O /tmp/tomcat.tgz &amp;&amp; \ tar xzvf /tmp/tomcat.tgz -C /opt &amp;&amp; \ mv /opt/apache-tomcat-$&#123;TOMCAT_VERSION&#125; /opt/tomcat &amp;&amp; \ rm /tmp/tomcat.tgz &amp;&amp; \ rm -rf /opt/tomcat/webapps/examples &amp;&amp; \ rm -rf /opt/tomcat/webapps/docs &amp;&amp; \ rm -rf /opt/tomcat/webapps/manager &amp;&amp; \ rm -rf /opt/tomcat/webapps/host-manager &amp;&amp; \ rm -rf /opt/tomcat/webapps/ROOT/*ADD index.html /opt/tomcat/webapps/ROOTENV CATALINA_HOME /opt/tomcatENV PATH $PATH:$CATALINA_HOME/binEXPOSE 8080WORKDIR /opt/tomcat# Launch TomcatCMD [&quot;/opt/tomcat/bin/catalina.sh&quot;, &quot;run&quot;] index.html1Hello, Docker. 启动容器123456789$ docker-compose upCreating composehaproxyweb_webb_1Creating composehaproxyweb_weba_1Creating composehaproxyweb_haproxy_1Attaching to composehaproxyweb_weba_1, composehaproxyweb_webb_1, composehaproxyweb_haproxy_1haproxy_1 | &lt;7&gt;haproxy-systemd-wrapper: executing /usr/local/sbin/haproxy -p /run/haproxy.pid -f /usr/local/etc/haproxy/haproxy.cfg -Dswebb_1 | 30-Nov-2016 12:50:21.524 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version: Apache Tomcat/8.5.8weba_1 | 30-Nov-2016 12:50:21.531 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version: Apache Tomcat/8.5.8... 访问 http://localhost可以看到 Hello, Docker. 访问 http://localhost:70/stats 后台运行增加 -d 参数 1234$ docker-compose up -dStarting composehaproxyweb_webb_1Starting composehaproxyweb_weba_1Starting composehaproxyweb_haproxy_1 查看 compose 进程123456$ docker-compose ps Name Command State Ports-------------------------------------------------------------------------------------------------------------composehaproxyweb_haproxy_1 /docker-entrypoint.sh hapr ... Up 0.0.0.0:70-&gt;70/tcp, 0.0.0.0:80-&gt;80/tcpcomposehaproxyweb_weba_1 /opt/tomcat/bin/catalina.s ... Up 8080/tcpcomposehaproxyweb_webb_1 /opt/tomcat/bin/catalina.s ... Up 8080/tcp 在 weba 上执行命令，比如 env 12345678910111213141516171819202122232425262728293031$ docker-compose run weba envPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/tomcat/binHOSTNAME=4fd738ffc9b2TERM=xtermCOMPOSEHAPROXYWEB_WEBA_1_PORT=tcp://172.17.0.2:8080COMPOSEHAPROXYWEB_WEBA_1_PORT_8080_TCP=tcp://172.17.0.2:8080COMPOSEHAPROXYWEB_WEBA_1_PORT_8080_TCP_ADDR=172.17.0.2COMPOSEHAPROXYWEB_WEBA_1_PORT_8080_TCP_PORT=8080COMPOSEHAPROXYWEB_WEBA_1_PORT_8080_TCP_PROTO=tcpCOMPOSEHAPROXYWEB_WEBA_1_NAME=/composehaproxyweb_weba_run_1/composehaproxyweb_weba_1COMPOSEHAPROXYWEB_WEBA_1_ENV_TOMCAT_VERSION=8.5.8COMPOSEHAPROXYWEB_WEBA_1_ENV_CATALINA_HOME=/opt/tomcatWEBA_PORT=tcp://172.17.0.2:8080WEBA_PORT_8080_TCP=tcp://172.17.0.2:8080WEBA_PORT_8080_TCP_ADDR=172.17.0.2WEBA_PORT_8080_TCP_PORT=8080WEBA_PORT_8080_TCP_PROTO=tcpWEBA_NAME=/composehaproxyweb_weba_run_1/webaWEBA_ENV_TOMCAT_VERSION=8.5.8WEBA_ENV_CATALINA_HOME=/opt/tomcatWEBA_1_PORT=tcp://172.17.0.2:8080WEBA_1_PORT_8080_TCP=tcp://172.17.0.2:8080WEBA_1_PORT_8080_TCP_ADDR=172.17.0.2WEBA_1_PORT_8080_TCP_PORT=8080WEBA_1_PORT_8080_TCP_PROTO=tcpWEBA_1_NAME=/composehaproxyweb_weba_run_1/weba_1WEBA_1_ENV_TOMCAT_VERSION=8.5.8WEBA_1_ENV_CATALINA_HOME=/opt/tomcatTOMCAT_VERSION=8.5.8CATALINA_HOME=/opt/tomcatHOME=/root 停止后台进程 1234$ docker-compose stopStopping composehaproxyweb_haproxy_1 ... doneStopping composehaproxyweb_webb_1 ... doneStopping composehaproxyweb_weba_1 ... done 问题No such image这是缓存造成的一个问题，可能由于你直接手动删除了 compose 生成的 image。你可以通过 docker-compose ps 查看，也通过 docker-compose rm 直接清除。想要避免这个问题，你需要 docker-compose down。]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 实战（三）：Docker 常用命令]]></title>
    <url>%2F2016%2F11%2F24%2Fdocker-three-command%2F</url>
    <content type="text"><![CDATA[images 查看镜像12345678$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEtomcat latest c6cfe59eb987 6 days ago 357 MBbatizhao/ubuntu latest a15b93298276 9 days ago 297.2 MBubuntu 16.04 f753707788c5 5 weeks ago 127.2 MBubuntu latest f753707788c5 5 weeks ago 127.2 MBswarm latest 942fd5fd357e 3 months ago 19.47 MB ps 查看容器运行中的 1234$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe6eef58b3544 tomcat "catalina.sh run" 16 minutes ago Up 14 minutes 0.0.0.0:32770-&gt;8080/tcp evil_wozniak 所有的 12345678910$ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe6eef58b3544 tomcat "catalina.sh run" 17 minutes ago Up 15 minutes 0.0.0.0:32770-&gt;8080/tcp evil_wozniakd4faf2e9badf tomcat "catalina.sh run" 35 minutes ago Exited (130) 26 minutes ago ecstatic_bohr47d97451e8a1 tomcat "catalina.sh run" 36 minutes ago Exited (130) 35 minutes ago berserk_mclean54c65a764bf0 batizhao/ubuntu "/usr/bin/supervisord" 7 days ago Exited (0) 7 days ago jolly_allen9246f78c4b8f batizhao/ubuntu "/usr/bin/supervisord" 7 days ago Exited (0) 7 days ago stupefied_gatesd49845097aa0 batizhao/ubuntu "/usr/bin/supervisord" 7 days ago Exited (0) 7 days ago sad_stallman311ba7bb9ffd swarm "/swarm --help" 7 days ago Exited (0) 7 days ago naughty_bhaskara run 运行容器启动 MySQL Docker 容器 123$ docker run -itd -e MYSQL_ROOT_PASSWORD=passw0rd -e MYSQL_ROOT_HOST=% -e MYSQL_DATABASE=iwamp2 -p 3306:3306 --name=mysql batizhao/mysql:latest Tomcat Docker 容器连接宿主机 MySQL 123$ docker run -itd -p 9000:8080 --name iwamp -v /Users/batizhao/Downloads/web:/opt/tomcat/webapps --add-host='iwamp2.dev:172.17.0.1' batizhao/tomcat:8-jre8 Tomcat Docker 容器连接 MySQL Docker 容器 123$ docker run -it -p 9000:8080 --name iwamp2 -v /Users/batizhao/Downloads/web:/opt/tomcat/webapps --link mysql:iwamp2.dev batizhao/tomcat:8-jre8 Tomcat Docker 容器连接物理 Oracle 数据库 123$ docker run -itd -p 9000:8080 --name iwamp -v /Users/batizhao/Downloads/web:/opt/tomcat/webapps --add-host='iwamp.dev:172.31.21.216' batizhao/tomcat:8-jre8 rm 删除容器删除单个容器 123$ docker rm 47d97451e8a147d97451e8a1 删除所有停止运行的容器 1$ docker rm $(docker ps -a -q) rmi 删除镜像删除 REPOSITORY 为 swarm，TAG 为 latest 的镜像 123$ docker rmi swarmError response from daemon: conflict: unable to remove repository reference "swarm" (must force) - container 311ba7bb9ffd is using its referenced image 942fd5fd357e 如果提示以上错误，可以先删除所关联容器，或者直接使用 -f 参数 12345$ docker rmi -f swarmUntagged: swarm:latestUntagged: swarm@sha256:c9e1b4d4e399946c0542accf30f9a73500d6b0b075e152ed1c792214d3509d70Deleted: sha256:942fd5fd357e2fe2fcecbaf3dd77c313f22ce18a84a5a4d288c0df407a61e623 批量删除 tag 为 none 的镜像 1234$ docker images|grep none|awk '&#123;print $3&#125;'|xargs docker rmi -fDeleted: sha256:d43fcd0c191c0fa7ae1df73ea59ed374a5e9b5c25788ddc4183800257cc8a38fDeleted: sha256:59e448d53f303721ca12a40513c3dbc8651cc046311186825b9d7eec0805baac port 查看端口映射12$ docker port e6eef58b3544 80800.0.0.0:32770 exec 进入运行中的容器123$ docker exec -it e6eef58b3544 bashroot@e6eef58b3544:/usr/local/tomcat#]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 实战（二）：基于 Dockerfile 构建基础镜像]]></title>
    <url>%2F2016%2F11%2F15%2Fdokcer-two-dockerfile%2F</url>
    <content type="text"><![CDATA[这里使用 Dockerfile 构建一个 Ubuntu 的基础镜像，并且安装了 OpenSSH、Supervisor 等基础服务。使用 Supervisor 可以更好的同时控制多个我们希望执行的程序。 最新的 Docker 版本建议安装 Docker for Mac，原先的 Docker Toolbox 方式已经不建议使用。 创建 Dockerfile 脚本Dockerfile123456789101112131415161718192021222324252627282930313233# Docker file for ubuntu 16FROM ubuntu:16.04# 创建者信息MAINTAINER batizhao &lt;zhaobati@gmail.com&gt;# 使用阿里云镜像RUN sed -i 's/archive.ubuntu.com/mirrors.aliyun.com/g' /etc/apt/sources.list# 安装基础工具包RUN \ apt-get update &amp;&amp; \ apt-get -y upgrade &amp;&amp; \ apt-get install -y vim openssh-server supervisor &amp;&amp; \ rm -rf /var/lib/apt/lists/*# 创建 ssh 和 supervisor 目录RUN mkdir -p /var/run/sshdRUN mkdir -p /var/log/supervisor# 设置 root ssh 远程登录密码为 passwordRUN echo "root:password" | chpasswdRUN sed -i 's/PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_configRUN sed 's@session\s*required\s*pam_loginuid.so@session optional pam_loginuid.so@g' -i /etc/pam.d/sshd# 添加 supervisor 的配置文件COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf# 开放 SSH 22 端口EXPOSE 22# 容器启动命令CMD ["/usr/bin/supervisord"] 创建 Supervisor 脚本在 Dockerfile 文件所在目录创建 supervisord.conf12345[supervisord]nodaemon=true[program:sshd]command=/usr/sbin/sshd -D 在执行以下步骤之前，需要先启动 Docker。 使用 Dockerfile 构建镜像并运行在 Dockerfile 文件所在目录执行命令1$ docker build -t batizhao/ubuntu . 启动 Docker 容器1$ docker run -p 2200:22 -it batizhao/ubuntu 宿主机做为 SSH 客户端1$ ssh root@localhost -p 2200 提交到仓库1$ docker push batizhao/ubuntu]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 实战（一）：在 Mac 上安装配置 Docker]]></title>
    <url>%2F2016%2F11%2F14%2Fdocker-one-install%2F</url>
    <content type="text"><![CDATA[在安装之前最新的 Docker 版本建议安装 Docker for Mac，原先的 Docker Toolbox 方式已经不建议使用。 If you already have an installation of Docker Toolbox, please read these topics first to learn how Docker for Mac and Docker Toolbox differ, and how they can coexist. 新老两种方法对比以及 Toolbox 的卸载Docker for Mac vs. Docker Toolbox 安装Welcome to Docker for Mac! Please read through these topics on how to get started. To give us feedback on your experience with the app and report bugs or problems, log in to our Docker for Mac forum. 基本的安装步骤Get started with Docker for Mac 使用镜像库加速国内访问 Docker 仓库会比较慢，可以使用 2 个加速地址： 阿里云 DaoCloud 注册并登录以上地址可以获得专属加速地址。 在 Mac 上，点击桌面顶栏的 Docker 图标，选择 Preferences ，在 Advanced 标签下的 Registry mirrors 列表中加入专属地址。点击 Apply &amp; Restart 按钮使设置生效。 第一个镜像12345$ docker pull ubuntuUsing default tag: latestlatest: Pulling from library/ubuntuDigest: sha256:2d44ae143feeb36f4c898d32ed2ab2dffeb3a573d2d8928646dfc9cb7deb1315Status: Image is up to date for ubuntu:latest 启动 ubuntu 容器 1$ docker run -it ubuntu]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[edX 学习四：制作课程]]></title>
    <url>%2F2014%2F04%2F18%2Fbuilding-a-course%2F</url>
    <content type="text"><![CDATA[前边提到，主要有两种方式制作课程：Studio 和 LMS(+Github)。这里对这两种方式做简单的说明。 Studio如果取得制作课程的权限，第一次登录的时候是这样的 填入课程具体名称、组织、代码（因为需要根据这些信息生成 URL，所以要注意长度和不能包含特殊字符、空格等） 只能看到自己通过 Studio 制作的课程 edX 的工作人员可以看到所有通过 Studio 制作的课程 创建成功后进入 Course Outline 页面，可以使用 Checklists 完成课程的制作。 Add Course Team Members 团队管理员可以添加或删除团队成员，或授予管理员权限给其他团队成员。其他团队成员可以编辑课程。 Course Outline 对课程内容进行制作 其它的功能还有： 课程导入导出 学生视图（设置课程概览模板、图片、视频、开始结束日期、学生登记日期） 设置功课类型、分数范围、级别和打分策略 这些都可以查看 Checklists 中的 Explore edX’s Support Tools 章节。 LMS(+Github)环境中的示例课程 edx4edx_lite 的所有内容通过 Github 管理，下边是此课程的目录结构。 首先是 course.xml，内容相当于点击“New Course”填写的内容 &lt;?xml version=&quot;1.0&quot;?&gt; &lt;course url_name=&quot;edx4edx&quot; org=&quot;MITx&quot; course=&quot;edx4edx&quot;/&gt; course 目录下的课程同名 edx4edx.xml 文件相当于 Course Outline 的顶级目录 &lt;course graceperiod=&quot;1 day 5 hours 59 minutes 59 seconds&quot; org=&quot;MITx&quot; course=&quot;edx4edx&quot; ispublic=&quot;True&quot; semester=&quot;edx4edx&quot;&gt; &lt;chapter url_name=&quot;Introduction_chapter&quot;/&gt; &lt;chapter url_name=&quot;Assessment_Problems_chapter&quot;/&gt; &lt;chapter url_name=&quot;Author_tools_chapter&quot;/&gt; &lt;/course&gt; Introduction_chapter.xml &lt;?xml version=&quot;1.0&quot;?&gt; &lt;chapter display_name=&quot;Introduction&quot;&gt; &lt;sequential url_name=&quot;edx4edx_Course_sequential&quot;/&gt; &lt;/chapter&gt; Assessment_Problems_chapter.xml &lt;?xml version=&quot;1.0&quot;?&gt; &lt;chapter display_name=&quot;Assessment Problems&quot;&gt; &lt;sequential url_name=&quot;Sample_Problems_sequential&quot;/&gt; &lt;sequential url_name=&quot;Advanced_Problems_Custom_Response_and_Randomization_sequential&quot;/&gt; &lt;sequential url_name=&quot;Advanced_Problems_Hints_sequential&quot;/&gt; &lt;sequential url_name=&quot;Advanced_Problems_Scripts_and_Javascript_sequential&quot;/&gt; &lt;sequential url_name=&quot;Advanced_Problems_Code_Grading_sequential&quot;/&gt; &lt;sequential url_name=&quot;Rich_Interface_Examples&quot;/&gt; &lt;/chapter&gt; edx4edx_Course_sequential.xml &lt;?xml version=&quot;1.0&quot;?&gt; &lt;sequential format=&quot;&quot; Due=&quot;Dec 12-25&quot; display_name=&quot;edx4edx Course&quot;&gt; &lt;vertical url_name=&quot;edx4edx_Course_vertical&quot;/&gt; &lt;/sequential&gt; edx4edx_Course_vertical.xml &lt;?xml version=&quot;1.0&quot;?&gt; &lt;vertical display_name=&quot;edx4edx_Course_vertical&quot;&gt; &lt;html url_name=&quot;edx4edx_Course_html&quot;/&gt; &lt;/vertical&gt; 整个层级关系是 course - chapter - sequential - problem(or vertical) - html]]></content>
      <categories>
        <category>mooc</category>
      </categories>
      <tags>
        <tag>edx</tag>
        <tag>mooc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[edX 学习三：开始课程制作之前]]></title>
    <url>%2F2014%2F04%2F15%2Fedx-course-management%2F</url>
    <content type="text"><![CDATA[在制作课程之前，先要知道下 edX.org 和 edX Edge 的关系。 edX.org 和 edX Edge从外观来看，这两个站点几乎是一样的，但内容和目的不同。这是两个完全独立的站点，包括所有用户信息、课程数据、数据库、服务器都是独立的。现在有很多的课程都即将在 Edge 毕业，在将来成为 edX 上的正式公开课程。但也有很多课程不需要经过 Edge，而直接在 edX 上开课。官方建议在 edX 上正式开课前，先在 Edge 上做一些测试和学习。 edX.orgedX.org 的在线课程来自 edX 合作伙伴。在和 edX 签定协议之后，可以在 edX.org 上发布公开课程，并开放给来自世界各地的学生。课程通过 studio.edx.org 管理。 edX.org studio.edx.org edX EdgeEdge 是一个私有内容站点。在这里课程不是公开的，没有课程目录，也不能被搜索引擎索引，只有受到明确邀请并且知道具体的课程 URL 才可以访问课程。课程通过 studio.edge.edx.org 管理。 没有任何公开课程，需要先注册登录 注册激活，登录以后看不到任何课程内容，没有课程目录，也没有 “Find Courses” 按钮 只能通过某个 URL，例如官方提供的 edX101 注册课程 studio.edge.edx.org，和 edx.org 不同的是这里不需要给 edx 发邮件，只需要点击“Request the Ability to Create Courses”等待审核通过（需要提供大学或者课程相关的名字）。 制作课程 LMS（+ Github） – 这种方式结合 Github ，可以直接编辑课程相关 XML 文件，实现课程的版本控制，保留课程修改纪录。通过一个 webhook 实现课程的自动更新。上篇提到的 edx4edx_lite 就是这种方式。 Studio – Studio 是用于构建课程的 edX 工具。这是一种可视化、所见即所得的编辑方式，基于 Web 界面。可以使用 Studio 为学生来创建课程内容、问题、视频和其他资源。使用 Studio，可以管理日程安排和课程团队、设置分级策略、发布课程，等等。可以直接通过浏览器使用的 Studio，不需要任何额外的软件。但只能单人编辑工作、没有课程修改纪录。课程内容数据存储在 MongoDB。 使用 LaTeX 制作课程LaTeX 是一个强大的排版系统，广泛应用于数学、科技、工程等学术领域（本篇 Markdown 格式内容转换成 PDF 时也需要使用到 LaTeX 模板），非常适合于制作一些科学图表、数学公式，很多学术论文都使用了此系统。 MIT 开源了 latex2edx，使用 LaTeX 来制作整个或部分 edX 课程。 latex2edx 已被用来生产许多 MITX 上 edX 课程，包括 8.01x、8.02x、16.101x 等。它可以通过一个 TEX 模板，同时生成一个在线课程，以及一个 PDF 文件。这种其实是第一种方式。]]></content>
      <categories>
        <category>mooc</category>
      </categories>
      <tags>
        <tag>edx</tag>
        <tag>mooc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[edX 学习二：搭建 edX 平台]]></title>
    <url>%2F2014%2F04%2F14%2Fquick-start-to-working-with-the-edx-platform%2F</url>
    <content type="text"><![CDATA[折腾了好几天，试过官方的 edX Developer Stack 和 edX Ubuntu 12.04 installation 两种方法没成功之后，终于按照 Quick Start to working with the edX Platform 这篇文章搞定，缺点就是不是最新的 edX 环境。 按照官方文档，在安装时主要会卡在两个地方（主要还是网络问题）： 安装 MongoDB（这个地方时间长点还可以执行下去，大概 10 几个小时） 安装 ORA（花了一天一夜也没执行下去） 最后这两种方法都放弃，使用了直接下载 box 的方式。就是 box 文件下载需要点时间，我家里 20M 光纤用了 4 个小时左右。 使用 MITxVM distribution 安装The MITxVM box is built on a base Ubuntu 12.04LTS distribution. The edX platform runs using django/python, and is served via gunicorn and nginx. Virtual Box is used to provide a host-only network, 192.168.42.*. The four edX services listen on eth0 on four separate IP addresses. The system uses mysql for the main database, and mongo for Studio. Installed repos include edx-platform, xqueue, xserver, latex2edx, edx-ora, ease. 1. 安装 Vagrant 和 VirtualBox Vagrant VirtualBox 2. 下载示例课程 edx4edx_lite 到 data 目录# mkdir mitx-vagrant # cd mitx-vagrant # mkdir data # cd data # git clone https://github.com/mitocw/edx4edx_lite.git 3. 在 mitx-vagrant 启动虚拟机。下载 mitxvm-edx-platform-02sep13a.box (large file: 3.8 GB) md5sum: 7d3671a92f8ba4f8e6b54db91a90bc91 # vagrant init mitxvm mitxvm-edx-platform-02sep13a.box # vagrant up ... ==&gt; default: Configuring and enabling network interfaces... ==&gt; default: Mounting shared folders... default: /vagrant =&gt; /opt/mitx-vagrant ==&gt; default: VM already provisioned. Run `vagrant provision` or use `--provision` to force it 4. 访问站点 http://192.168.42.2 – LMS（www.edx.org，这个环境没有找到 studio.edx.org 对应的站点） http://192.168.42.2/admin – LMS 后台管理，需要通过下边的 setstaff 命令激活访问账号 http://192.168.42.5 – Edge（edge.edx.org） http://192.168.42.3 – Edge Studio (studio.edge.edx.org) http://192.168.42.4 – Preview 登录账号: email “xadmin@mitxvm.local”，password “xadmin”。 如果要创建用户，激活用户需要使用 “xmanage” 命令，否则无法收到激活邮件。 登录以后，可以看到 edx4edx_lite 这个示例课程。 5. 虚拟机管理MITx virtual machine Vagrant box 使用 xmanage： # vagrant ssh -- xmanage help ommands available: restart-lms - restart the LMS (for vagrant boxes, running at http://192.168.42.2) This will force re-loading of course data restart-cms - restart the CMS (aka the Studio system) restart-edge - restart the Edge server (part of the Studio system) restart-preview - restart the Preview server (part of the Studio system) restart-xqueue - restart the xqueue main system restart-consumer - restart the xqueue consumer restart-xserver - restart the xserver (python code grader) logs &lt;appname&gt; - view last 100 lines of log file for &lt;appname&gt; appname should be one of lms, cms, edge, preview, xserver, xqueue activate &lt;user&gt; - activate user specified by username &lt;user&gt; setstaff &lt;user&gt; - make user (specified by username &lt;user&gt;) into staff update-mitx - update mitx system code (use with care!) update - update this management script (from central repo) help - print out this message, as well as local NOTES.txt file 5. 问题 Vagrant error : Failed to mount folders in Linux guest # vagrant ssh # sudo ln -s /opt/VBoxGuestAdditions-4.3.10/lib/VBoxGuestAdditions /usr/lib/VBoxGuestAdditions # vagrant reload Unknown command: ‘activate_user’ 这里发现 xmanage activate 命令不可用，其它命令正常。在 Google Groups 上看到这个问题，但没解决。如果遇到以下错误，这里可以通过 admin 登录来激活。 $ xmanage activate batizhao activating user batizhao Unknown command: &apos;activate_user&apos; Type &apos;django-admin.py help&apos; for usage. To complete the activation, please logout then log back in]]></content>
      <categories>
        <category>mooc</category>
      </categories>
      <tags>
        <tag>edx</tag>
        <tag>mooc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[edX 学习一：了解 edX 相关组件]]></title>
    <url>%2F2014%2F04%2F11%2Flearning-edx-one%2F</url>
    <content type="text"><![CDATA[edX 概览edX 平台主要采用 Python 语言实现，还包括了个别的 Ruby 和 Node.js。部分服务使用到了 Java ，数据库使用了 MySQL 和 MongoDB，所有这些代码都是在 AGPL 协议下开源的。 XBlock 是 edX 的下一代组件架构。对机器学习评分感兴趣可以看 Ease 和 Discern 。部署和配置工具方面可以看 Configuration。各主要组件的详细介绍可以看下边。 edx-platform主代码库 edx-platform 中包含 LMS（Learning Management System）、创作工具（Authoring Tool）、制作环境（Studio）。 它还包括 XModules（课件组件，在接下来的几个月准备升级到新 XBlock 架构）和不同的评分器。 XBlockXBlock 是一个构造课件的组件。 XBlock API 目前还处在 pre-alpha，这个代码库中包含了 XBlock 实现的核心代码，一个简单的应用工作台和一个小型的、简单的运行环境。 ORA (Open Response Assessor)Open Response Assessor（开放响应评估员）从 XQueue 获得消息，通过机器学习的评分、同行的评分和适当的人工评分，并返回结果到 LMS。 Discern 和 EaseDiscern 是一个允许任何人使用的基于机器学习的自动文本化分类 API 服务。提供了一个高性能的、可扩展的解决方案，可以有效地帮助学生学习。反馈是这个过程的一个重要组成部分，该反馈系统非常灵活。注意，你需要 Ease 代码库使用这里所有的功能。 Ease（增强的 AI 评分引擎）是一个基于机器学习的文本内容分类库。这对于诸如学生论文评分的任务非常有用。它提供预测任意文本和数值得分的功能。这里的目标是提供一个高性能的，可扩展的解决方案，可以预测任意值的目标。请注意，这仅仅是一个库。你需要实现你自己的代码才可以运行。Discern API 是对 Ease 的进一步封装。 CS Comments Service这是一个独立的评论系统，支持投票和嵌套的评论。它还是一个教师备课和教学目标讨论平台。这个系统的不同之处是用 Ruby 实现，并且用到了 elasticsearch 做分布式实时搜索引擎。 ConfigurationConfiguration 提供了一个简单、灵活的配置，让任何人都能搭建 edX 平台，运行一个实例。使用 Amazon CloudFormation Template 创建一个新的 AWS 虚拟私有云。配置阶段是由 Ansible 管理的。 CodeJailCodeJail 提供了一个安全沙盒，管理不受信任代码的执行。它的主要目的是为 Python 执行，也可用于其他语言。强制使用 AppArmor ，如果操作系统不支持 AppArmor，那么 CodeJail 不会起到作用。 XQueueXQueue 定义了一个 LMS 和外部服务的接口，实际使用的是 RabbitMQ。例如，当一个学生在 LMS 提交一个问题，它就会被发送到XQueue。XQueue 有一个外部服务分级的机制，然后发回响应到 LMS。 XServerXServer 接受学生从 LMS 提交的代码，并使用课件检查器运行。 NotifierNotifier 为订阅用户发送新内容的每日摘要，目标是最终支持实时和各种类型的内容在不同渠道（如短信）的批量通知。 Django-wiki一个有着出色的界面和复杂的功能的维基系统、知识库。]]></content>
      <categories>
        <category>mooc</category>
      </categories>
      <tags>
        <tag>edx</tag>
        <tag>mooc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK 7 - INSTALL_PARSE_FAILED_NO_CERTIFICATES]]></title>
    <url>%2F2013%2F11%2F06%2Fjdk-7---install_parse_failed_no_certificates%2F</url>
    <content type="text"><![CDATA[在 JDK7 环境，在 android deploy 时会出现一个 INSTALL_PARSE_FAILED_NO_CERTIFICATES 的错误。如果使用 Maven 打包，需要在 maven-jarsigner-plugin 中增加以下参数 &lt;arguments&gt; &lt;argument&gt;-sigalg&lt;/argument&gt; &lt;argument&gt;MD5withRSA&lt;/argument&gt; &lt;argument&gt;-digestalg&lt;/argument&gt; &lt;argument&gt;SHA1&lt;/argument&gt; &lt;/arguments&gt; What is the difference between the Java 1.6 and 1.7 jarsigner http://stackoverflow.com/questions/8739564/what-is-the-difference-between-the-java-1-6-and-1-7-jarsigner/]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>maven</tag>
        <tag>jdk7</tag>
        <tag>android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 Mac10.9 和 JDK7 环境中运行 IntelliJ IDEA]]></title>
    <url>%2F2013%2F10%2F31%2Fhow-do-i-run-idea-intellij-on-mac-os-x-with-jdk-7%2F</url>
    <content type="text"><![CDATA[升级到 Mavericks 之后，需要自行安装 JDK 环境。如果你安装了 JDK7，那么在安装之后，在命令行中运行 javac、java 命令没有问题，但是 Eclipse 和 IDEA 都无法启动，系统要求安装 JDK6。 修改 jdk Info.plist增加后边的 4 个 string。/Library/Java/JavaVirtualMachines/jdk1.7.0_XX.jdk/Contents/Info.plist1234567&lt;array&gt; &lt;string&gt;CommandLine&lt;/string&gt; &lt;string&gt;JNI&lt;/string&gt; &lt;string&gt;BundledApp&lt;/string&gt; &lt;string&gt;WebStart&lt;/string&gt; &lt;string&gt;Applets&lt;/string&gt;&lt;/array&gt; 修改 IDEA Info.plist，修改 JVMVersion 从 1.6* 到 1.7*。/Applications/IntelliJ IDEA XXX.app/Contents/Info.plist file12&lt;key&gt;JVMVersion&lt;/key&gt;&lt;string&gt;1.7*&lt;/string&gt; 启动 IDEA12，按以下步骤选择相应的 JDK Home。File - Project Structure - Project - New - JDK 如果是 Eclipse，那做完第一步就可以正常启动了；如果是 IDEA，那么需要三步都做完整。 修复 mvn -v 错误1234567891011$ mvn -vError: JAVA_HOME is not defined correctly. We cannot execute /System/Library/Frameworks/JavaVM.framework/Versions/CurrentJDK/Home/bin/java$ vim /etc/profileexport JAVA_HOME=`/usr/libexec/java_home -v 1.7`export PATH=$JAVA_HOME/bin:$PATH$ source /etc/profile]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>jdk7</tag>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Solr（二）: 整合 MySQL]]></title>
    <url>%2F2013%2F06%2F28%2Fsolr-and--mysql%2F</url>
    <content type="text"><![CDATA[找到配置文件： ${solr/home}/collection1/conf/solrconfig.xml 增加以下配置： &lt;requestHandler name=&quot;/dataimport&quot; class=&quot;org.apache.solr.handler.dataimport.DataImportHandler&quot;&gt; &lt;lst name=&quot;defaults&quot;&gt; &lt;str name=&quot;config&quot;&gt;data-config.xml&lt;/str&gt; &lt;/lst&gt; &lt;/requestHandler&gt; 在 conf 下新建 data-config.xml &lt;dataConfig&gt; &lt;dataSource type=&quot;JdbcDataSource&quot; driver=&quot;com.mysql.jdbc.Driver&quot; url=&quot;jdbc:mysql://localhost:3306/test?useUnicode=true&amp;amp;characterEncoding=utf8&quot; user=&quot;root&quot; password=&quot;&quot; /&gt; &lt;document name=&quot;documents&quot;&gt; &lt;entity name=&quot;user&quot; pk=&quot;id&quot; query=&quot;select * from user&quot; deltaImportQuery=&quot;select * from user where id=&apos;${dataimporter.delta.id}&apos;&quot; deltaQuery=&quot;select id from user where last_modified &gt; &apos;${dataimporter.last_index_time}&apos;&quot; &lt;field column=&quot;id&quot; name=&quot;id&quot; /&gt; &lt;field column=&quot;username&quot; name=&quot;username&quot; /&gt; &lt;/entity&gt; &lt;/document&gt; &lt;/dataConfig&gt; 复制相关的 jar 包到 solr lib cp /opt/solr/dist/solr-dataimporthandler-4.3.0.jar /opt/tomcat/webapps/solr/WEB-INF/lib cp /opt/solr/dist/solr-dataimporthandler-extras-4.3.0.jar /opt/tomcat/webapps/solr/WEB-INF/lib cp /opt/solr/example/solr-webapp/webapp/WEB-INF/lib/mysql-connector-java-5.1.6.jar /opt/tomcat/webapps/solr/WEB-INF/lib 修改 schema.xml（其它的字段已经存在） &lt;field name=&quot;username&quot; type=&quot;text_general&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&gt; MySQL 脚本 CREATE TABLE `user` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `username` varchar(50) DEFAULT NULL, `last_modified` timestamp NULL DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (`id`) ); INSERT INTO `user` (`id`, `username`, `last_modified`) VALUES (1001,&apos;zhangsan&apos;,&apos;2013-06-27 13:01:44&apos;), (1002,&apos;lisi&apos;,&apos;2013-06-28 14:00:22&apos;), (1003,&apos;wangwu&apos;,&apos;2013-06-28 11:04:23&apos;); 批量导入（full-import）： http://localhost:8080/solr/dataimport?command=full-import&amp;commit=true 增量导入（delta-import）： http://localhost:8080/solr/dataimport?command=delta-import&amp;commit=true 导入状态查询（status）： http://localhost:8080/solr/dataimport 重新装载配置文件（reload-config）： http://localhost:8080/solr/dataimport?command=reload-config 终止导入（abort）： http://localhost:8080/solr/dataimport?command=abort]]></content>
      <categories>
        <category>solr</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>solr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Solr（一）: 在 Tomcat7 上安装]]></title>
    <url>%2F2013%2F05%2F29%2Finstalling-solr-on-tomcat%2F</url>
    <content type="text"><![CDATA[创建 solr 安装目录 # mkdir -p /opt/solr 解压缩 # unzip solr.zip -d /opt/solr 这时，可以直接使用 jetty 启动 # cd /opt/solr/example # java -jar start.jar 或者部署 war 到 tomcat cp /opt/solr/example/webapps/solr.war /opt/tomcat/webapps 启动 tomcat，会遇到一个错误 Context [/solr] startup failed due to previous errors 查看 localhost.yyyy-mm-dd.log 会发现是一个 SLF4j 的问题 org.apache.solr.common.SolrException: Could not find necessary SLF4j logging jars. If using Jetty, the SLF4j logging jars need to go in the jetty lib/ext directory. For other containers, the corresponding directory should be used. 要解决这个问题，执行以下步骤 # cp /opt/solr/example/lib/ext/* /opt/tomcat/webapps/solr/WEB-INF/lib # mkdir -p /opt/tomcat/webapps/solr/WEB-INF/classes # cp /opt/solr/example/resources/log4j.properties /opt/tomcat/webapps/solr/WEB-INF/classes 最后，需要设置 solr/home，这里直接指向 /opt/solr/example/solr，只需要修改 solr 的 web.xml &lt;env-entry&gt; &lt;env-entry-name&gt;solr/home&lt;/env-entry-name&gt; &lt;env-entry-value&gt;/opt/solr/example/solr&lt;/env-entry-value&gt; &lt;env-entry-type&gt;java.lang.String&lt;/env-entry-type&gt; &lt;/env-entry&gt; 中文的问题，需要修改 tomcat 的 server.xml，在 Connector 中增加以下参数 URIEncoding=&quot;UTF-8&quot; 这时，可以再次启动 tomcat 了。]]></content>
      <categories>
        <category>tomcat</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
        <tag>solr</tag>
        <tag>jetty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Simple-Spring-Memcached: AssignCache]]></title>
    <url>%2F2012%2F09%2F28%2Fusing-simple-spring-memcached-three%2F</url>
    <content type="text"><![CDATA[接上一篇的 MultiCache ，这篇主要讲一下 AssignCache 的使用。继续解决上一个场景中没有解决的问题。 根据某几个 role ID 查询所有的 User。查询关联两个以上的 Model，在更新其中一个时，需要让相关的缓存失效 Method: @Override @ReadThroughAssignCache(assignedKey = &quot;user/getUsersByRoleIds&quot;, namespace = &quot;user&quot;, expiration = 60) public List&lt;User&gt; getUsersByRoleIds(@ParameterValueKeyProvider final List&lt;Long&gt; ids) { return (List&lt;User&gt;) sqlMapClientTemplate.queryForList(&quot;getUsersByRoleIds&quot;, ids); } SQL: &lt;select id=&quot;getUsersByRoleIds&quot; parameterClass=&quot;list&quot; resultClass=&quot;user&quot;&gt; SELECT u.id, u.name, r.id as &quot;role.id&quot;, r.name as &quot;role.name&quot; FROM user u, user_role ur, role r WHERE u.id = ur.userid and r.id = ur.roleid and ur.roleid in (&lt;iterate conjunction=&quot;,&quot;&gt; #ids[]# &lt;/iterate&gt;) order by r.id &lt;/select&gt; Unit Test: @Test public void testGetUsersByRoleIds() throws Exception { List&lt;Long&gt; list = Arrays.asList(1L, 2L); List users = userDao.getUsersByRoleIds(list); log.info(&quot;users: &quot; + users); assertNotNull(users); assertEquals(3, users.size()); } Log4j log: set user:user/getUsersByRoleIds 8 60 378 {&quot;v&quot;:{&quot;java.util.ArrayList&quot;:[{&quot;me.batizhao.model.User&quot;:{&quot;id&quot;:1000,&quot;name&quot;:&quot;Tom&quot;,&quot;role&quot;:{&quot;me.batizhao.model.Role&quot;:{&quot;id&quot;:1,&quot;name&quot;:&quot;ROLE_ADMIN&quot;}}}},{&quot;me.batizhao.model.User&quot;:{&quot;id&quot;:1002,&quot;name&quot;:&quot;Jack&quot;,&quot;role&quot;:{&quot;me.batizhao.model.Role&quot;:{&quot;id&quot;:1,&quot;name&quot;:&quot;ROLE_ADMIN&quot;}}}},{&quot;me.batizhao.model.User&quot;:{&quot;id&quot;:1001,&quot;name&quot;:&quot;Jerry&quot;,&quot;role&quot;:{&quot;me.batizhao.model.Role&quot;:{&quot;id&quot;:2,&quot;name&quot;:&quot;ROLE_USER&quot;}}}}]}} 这个场景不太适合做 @UpdateAssignCache，这里使用 Simple-Spring-Memcached: SingleCache 中的 UserCache 类，增加一个方法 @InvalidateAssignCache(assignedKey = &quot;user/getUsersByRoleIds&quot;, namespace = &quot;user&quot;) public void invalidategetUsersByRoleIds(){ } 然后在 RoleManagerImpl.updateRole() 中增加一句 userCache.invalidategetUsersByRoleIds(); 先执行 DAO Unit Test: @Test public void testGetUsersByRoleIds() throws Exception { List&lt;Long&gt; list = Arrays.asList(1L, 2L); List users = userDao.getUsersByRoleIds(list); log.info(&quot;users: &quot; + users); assertNotNull(users); assertEquals(3, users.size()); } Memcached Log: &lt;21 get user:user/getUsersByRoleIds &gt;21 END &lt;21 set user:user/getUsersByRoleIds 8 60 378 &gt;21 STORED 再执行 Service Unit Test: @Test public void testUpdateRole() throws Exception { Role role = new Role(); role.setId(1L); role.setName(&quot;ROLE_ADMIN&quot;); roleManager.updateRole(role); role = roleManager.getRole(1L); log.info(&quot;Role: &quot; + role); assertEquals(&quot;ROLE_ADMIN&quot;, role.getName()); } Memcached Log: &lt;21 delete user:user/getUsersByRoleIds &gt;21 DELETED 以上章节的所有代码在 Github.]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>memcached</tag>
        <tag>spring</tag>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Simple-Spring-Memcached Annotation]]></title>
    <url>%2F2012%2F09%2F27%2Fsimple-spring-memcached-annotation%2F</url>
    <content type="text"><![CDATA[因为公司有的项目还运行在 MyBatis2 上边，并且 SSM 暂时还不可以直接使用在 MyBatis3 上边（可以通过 Spring Cache 或者直接使用 mybatis-memcached 来实现，但后一种方式不适合那种需要对 Cache 进行精细控制的场景）。所以这里主要写一下 SSM3 Annotation 的使用。完整的Spring3 + Memcached + MyBatis2 代码在 ssm3-mybatis2-memcached。 1. SSM AnnotationSingleCache 类操作单个 POJO 的 Cache 数据，由 ParameterValueKeyProvider 和 CacheKeyMethod 来标识组装 key。 Java Code: @ReadThroughSingleCache(namespace = &quot;user&quot;, expiration = 600) public User getUser(@ParameterValueKeyProvider Long id) Memcache Log: get user:1 set user:1 MultiCache 类操作 List 型的 Cache 数据（看做是 SingleCache 的批处理），由 ParameterValueKeyProvider 和 CacheKeyMethod 来标识组装 key。 Java Code: @ReadThroughMultiCache(namespace = &quot;user/getUsersByUserIds&quot;, expiration = 600) public List&lt;User&gt; getUsersByUserIds(@ParameterValueKeyProvider final List&lt;Long&gt; ids) If ids=[1,2,3], Then Memcache Log: get user:1 user:2 user:3 set user:1 set user:2 set user:3 AssignCache 类操作所有类型的 Cache 数据。适用于无参方法或者需要自定义 Key 的场景。指定 key 操作 Cache 数据，由 annotation 中的 assignedKey 指定 key。 Java Code: @ReadThroughAssignCache(assignedKey = &quot;user/getAllUsers&quot;, namespace = &quot;role&quot;, expiration = 600) public List&lt;User&gt; getAllUsers() 2. SingleCache Annotation@ReadThroughSingleCache作用：生成 Cache。 过程： 在执行方法之前先检查缓存。 如果找到相应的 Key，返回 Value，方法 return。 如果没有找到相应的 Key，查询数据库，再 set cache，方法 return。 Key 生成规则：ParameterValueKeyProvider 指定的参数，如果该参数对象中包含 CacheKeyMethod 注解的方法，则调用其方法，否则调用 toString 方法。 @InvalidateSingleCache作用：清除 Cache 中的数据。 key 生成规则： 使用 ParameterValueKeyProvider 注解时，与 ReadThroughSingleCache 一致。 使用 ReturnValueKeyProvider 注解时，key 为返回的对象的 CacheKeyMethod 或 toString 方法生成。 Java Code: @InvalidateSingleCache(namespace = &quot;user/list&quot;) public void invalidateGetUsersByRoleId(@ParameterValueKeyProvider Long id) @UpdateSingleCache作用：更新 Cache 中的数据。 Key 生成规则：ParameterValueKeyProvider 指定。 ParameterDataUpdateContent：方法参数中的数据，作为更新缓存的数据。 ReturnDataUpdateContent：方法调用后生成的数据，作为更新缓存的数据。 上述两个注解，必须与 Update* 系列的注解一起使用。 Java Code: @UpdateSingleCache(namespace = &quot;user&quot;, expiration = 60) public void updateUser(@ParameterValueKeyProvider @ParameterDataUpdateContent User user) @UpdateSingleCache(namespace = &quot;user&quot;, expiration = 60) @ReturnDataUpdateContent public void updateUser(@ParameterValueKeyProvider User user) 参考：使用SSM注解做缓存操作]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>memcached</tag>
        <tag>spring</tag>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Simple-Spring-Memcached: MultiCache]]></title>
    <url>%2F2012%2F09%2F27%2Fusing-simple-spring-memcached-two%2F</url>
    <content type="text"><![CDATA[上一篇讲了 SingleCache ，这篇主要讲一下 MultiCache 的使用。在此之前，要先理解一下 Namespace 和 Key 两个参数。在 Memcached，读写数据都是根据 namespace 和 key 来进行的。 Namespace 和 Key这里首先要理解 SSM 中的 namespace 这个参数。其实这个参数主要是和你的方法返回结果相关。在这篇和上一篇联系起来看，可以使用同一个 namespace。这里可以把 MultiCache 看成 SingleCache 的批量操作。因为无论是 SingleCache 还是 MultiCache ，最终的 Cache 操作其实就是 get user:id {“v”:{“me.batizhao.model.User”:{“id”:1000,”name”:”Messi”}}} set user:id {“v”:{“me.batizhao.model.User”:{“id”:1000,”name”:”Messi”}}} 如果不理解 namespace 和 ParameterValueKeyProvider，会带来的问题是 缓存命中率（大大影响缓存使用效率） 内容空间（占用更多的内存） 需要更多的清除操作（程序复杂度增加） 场景一：根据某几个 user ID 查询所有的 User。在更新某个 user 时，同时更新相关的缓存Method: @Override @ReadThroughMultiCache(namespace = &quot;user&quot;, expiration = 60) public List&lt;User&gt; getUsersByUserIds(@ParameterValueKeyProvider List&lt;Long&gt; ids) { return (List&lt;User&gt;) sqlMapClientTemplate.queryForList(&quot;getUsersByUserIds&quot;, ids); } SQL: &lt;select id=&quot;getUsersByUserIds&quot; parameterClass=&quot;list&quot; resultClass=&quot;user&quot;&gt; SELECT u.id, u.name FROM user u WHERE u.id in (&lt;iterate conjunction=&quot;,&quot;&gt; #ids[]# &lt;/iterate&gt;) &lt;/select&gt; Memcached Log: &lt;21 get user:1000 user:1001 &gt;21 sending key user:1000 &gt;21 END &lt;21 set user:1001 &gt;21 STORED 更新。这里 Batch 调用 updateUser，因为方法没有返回值，所以只能使用 @ParameterDataUpdateContent @Override @UpdateMultiCache(namespace = &quot;user&quot;, expiration = 60) public void updateUsersByUserIds(@ParameterValueKeyProvider @ParameterDataUpdateContent final List&lt;User&gt; users) { sqlMapClientTemplate.execute(new SqlMapClientCallback() { @Override public Object doInSqlMapClient(SqlMapExecutor executor) throws SQLException { executor.startBatch(); for(User user: users){ executor.update(&quot;updateUser&quot;, user); } executor.executeBatch(); return null; } }); } 看一下 Memcached Log，user:1000 直接从 Cache 返回，只有 user:1001 做了数据库操作。这是因为我在执行 getUsersByUserIds 之前，执行了上一篇中的 getUser(1000L)，先生成了 Cache。 场景二：根据某几个 role ID 查询所有的 User。查询关联两个以上的 Model，在更新其中一个时，需要让相关的缓存失效Method: @Override @ReadThroughMultiCache(namespace = &quot;user/getUsersByRoleIds&quot;, expiration = 60) public List&lt;User&gt; getUsersByRoleIds(@ParameterValueKeyProvider final List&lt;Long&gt; ids) { return (List&lt;User&gt;) sqlMapClientTemplate.queryForList(&quot;getUsersByRoleIds&quot;, ids); } 这里为什么不用 namespace = “user”？ SQL: &lt;select id=&quot;getUsersByRoleIds&quot; parameterClass=&quot;list&quot; resultClass=&quot;user&quot;&gt; SELECT u.id, u.name, r.id as &quot;role.id&quot;, r.name as &quot;role.name&quot; FROM user u, user_role ur, role r WHERE u.id = ur.userid and r.id = ur.roleid and ur.roleid in (&lt;iterate conjunction=&quot;,&quot;&gt; #ids[]# &lt;/iterate&gt;) &lt;/select&gt; 看一下返回的结果集： +------+-------+---------+------------+ | id | name | role.id | role.name | +------+-------+---------+------------+ | 1000 | Tom | 1 | ROLE_ADMIN | | 1001 | Jerry | 2 | ROLE_USER | | 1002 | Jack | 1 | ROLE_ADMIN | +------+-------+---------+------------+ 测试代码是这样的： @Test public void testGetUsersByRoleIds() throws Exception { List&lt;Long&gt; list = Arrays.asList(1L, 2L); List users = userDao.getUsersByRoleIds(list); log.info(&quot;users: &quot; + users); assertNotNull(users); assertEquals(3, users.size()); } 这个方法不会生成任何缓存。SSM 会告诉你 com.google.code.ssm.aop.ReadThroughMultiCacheAdvice: Did not receive a correlated amount of data from the target method. 因为 SSM 会先根据结果集生成缓存（使用返回对象 User 的 @CacheKeyMethod 方法） set user/getUsersByRoleIds:1 user/getUsersByRoleIds:2 但是返回的结果集中没有 1 和 2，就算有，也不是我们想要的 role.id，而是 user.id。刚开始我尝试和 SSM 的开发者沟通这个问题。他让我尝试修改一下 annotation @Override @ReadThroughMultiCache(namespace = &quot;user/getUsersByRoleIds&quot;, expiration = 60, option = @ReadThroughMultiCacheOption(generateKeysFromResult = true)) public List&lt;User&gt; getUsersByRoleIds(@ParameterValueKeyProvider final List&lt;Long&gt; ids) { return (List&lt;User&gt;) sqlMapClientTemplate.queryForList(&quot;getUsersByRoleIds&quot;, ids); } 之后变成这个样子： get user/getUsersByRoleIds:1 user/getUsersByRoleIds:2 set user/getUsersByRoleIds:1000 8 60 120 {&quot;v&quot;:{&quot;me.batizhao.model.User&quot;:{&quot;id&quot;:1000,&quot;name&quot;:&quot;Tom&quot;,&quot;role&quot;:{&quot;me.batizhao.model.Role&quot;:{&quot;id&quot;:1,&quot;name&quot;:&quot;ROLE_ADMIN&quot;}}}}} set user/getUsersByRoleIds:1001 8 60 121 {&quot;v&quot;:{&quot;me.batizhao.model.User&quot;:{&quot;id&quot;:1001,&quot;name&quot;:&quot;Jerry&quot;,&quot;role&quot;:{&quot;me.batizhao.model.Role&quot;:{&quot;id&quot;:2,&quot;name&quot;:&quot;ROLE_USER&quot;}}}}} set user/getUsersByRoleIds:1002 8 60 121 {&quot;v&quot;:{&quot;me.batizhao.model.User&quot;:{&quot;id&quot;:1002,&quot;name&quot;:&quot;Jack&quot;,&quot;role&quot;:{&quot;me.batizhao.model.Role&quot;:{&quot;id&quot;:1,&quot;name&quot;:&quot;ROLE_ADMIN&quot;}}}}} 在这个返回的 List 中，role.id 不可以做为 Key，因为不是唯一的。试了一下返回 HaspMap，@ReadThroughMultiCache 只支持 List。所以想到两种解决办法： 循环调用上一篇中的 getUsersByRoleId 方法。 使用 @ReadThroughAssignCache（缺点是缓存做为一整块，不能像 @ReadThroughMultiCache 一样对单个 User 做操作了），到时只能整体清除。 第一个方案可以自行试验一下，下一篇讲 AssignCache 如何解决这个问题。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>memcached</tag>
        <tag>spring</tag>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 Simple-Spring-Memcached: SingleCache]]></title>
    <url>%2F2012%2F09%2F27%2Fusing-simple-spring-memcached-one%2F</url>
    <content type="text"><![CDATA[上一篇大概讲了一下 SSM anotation。这章详细看一下 SingleCache 的使用。 首先是接下来的几个内容都会用到的两个 POJORole Model: public class Role implements Serializable { private static final long serialVersionUID = -4708064835003250669L; private Long id; private String name; @CacheKeyMethod public String cacheKey() { return id.toString(); } public Long getId() { return id; } public void setId(Long id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public boolean equals(Object obj) { return EqualsBuilder.reflectionEquals( this, obj); } public int hashCode() { return HashCodeBuilder .reflectionHashCode(this); } public String toString() { return ToStringBuilder.reflectionToString( this, ToStringStyle.MULTI_LINE_STYLE); } } User Model: public class User implements Serializable { private static final long serialVersionUID = -822125371522084989L; private Long id; private String name; private Role role; @CacheKeyMethod public String cacheKey() { return id.toString(); } public Long getId() { return id; } public void setId(Long id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public Role getRole() { return role; } public void setRole(Role role) { this.role = role; } public boolean equals(Object obj) { return EqualsBuilder.reflectionEquals( this, obj); } public int hashCode() { return HashCodeBuilder .reflectionHashCode(this); } public String toString() { return ToStringBuilder.reflectionToString( this, ToStringStyle.MULTI_LINE_STYLE); } } 场景一：根据某个 user ID 查询某个 User。在更新时，更新缓存中的这个 User。Method: @Override @ReadThroughSingleCache(namespace = &quot;user&quot;, expiration = 600) public User getUser(@ParameterValueKeyProvider Long id) { return (User) sqlMapClientTemplate.queryForObject(&quot;getUser&quot;, id); } @Override @UpdateSingleCache(namespace = &quot;user&quot;, expiration = 60) public void updateUser(@ParameterValueKeyProvider @ParameterDataUpdateContent User user) { sqlMapClientTemplate.update(&quot;updateUser&quot;, user); } SQL: &lt;update id=&quot;updateUser&quot; parameterClass=&quot;user&quot;&gt; UPDATE user SET name = #name# WHERE id = #id# &lt;/update&gt; &lt;select id=&quot;getUser&quot; parameterClass=&quot;java.lang.Long&quot; resultClass=&quot;user&quot;&gt; SELECT * FROM user WHERE id = #id# &lt;/select&gt; 只要有相同的 namespace 在 getUser 时，会根据 @ParameterValueKeyProvider 找到 User 对象的 @CacheKeyMethod 方法，到 Memcached 中 get user:id。 在 updateUser 时，会根据 @ParameterValueKeyProvider 找到 User 对象的 @CacheKeyMethod 方法，到 Memcached 中 set user:id 场景二：根据某个 role ID 查询所有的 User。查询关联两个以上的 Model（User，Role），在更新 Role 时，需要让相关的缓存失效。Method: @ReadThroughSingleCache(namespace = &quot;user/list&quot;, expiration = 600) public List&lt;User&gt; getUsersByRoleId(@ParameterValueKeyProvider Long id) { return (List&lt;User&gt;) sqlMapClientTemplate.queryForList(&quot;getUsersByRoleId&quot;, id); } SQL: &lt;select id=&quot;getUsersByRoleId&quot; parameterClass=&quot;java.lang.Long&quot; resultClass=&quot;user&quot;&gt; SELECT u.id, u.name, r.id as &quot;role.id&quot;, r.name as &quot;role.name&quot; FROM user u, user_role ur, role r WHERE u.id = ur.userid and r.id = ur.roleid and ur.roleid = #id# &lt;/select&gt; 当更新 Role 时: @UpdateSingleCache(namespace = &quot;role&quot;, expiration = 60) public void updateRole(@ParameterValueKeyProvider @ParameterDataUpdateContent Role role) { sqlMapClientTemplate.update(&quot;updateRole&quot;, role); } 需要让 getUsersByRoleId 的缓存失效。这时最简单的办法是直接使用 annotation @InvalidateSingleCache @UpdateSingleCache(namespace = &quot;role&quot;, expiration = 60) @InvalidateSingleCache(namespace = &quot;user/list&quot;) public void updateRole(@ParameterValueKeyProvider @ParameterDataUpdateContent Role role) { sqlMapClientTemplate.update(&quot;updateRole&quot;, role); } 但是，如果有多个类似的 Cache 需要清除，那这种办法就不适用了。这时可以每个 POJO 定义一个专门用来 invalidate 的类： @Component public class UserCache { @InvalidateSingleCache(namespace = &quot;user/list&quot;) public void invalidateGetUsersByRoleId(@ParameterValueKeyProvider Long id){ } ｝ 在 Service 中调用相关的方法： @Service public class RoleManagerImpl implements RoleManager { @Autowired private RoleDao roleDao; @Autowired private UserCache userCache; @Override public void updateRole(Role role) { roleDao.updateRole(role); userCache.invalidateGetUsersByRoleId(role.getId()); groupCache.invalidate(role.getId()); ... } } 下一篇会讲一下 MultiCache 的使用。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>memcached</tag>
        <tag>spring</tag>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 CentOS6 上安装 ActiveMQ Service]]></title>
    <url>%2F2012%2F09%2F18%2Finstalling-activemq-as-a-service-on-centos6%2F</url>
    <content type="text"><![CDATA[这里主要讲一下在 CentOS6 x64 上如何把 ActiveMQ5.5 添加到 Service 里自动启动。 打开配置文件 # vim /opt/activemq /bin/linux-x86-64/wrapper.conf 修改两个参数 set.default.ACTIVEMQ_HOME=/opt/activemq set.default.ACTIVEMQ_BASE=/opt/activemq 建立一个软链接（用全路径） # ln -s /opt/activemq/bin/linux-x86-64/activemq /etc/init.d/activemq 加入到启动项 # chkconfig --add activemq 使用服务 # service activemq start|stop|status]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>java</tag>
        <tag>jms</tag>
        <tag>activemq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 CentOS6 上安装 vsftpd]]></title>
    <url>%2F2012%2F09%2F17%2Finstall-vsftpd-on-centos6%2F</url>
    <content type="text"><![CDATA[vsftpd 是一款在 Linux 发行版中最受推崇的 FTP 服务器程序。特点是小巧轻快，安全易用。vsftpd 的名字代表”very secure FTP daemon”, 安全是它的开发者 Chris Evans 考虑的首要问题之一。在这个 FTP 服务器设计开发的最开始的时候，高安全性就是一个目标。 在初次安装过程中遇到了很多问题，主要是关于帐号设置的问题，这里纪录一下。 1. 安装 vsftpd通过 yum 安装 # yum install vsftpd 设置开机自动启动 # chkconfig vsftpd on 启动服务 # service vsftpd start Starting vsftpd for vsftpd: [ OK ] 配置防火墙 # vim /etc/sysconfig/iptables 增加你需要开放的端口 # -A INPUT -m state --state NEW -m tcp -p tcp --dport 21 -j ACCEPT 保存以后重启防火墙 # service iptables restart 2. 配置 vsftpd增加 ftpuser # useradd -g ftp -s /sbin/nologin ftpuser 设置密码 # passwd ftpuser 修改配置文件 # cd /etc/vsftpd/ # mv vsftpd.conf vsftpd.conf.bak # vim vsftpd.conf anonymous_enable=NO #允许登入者有写权限 write_enable=YES #允许本地用户访问 local_enable=YES #本地用户新增档案时的umask值 local_umask=022 #定义欢迎话语的字符串 ftpd_banner=Welcome to FTP server. #启用上传/下载日志记录 xferlog_enable=YES #日志文件所在的路径及名称 xferlog_file=/var/log/vsftpd.log #将日志文件写成xferlog的标准格式 xferlog_std_format=YES #在chroot_list中列出的用户不允许切换到家目录的上级目录 chroot_list_enable=YES #如果chroot_local_user设置了YES，那么chroot_list_file #设置的文件里，是不被chroot的用户(可以向上改变目录) #如果chroot_local_user设置了NO，那么chroot_list_file #设置的文件里，是被chroot的用户(无法向上改变目录) chroot_local_user=YES chroot_list_file=/etc/vsftpd/chroot_list userlist_enable=YES userlist_deny=YES userlist_file=/etc/vsftpd/user_list #FTP服务器以standalone模式运行 listen=YES #FTP服务器启用PORT模式 port_enable=YES #禁用FTP服务器的PASV模式 pasv_enable=NO #FTP服务器监听21端口 listen_port=21 #600秒钟不对FTP服务器进行任何操作，则断开该FTP连接 idle_session_timeout=600 #建立FTP数据连接的超时时间为120秒 data_connection_timeout=120 #不限制用户的连接数量 max_clients=0 #每个IP与FTP服务器同时建立连接数 max_per_ip=100 #PAM认证文件 pam_service_name=vsftpd use_localtime=YES #支持ASCII模式 ascii_upload_enable=YES ascii_download_enable=YES 重启 # service vsftpd restart 3. 问题 500 OOPS: cannot change directory:/home/ftpuser # getsebool -a|grep ftp # setsebool -P ftp_home_dir on 500 OOPS: could not read chroot() list file:/etc/vsftpd/chroot_list 缺少 chroot_list 文件，需要手动建立。 # vim chroot_list 530 Login incorrect. 可能一： # vim vsftpd.conf 增加： pam_service_name=vsftpd 可能二：密码不正确。 550 Permission denied 这个问题可能是多种原因造成的，最常见的是缺少 write_enable=YES 但在我的环境中，出现这个问题是因为服务端 iptables、客户端防火墙、vsftpd pasv 三者之间的配置造成的。参考 iptables 中配置 vsftp 的访问 主动模式下，客户连接 TCP/21，服务器通过 TCP/20 连接客户的随机端口。这种情况下，通过状态防火墙可以解决 iptables -A INPUT -m state –state NEW,RELATED,ESTABLISHED -j ACCEPT 被动模式下，客户连接 TCP/21，客户再通过其他端口连接服务器的随机端口。因为服务器在被动模式下没有打开临时端口让 client 连过来，因此需要几个条件： client 没有防火墙时，用主动模式连接即可。 server 没有防火墙时，用被动模式即可。 双方都有防火墙时，vsftpd 设置被动模式高端口范围，server 打开那段范围，client 用被动模式连接即可。 加载 ip_conntrack_ftp 模块，使 server 支持 connection tracking，支持临时打洞，client 用被动模式即可。 server 使用 ip_conntrack_ftp、client 使用 ip_conntrack_ftp 和 ip_nat_ftp，支持临时打洞和临时 NAT 穿越打洞，双方使用主动或被动模式均可。 因为我 client 和 server 都有防火墙，所以对前边的 vsftpd 设置稍做修改 #pasv_enable=NO pasv_min_port=2222 pasv_max_port=2322 修改 iptables -A INPUT -p tcp --dport 2222:2322 -j ACCEPT 重启 # service iptables restart # service vsftpd restart]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ftp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 CentOS6 上安装 memcached]]></title>
    <url>%2F2012%2F09%2F07%2Finstall-memcached-on-centos6%2F</url>
    <content type="text"><![CDATA[1. 安装 libevent# yum list|grep libevent* libevent.x86_64 1.4.13-4.el6 libevent.i686 1.4.13-4.el6 libevent-devel.i686 1.4.13-4.el6 libevent-devel.x86_64 1.4.13-4.el6 libevent-doc.noarch 1.4.13-4.el6 libevent-headers.noarch 1.4.13-4.el6 # yum install libevent.x86_64 libevent-devel.x86_64 2. 安装 memcached# wget http://memcached.googlecode.com/files/memcached-1.4.15.tar.gz # tar zxvf memcached-1.4.15.tar.gz # cd memcached-1.4.15 # ./configure # make # make install 3. 使用 memcached参数 -p 监听端口 -l 连接的IP地址,默认是本机 -d start 启动 memecached 服务 -d restart 重启 -d stop|shutdown关闭服务 -d install 安装 -d uninstall 卸载 -u 以身份运行仅在 root 下有效 -m 最大内存使用，单位 MB，默认 64MB -M 内存耗尽时返回错误 -c 最大同时连接数量,默认是 1024 -f 块大小增长因为,默认是 1.25 -n 最小分配空间, key + value + flags 默认48 -h 显示帮助 启动 # memcached -d -u root 验证 ＃ telnet localhost 11211 Trying ::1... Connected to localhost. Escape character is &apos;^]&apos;. 查看当前状态 ＃ stats TAT pid 26427 STAT uptime 2137 STAT time 1346995532 STAT version 1.4.15 STAT libevent 1.4.13-stable STAT pointer_size 64 STAT rusage_user 0.012998 STAT rusage_system 0.023996 STAT curr_connections 10 STAT total_connections 14 STAT connection_structures 12 STAT reserved_fds 20 STAT cmd_get 0 STAT cmd_set 0 STAT cmd_flush 0 STAT cmd_touch 0 STAT get_hits 0 STAT get_misses 0 STAT delete_misses 0 STAT delete_hits 0 STAT incr_misses 0 STAT incr_hits 0 STAT decr_misses 0 STAT decr_hits 0 STAT cas_misses 0 STAT cas_hits 0 STAT cas_badval 0 STAT touch_hits 0 STAT touch_misses 0 STAT auth_cmds 0 STAT auth_errors 0 STAT bytes_read 68 STAT bytes_written 2091 STAT limit_maxbytes 67108864 STAT accepting_conns 1 STAT listen_disabled_num 0 STAT threads 4 STAT conn_yields 0 STAT hash_power_level 16 STAT hash_bytes 524288 STAT hash_is_expanding 0 STAT bytes 0 STAT curr_items 0 STAT total_items 0 STAT expired_unfetched 0 STAT evicted_unfetched 0 STAT evictions 0 STAT reclaimed 0 END 远程使用需要打开 iptables 端口 # vim /etc/sysconfig/iptables ---- -A INPUT -m state --state NEW -m tcp -p tcp --dport 11211 -j ACCEPT -A INPUT -m state --state NEW -m udp -p udp --dport 11211 -j ACCEPT # service iptables restart]]></content>
      <categories>
        <category>memcached</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>memcached</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven 自动化构建（dev, test, prod）]]></title>
    <url>%2F2012%2F08%2F22%2Fmaven-package%2F</url>
    <content type="text"><![CDATA[1. 需求描述 在项目构建时，需要根据环境的不同生成不同的安装包。不希望每次通过人工修改配置。 有非常多的不同的 prod 环境配置，100+。 在 Maven 的多模块项目中，需要有一个完整的 properties 来定义各个不同的环境，而不是分散在各个 Module 中。 不希望这些 properties 定义在 pom 中，而是需要独立出来，通过动态参数加载。 最终可以通过 mvn package -Pdev, mvn package -Ptest, mvn package -Pprod 完成各种环境的构建。 项目模块假设是以下的一个结构： &lt;modules&gt; &lt;module&gt;cas&lt;/module&gt; &lt;module&gt;core&lt;/module&gt; &lt;module&gt;web&lt;/module&gt; &lt;module&gt;client&lt;/module&gt; &lt;/modules&gt; 其中，cas，web，client 是三个 war，core 是个 jar。要完成上述目标，还需要用到 properties-maven-plugin 这个插件，可以把需要定制的 properties 放到一个外部的文件，剥离 pom，并且做到动态加载配置。 2. maven-assembly-plugin一开始想用 maven-assembly-plugin，assembly 功能很强大，可定制的程度很高。试了一下，也可以解决上边的需求。 pom &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;descriptor&gt;src/main/assemble/package.xml&lt;/descriptor&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; package.xml &lt;assembly xmlns=&quot;http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.2&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.2 http://maven.apache.org/xsd/assembly-1.1.2.xsd&quot;&gt; &lt;id&gt;distribution&lt;/id&gt; &lt;formats&gt; &lt;format&gt;war&lt;/format&gt; &lt;/formats&gt; &lt;includeBaseDirectory&gt;false&lt;/includeBaseDirectory&gt; &lt;fileSets&gt; &lt;fileSet&gt; &lt;directory&gt;${project.build.directory}/${project.build.finalName}&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**&lt;/include&gt; &lt;/includes&gt; &lt;excludes&gt; &lt;exclude&gt;**/deploy*.xml&lt;/exclude&gt; &lt;/excludes&gt; &lt;outputDirectory&gt;/&lt;/outputDirectory&gt; &lt;/fileSet&gt; &lt;fileSet&gt; &lt;directory&gt;${project.build.directory}/${project.build.finalName}&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/deploy*.xml&lt;/include&gt; &lt;/includes&gt; &lt;filtered&gt;true&lt;/filtered&gt; &lt;outputDirectory&gt;/&lt;/outputDirectory&gt; &lt;/fileSet&gt; &lt;/fileSets&gt; &lt;/assembly&gt; 但存在的问题如下： 如果使用了 assembly，那么如果在配置文件中有变量，你还是需要对 maven-war-plugin 的打包过程进行定制，否则会造成 assembly filter 起作用，war 打出的包中变量没有替换的情况。也是就是如果需要保证 mvn package 构建出的 war 是有效的，既需要定制 assembly，也需要定制 war。 assembly 生成的文件名默认会附加 @id@ 在文件名上，好像没有配置的方法。 综上，在这个项目中，因为没有生成不同格式发布包的要求，只需要生成 war，所以，不考虑引入 assembly ，只对 maven-war-plugin 的构建过程进行定制。 3. maven-war-plugin在 root pom 中： &lt;pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt; &lt;version&gt;2.1.1&lt;/version&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;test&lt;/id&gt; &lt;properties&gt; &lt;profile&gt;test&lt;/profile&gt; &lt;/properties&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;properties&gt; &lt;profile&gt;dev&lt;/profile&gt; &lt;/properties&gt; 在 root src 下边增加 config 目录，把相关的配置文件放到下边： dev.properties test.properties prod1.properties prod2.properties … dev 内容： ldap.port=389 ldap.host=ldap.dev.org ldap.searchbase=dc\=dev,dc\=org ldap.authdn=cn\=Directory Manager ldap.passwd=password test 内容： ldap.port=389 ldap.host=ldap.dev.org ldap.searchbase=dc\=test,dc\=org ldap.authdn=cn\=Directory Manager ldap.passwd=password 在 cas module pom 中： &lt;plugin&gt; &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;webResources&gt; &lt;!-- 对需要修改参数的配置文件进行配置 filtering --&gt; &lt;resource&gt; &lt;directory&gt;src/main/webapp/WEB-INF&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/deploy*.xml&lt;/include&gt; &lt;/includes&gt; &lt;targetPath&gt;WEB-INF&lt;/targetPath&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;/resource&gt; &lt;/webResources&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;properties-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.0-alpha-2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;initialize&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;read-project-properties&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;files&gt; &lt;!-- 指向 root 目录中的配置文件，并且根据 profile 动态加载文件名 --&gt; &lt;file&gt;${basedir}/../src/config/${profile}.properties&lt;/file&gt; &lt;/files&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; 现在在 cas module 的根目录运行 mvn package 或者 mvn package -Ptest，可以看到需要修改的 ${ldap.host} 等已经被替换。之后只需要在每个 module 下边对相关的插件进行定制，就可以完成之前的目标。]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自定义 CAS assertion 返回值]]></title>
    <url>%2F2012%2F07%2F19%2Fsetting-cas-assertion%2F</url>
    <content type="text"><![CDATA[这篇主要是讲 CAS 和 LDAP 集成时，如何返回一些特别的内容到 Spring Security 客户端。这样我们可以从 LDAP 取到 user 的 role，和 Spring Security 进行比对授权。但是，CAS 默认配置的 UsernamePasswordCredentialsToPrincipalResolver 不允许我们在与Spring Security 集成时传递回特别的属性信息，所以我们需要对 CAS 进行修改允许我们这样做。 CAS 提供了高级的配置使客户端与 CAS 服务端进行数据交换。在 CAS 服务器传递 ticket 校验结果时，可以将基于 CAS 认证时查询到的信息进行传递。这些信息以键值对的方式进行传递，并可以包含用户相关的任何数据。我们将会使用这个功能在 CAS 响应中传递用户的属性，包括 GrantedAuthority 信息。 1. deployerConfigContext.xmlCAS 服务器的 org.jasig.cas.authentication.AuthenticationManager 负责基于提供的凭证信息进行用户认证。与 Spring Security 很相似，实际的认证委托给了一个（或更多）实现了 org.jasig.cas.authentication.handler.AuthenticationHandler 接口的处理类（在 Spring Security 中对应的接口是 AuthenticationProvider ）。 org.jasig.cas.authentication.principal.CredentialsToPrincipalResolver 用来将传递进来的安全实体信息转换成完整的org.jasig.cas.authentication.principal.Principal（类似于 Spring Security中 UserDetailsService 实现所作的那样）。 2. 默认返回值HomeController.java @Controller @RequestMapping(&quot;home&quot;) public class HomeController { @RequestMapping(method = RequestMethod.GET) public ModelAndView home() { ModelAndView mv = new ModelAndView(&quot;home&quot;); final Authentication auth = SecurityContextHolder.getContext().getAuthentication(); mv.addObject(&quot;auth&quot;, auth); if (auth instanceof CasAuthenticationToken) { mv.addObject(&quot;isCasAuthentication&quot;, Boolean.TRUE); } return mv; } } home.jsp &lt;h1&gt;View Profile&lt;/h1&gt; &lt;p&gt; Some information about you, from CAS: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Auth:&lt;/strong&gt; ${auth}&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Username:&lt;/strong&gt; ${auth.principal}&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Credentials:&lt;/strong&gt; ${auth.credentials}&lt;/li&gt; &lt;c:if test=&quot;${isCasAuthentication}&quot;&gt; &lt;li&gt;&lt;strong&gt;Assertion:&lt;/strong&gt; ${auth.assertion}&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Assertion Attributes:&lt;/strong&gt; &lt;c:forEach items=&quot;${auth.assertion.attributes}&quot; var=&quot;attr&quot;&gt; ${attr.key}:${attr.value}&lt;br/&gt; &lt;/c:forEach&gt; &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Assertion Attribute Principal:&lt;/strong&gt; ${auth.assertion.principal}&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Assertion Principal Attributes:&lt;/strong&gt; &lt;c:forEach items=&quot;${auth.assertion.principal.attributes}&quot; var=&quot;attr&quot;&gt; ${attr.key}:${attr.value}&lt;br/&gt; &lt;/c:forEach&gt; &lt;/li&gt; &lt;/c:if&gt; &lt;/ul&gt; 这个 jsp 的返回结果大致是这样的，只可以返回 Username。 Auth: org.springframework.security.cas.authentication.CasAuthenticationToken@1358c3fc: Principal: org.springframework.security.core.userdetails.User@aa9c3074: Username: zhangsan; Password: [PROTECTED]; Enabled: true; AccountNonExpired: true; credentialsNonExpired: true; AccountNonLocked: true; Granted Authorities: I&apos;M ZHANGSAN.; Credentials: [PROTECTED]; Authenticated: true; Details: org.springframework.security.web.authentication.WebAuthenticationDetails@fffed504: RemoteIpAddress: 127.0.0.1; SessionId: 748C80EE4C063D18F2461486F82FC2C8; Granted Authorities: I&apos;M ZHANGSAN. Assertion: org.jasig.cas.client.validation.AssertionImpl@5b3ba312 Credentials (Service/Proxy Ticket): ST-3-zbmPK3BMjrAmTEwwMlxi-cas Username: org.springframework.security.core.userdetails.User@aa9c3074: Username: zhangsan; Password: [PROTECTED]; Enabled: true; AccountNonExpired: true; credentialsNonExpired: true; AccountNonLocked: true; Granted Authorities: I&apos;M ZHANGSAN. Credentials: ST-3-zbmPK3BMjrAmTEwwMlxi-cas Assertion: org.jasig.cas.client.validation.AssertionImpl@5b3ba312 Assertion Attributes: Assertion Attribute Principal: zhangsan Assertion Principal Attributes: 3. 自定义返回值我们需要先修改 deployerConfigContext.xml，重新定义 attributeRepository 这个 Bean &lt;bean id=&quot;attributeRepository&quot; class=&quot;org.jasig.services.persondir.support.ldap.LdapPersonAttributeDao&quot;&gt; &lt;property name=&quot;contextSource&quot; ref=&quot;contextSource&quot;/&gt; &lt;property name=&quot;requireAllQueryAttributes&quot; value=&quot;true&quot;/&gt; &lt;property name=&quot;baseDN&quot; value=&quot;ou=people,dc=dev,dc=org&quot;/&gt; &lt;property name=&quot;queryAttributeMapping&quot;&gt; &lt;map&gt; &lt;entry key=&quot;username&quot; value=&quot;uid&quot;/&gt; &lt;/map&gt; &lt;/property&gt; &lt;property name=&quot;resultAttributeMapping&quot;&gt; &lt;map&gt; &lt;entry key=&quot;uid&quot; value=&quot;username&quot;/&gt; &lt;entry key=&quot;displayName&quot; value=&quot;displayName&quot;/&gt; &lt;entry key=&quot;description&quot; value=&quot;role&quot;/&gt; &lt;entry key=&quot;telephoneNumber&quot; value=&quot;telephoneNumber&quot;/&gt; &lt;/map&gt; &lt;/property&gt; &lt;/bean&gt; 这个类将 Principal 与后端的 LDAP 目录进行匹配。queryAttributeMapping 属性将 Principal 的 username 域与 LDAP 的uid 属性相匹配。resultAttributeMapping 中的键值对 key 表示 LDAP 中的属性，而 value 表示返回的 assertion 属性。而这个 role 属性就是 GrantedAuthorityFromAssertionAttributesUserDetailsService 要进行查找的。 在客户端需要配置 GrantedAuthorityFromAssertionAttributesUserDetailsService ，它的工作就是读取 CAS assertion、寻找特定的属性并将属性值与用户的 GrantedAuthority 进行匹配。假设 assertion 返回了一个名为 role 的属性。我们只需要在applicationContext-security.xml 中简单配置一个新的 bean &lt;bean id=&quot;authenticationUserDetailsService&quot; class=&quot;org.springframework.security.cas.userdetails.GrantedAuthorityFromAssertionAttributesUserDetailsService&quot;&gt; &lt;constructor-arg&gt; &lt;array&gt; &lt;value&gt;role&lt;/value&gt; &lt;/array&gt; &lt;/constructor-arg&gt; &lt;/bean&gt; 但现在还不行，我们还需要在 Server 端做一些工作。继续 deployerConfigContext.xml，找到 UsernamePasswordCredentialsToPrincipalResolver，增加attributeRepository 属性 &lt;bean class=&quot;org.jasig.cas.authentication.principal.UsernamePasswordCredentialsToPrincipalResolver&quot;&gt; &lt;property name=&quot;attributeRepository&quot; ref=&quot;attributeRepository&quot;/&gt; &lt;/bean&gt; 找到 serviceRegistryDao，在所有的 list bean 中增加 &lt;property name=&quot;ignoreAttributes&quot; value=&quot;true&quot; /&gt; 编辑 WEB-INF/view/jsp/protocol/2.0/casServiceValidationSuccess.jsp，在 cas:user 后边增加以下内容 &lt;cas:attributes&gt; &lt;c:forEach var=&quot;attr&quot; items=&quot;${assertion.chainedAuthentications[fn:length(assertion.chainedAuthentications)-1].principal.attributes}&quot; varStatus=&quot;loopStatus&quot; begin=&quot;0&quot; end=&quot;${fn:length(assertion.chainedAuthentications[fn:length(assertion.chainedAuthentications)-1].principal.attributes)-1}&quot; step=&quot;1&quot;&gt; &lt;cas:${fn:escapeXml(attr.key)}&gt;${fn:escapeXml(attr.value)}&lt;/cas:${fn:escapeXml(attr.key)}&gt; &lt;/c:forEach&gt; &lt;/cas:attributes&gt; 现在可以 mvn package ，重新部署 CAS Server，重新登录访问 home.jsp，可以看到结果多出了一些内容 Auth: org.springframework.security.cas.authentication.CasAuthenticationToken@1358c3fc: Principal: org.springframework.security.core.userdetails.User@aa9c3074: Username: zhangsan; Password: [PROTECTED]; Enabled: true; AccountNonExpired: true; credentialsNonExpired: true; AccountNonLocked: true; Granted Authorities: I&apos;M ZHANGSAN.; Credentials: [PROTECTED]; Authenticated: true; Details: org.springframework.security.web.authentication.WebAuthenticationDetails@fffed504: RemoteIpAddress: 127.0.0.1; SessionId: 748C80EE4C063D18F2461486F82FC2C8; Granted Authorities: I&apos;M ZHANGSAN. Assertion: org.jasig.cas.client.validation.AssertionImpl@5b3ba312 Credentials (Service/Proxy Ticket): ST-3-zbmPK3BMjrAmTEwwMlxi-cas Username: org.springframework.security.core.userdetails.User@aa9c3074: Username: zhangsan; Password: [PROTECTED]; Enabled: true; AccountNonExpired: true; credentialsNonExpired: true; AccountNonLocked: true; Granted Authorities: ROLE_USER. Credentials: ST-3-zbmPK3BMjrAmTEwwMlxi-cas Assertion: org.jasig.cas.client.validation.AssertionImpl@5b3ba312 Assertion Attributes: Assertion Attribute Principal: zhangsan Assertion Principal Attributes: username:zhangsan telephoneNumber:18918588940 role:ROLE_USER. displayName:zhangsan 这样在客户端的 Spring Security 中就可以实现授权比对 &lt;sec:intercept-url pattern=&quot;/user&quot; access=&quot;ROLE_USER&quot;/&gt; 4. 参考 高级CAS配置]]></content>
      <categories>
        <category>cas</category>
      </categories>
      <tags>
        <tag>ldap</tag>
        <tag>cas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 CentOS6 上安装 Tomcat7]]></title>
    <url>%2F2012%2F07%2F17%2Finstall-tomcat7-on-centos%2F</url>
    <content type="text"><![CDATA[1. 下载# wget http://mirror.bit.edu.cn/apache/tomcat/tomcat-7/v7.0.29/bin/apache-tomcat-7.0.29.tar.gz 2. 安装# tar -xzvf apache-tomcat-7.0.29.tar.gz # mv apache-tomcat-7.0.29 /opt/tomcat7 # cd /opt/tomcat7 # bin/startup.sh 3. 配置在生产环境用 root 是不安全的，所以 # useradd -s /sbin/nologin tomcat # chown -R tomcat:tomcat /opt/tomcat7 做为 service，和操作系统一起启动 # cd /opt/tomcat7/bin # tar -xzvf commons-daemon-native.tar.gz # cd commons-daemon-1.0.10-native-src/unix # ./configure # make # cp jsvc ../.. # cd ../.. 在 daemon.sh 的注释后边，正文最开始增加下边五行内容 # vim daemon.sh ---- # chkconfig: 2345 10 90 # description: Starts and Stops the Tomcat daemon. JAVA_HOME=/usr/java/jdk1.6.0_31 CATALINA_HOME=/opt/tomcat7 CATALINA_OPTS=&quot;-Xms1024m -Xmx2048m -XX:PermSize=256m -XX:MaxPermSize=512m&quot; 增加到 service # cp daemon.sh /etc/init.d/tomcat # chkconfig --add tomcat 检查 # chkconfig --list|grep tomcat tomcat 0:关闭 1:关闭 2:启用 3:启用 4:启用 5:启用 6:关闭 打开端口 # vim /etc/sysconfig/iptables ---- -A INPUT -m state --state NEW -m tcp -p tcp --dport 8080 -j ACCEPT # service iptables restart]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 CentOS6 上安装 BIND（DNS Server）]]></title>
    <url>%2F2012%2F07%2F16%2Finstall-dns-server-on-centos6%2F</url>
    <content type="text"><![CDATA[1. DNS 安装方式一：从官方下载最新的 Release 版本编译安装（生产环境推荐，后边的配置步骤也会以这种方式说明，和 yum 安装的路径不同） 在安装之前需要先安装 gcc # yum install gcc.x86_64 gcc-c++.x86_64 gcc-objc++.x86_64 还需要有 openssl # yum install openssl openssl-devel 下载并安装 # wget http://ftp.isc.org/isc/bind9/9.9.1-P1/bind-9.9.1-P1.tar.gz # tar -zxvf bind-9.9.1-P1.tar.gz # cd bind-9.9.1-P1 # ./configure --enable-largefile --enable-threads --prefix=/usr/local/named # make # make install 查询版本号 # /usr/local/named/sbin/named -v ---- BIND 9.9.1-P1 方式二：使用 yum 安装 # yum -y install bind* caching-nameserver 查询版本号 # named -v BIND 9.8.2rc1-RedHat-9.8.2-0.10.rc1.el6 2. DNS 配置安装 RNDC，让其管理 bind # cd /usr/local/named/etc # /usr/local/named/sbin/rndc-confgen &gt; /usr/local/named/etc/rndc.conf # tail -n10 rndc.conf |head -n9 |sed -e s/#\//g &gt; named.conf 更新 Internet 根服务器地址 # cd /usr/local/named/ # wget ftp://ftp.internic.org/domain/named.root 配置 named.conf 文件，这是 bind 的主配置文件，最终的内容 # mkdir -p /usr/local/named/data # cd etc # vim named.conf 刚才 tail 命令时已经把 rndc.conf 的一部分内容加进来，现在再在前边加入以下内容 options { directory &quot;/usr/local/named&quot;; pid-file &quot;named.pid&quot;; listen-on port 53 {any;}; allow-query {any;}; dump-file &quot;/usr/local/named/data/cache_dump.db&quot;; statistics-file &quot;/usr/local/named/data/named_stats.txt&quot;; forward only; //增加转发功能 forwarders { 8.8.8.8; }; }; zone &quot;.&quot; IN { type hint; file &quot;named.root&quot;; }; zone &quot;localhost&quot; IN { type master; file &quot;localhost.zone&quot;; allow-update { none; }; }; zone &quot;0.0.127.in-addr.arpa&quot; IN { type master; file &quot;localhost.rev&quot;; allow-update { none; }; }; zone &quot;dev.org&quot; IN { type master; file &quot;dev.org.zone&quot;; }; zone &quot;247.4.10.in-addr.arpa&quot; IN { type master; file &quot;10.4.247.zone&quot;; }; 生成域名相应的配置文件 # cd /usr/local/named localhost 正向解析文件 # vim localhost.zone ---- $TTL 3600 @ IN SOA @ root ( 20100923 ;serial (d. adams) 3H ;refresh 15M ;retry 1W ;expiry 3600) ;minimum IN NS @ IN A 127.0.0.1 localhost 反向解析文件 # vim localhost.rev ---- $TTL 3600 @ IN SOA localhost. root.localhost. ( 20100923 ; serial 3600 ; refresh every hour 900 ; retry every 15 minutes 3600000 ; expire 1000 hours 3600) ; minimun 1 hour IN NS localhost. 1 IN PTR localhost. dev.org 正向解析文件 # vim dev.org.zone ---- $TTL 86400 @ IN SOA dns.dev.org root ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum @ IN NS dns.dev.org. cas IN A 10.4.247.20 dns IN A 10.4.247.20 ldap IN A 10.4.247.20 dev.org 反向解析文件 # vim 10.4.247.zone ---- $TTL 86400 @ IN SOA dns.dev.org root ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum @ IN NS dns.dev.org. cas IN A 10.4.247.20 dns IN A 10.4.247.20 ldap IN A 10.4.247.20 20 IN PTR cas.dev.org. 20 IN PTR dns.dev.org. 20 IN PTR ldap.dev.org. 3. 测试启动bind # /usr/local/named/sbin/named -gc /usr/local/named/etc/named.conf &amp; ---- 16-Jul-2012 10:47:03.896 ---------------------------------------------------- 16-Jul-2012 10:47:03.896 BIND 9 is maintained by Internet Systems Consortium, 16-Jul-2012 10:47:03.896 Inc. (ISC), a non-profit 501(c)(3) public-benefit 16-Jul-2012 10:47:03.896 corporation. Support and training for BIND 9 are 16-Jul-2012 10:47:03.896 available at https://www.isc.org/support 16-Jul-2012 10:47:03.896 ---------------------------------------------------- 16-Jul-2012 10:47:03.896 adjusted limit on open files from 4096 to 1048576 16-Jul-2012 10:47:03.896 found 16 CPUs, using 16 worker threads 16-Jul-2012 10:47:03.896 using 16 UDP listeners per interface 16-Jul-2012 10:47:03.897 using up to 4096 sockets 16-Jul-2012 10:47:03.903 loading configuration from &apos;/usr/local/named/etc/named.conf&apos; 16-Jul-2012 10:47:03.903 reading built-in trusted keys from file &apos;/usr/local/named/etc/bind.keys&apos; 16-Jul-2012 10:47:03.904 using default UDP/IPv4 port range: [1024, 65535] 16-Jul-2012 10:47:03.904 using default UDP/IPv6 port range: [1024, 65535] 16-Jul-2012 10:47:03.905 listening on IPv4 interface lo, 127.0.0.1#53 16-Jul-2012 10:47:03.911 listening on IPv4 interface em1, 10.4.247.20#53 16-Jul-2012 10:47:03.916 generating session key for dynamic DNS 16-Jul-2012 10:47:03.916 sizing zone task pool based on 5 zones 16-Jul-2012 10:47:03.919 set up managed keys zone for view _default, file &apos;managed-keys.bind&apos; 16-Jul-2012 10:47:03.922 command channel listening on 127.0.0.1#953 16-Jul-2012 10:47:03.922 ignoring config file logging statement due to -g option 16-Jul-2012 10:47:03.922 managed-keys-zone: loaded serial 0 16-Jul-2012 10:47:03.923 zone 0.0.127.in-addr.arpa/IN: loaded serial 20100923 16-Jul-2012 10:47:03.924 zone localhost/IN: has no NS records 16-Jul-2012 10:47:03.924 zone localhost/IN: not loaded due to errors. 16-Jul-2012 10:47:03.924 zone 247.4.10.in-addr.arpa/IN: loaded serial 0 16-Jul-2012 10:47:03.924 zone dev.org/IN: loaded serial 0 16-Jul-2012 10:47:03.925 all zones loaded 16-Jul-2012 10:47:03.925 running 修改本机的 DNS 设置（如果修改 resolv.conf 的话重启以后会失效） #vim /etc/sysconfig/network-scripts/ifcfg-em1 DNS1=10.4.247.20 安装 nslookup 工具 # yum install bind-utils # nslookup &gt; dns.dev.org Server: 10.4.247.20 Address: 10.4.247.20#53 Name: dns.dev.org Address: 10.4.247.20 &gt; cas.dev.org Server: 10.4.247.20 Address: 10.4.247.20#53 Name: cas.dev.org Address: 10.4.247.20 &gt; ldap.dev.org Server: 10.4.247.20 Address: 10.4.247.20#53 Name: ldap.dev.org Address: 10.4.247.20 &gt; 10.4.247.20 Server: 10.4.247.20 Address: 10.4.247.20#53 20.247.4.10.in-addr.arpa name = dns.dev.org. 20.247.4.10.in-addr.arpa name = ldap.dev.org. 20.247.4.10.in-addr.arpa name = cas.dev.org. 最后需要把 bind 加入到启动项，随操作系统一起启动 # cd /etc/rc.d # vim rc.local 在最后添加 # /usr/local/named/sbin/named -gc /usr/local/named/etc/named.conf &amp; 4. 注意服务一旦运行，基本就不会再关闭，重新加载配置使用 # /usr/local/named/sbin/rndc reload 如果 iptables 有打开，一定要打开端口 # netstat -tunpl|grep named ---- tcp 0 0 10.4.247.20:53 0.0.0.0:* LISTEN 2987/named tcp 0 0 127.0.0.1:53 0.0.0.0:* LISTEN 2987/named tcp 0 0 127.0.0.1:953 0.0.0.0:* LISTEN 2987/named udp 0 0 10.4.247.20:53 0.0.0.0:* 2987/named udp 0 0 127.0.0.1:53 0.0.0.0:* 2987/named # vim /etc/sysconfig/iptables ---- -A INPUT -m state --state NEW -m tcp -p tcp --dport 53 -j ACCEPT -A INPUT -m state --state NEW -m udp -p udp --dport 53 -j ACCEPT -A INPUT -m state --state NEW -m tcp -p tcp --dport 953 -j ACCEPT # service iptables restart]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>dns</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 CentOS6 上安装 OpenDJ（GUI）]]></title>
    <url>%2F2012%2F07%2F13%2Finstall-opendj-on-centos6-with-gui%2F</url>
    <content type="text"><![CDATA[之前有一篇有讲到在 RHEL6 上安装 OpenDJ，主要是讲通过命令行的方式安装，这次讲一下通过图形界面安装。 服务器设置这一步主要是 host name 的设置。要确保这个域名可以被解析，否则会抛出 javax.naming.CommunicationException: 0.0.0.0:4444 的异常。临时解决办法是在 hosts 中增加一条纪录 127.0.0.1 idams 复制选项如果是第一台 Server，选择 This will be a stand alone server 初始化数据输入你的 Base DN JVM 设置生产环境分配 2G 以上内存 所有配置 安装完成如果前边的 host name 有问题，会卡在这一步很长时间，查看 log ，会看到第一步提到的那个异常，这时其实已经安装成功，Cancel 即可，Server 已经启动。 控制面板这时点击 Launch Control Panel，进入到 administrator 界面 其它问题如果 host name 有问题，你在 local server 是无法使用 Control Panel 的 这时可以使用远程的方式控制，在另外一台主机启动 Control Panel 在远程登录之前，不要忘记设置防火墙 # vim /etc/sysconfig/iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 389 -j ACCEPT -A INPUT -m state --state NEW -m tcp -p tcp --dport 4444 -j ACCEPT # service iptables restart]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ldap</tag>
        <tag>sso</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 CentOS6 上安装 vncserver]]></title>
    <url>%2F2012%2F07%2F12%2Finstall-vncserver-for-centos63%2F</url>
    <content type="text"><![CDATA[在 CentOS 安装好之后，如果想要通过图形界面访问远程主机，需要安装 vnc server。 先查看本机是否有安装vnc： # rpm -q vnc-server 如果没有安装 # yum install vnc-server 把远程桌面的用户加入到配置文件中（这里的 2 和后边的端口有关系） # vim /etc/sysconfig/vncservers VNCSERVERS=&quot;2:root&quot; VNCSERVERARGS[2]=&quot;-geometry 1024x768&quot; 为配置的远程桌面用户设置密码（要在相应的帐号下边修改） # vncpasswd 启动 vnc server（这一步会生成 xstartup 文件） # service vncserver start 修改登录配置 # cd ~/.vnc/ (/root/.vnc) # vim xstartup 把最后一行 twm &amp; 注释，增加新的一行 gnome-session &amp;。最终的文件： #!/bin/sh [ -r /etc/sysconfig/i18n ] &amp;&amp; . /etc/sysconfig/i18n export LANG export SYSFONT vncconfig -iconic &amp; unset SESSION_MANAGER unset DBUS_SESSION_BUS_ADDRESS OS=`uname -s` if [ $OS = &apos;Linux&apos; ]; then case &quot;$WINDOWMANAGER&quot; in *gnome*) if [ -e /etc/SuSE-release ]; then PATH=$PATH:/opt/gnome/bin export PATH fi ;; esac fi if [ -x /etc/X11/xinit/xinitrc ]; then exec /etc/X11/xinit/xinitrc fi [ -r $HOME/.Xresources ] &amp;&amp; xrdb $HOME/.Xresources xsetroot -solid grey vncconfig -iconic &amp; xterm -geometry 80x24+10+10 -ls -title &quot;$VNCDESKTOP Desktop&quot; &amp; #twm &amp; gnome-session &amp; 重启 vncserver # service vncserver restart 这时有可能还是不能访问，因为有防火墙。接下来配置防火墙 # netstat -tunpl|grep vnc tcp 0 0 0.0.0.0:5902 0.0.0.0:* LISTEN 6253/Xvnc tcp 0 0 0.0.0.0:6002 0.0.0.0:* LISTEN 6253/Xvnc tcp 0 0 :::6002 :::* LISTEN 6253/Xvnc # vim /etc/sysconfig/iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 5902 -j ACCEPT -A INPUT -m state --state NEW -m tcp -p tcp --dport 6002 -j ACCEPT # service iptables restart 访问 通过浏览器：vnc://192.168.1.102:5902 通过客户端：192.168.1.102:2 有的客户端使用 192.168.1.102:6002]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>vnc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自定义 CAS 主题]]></title>
    <url>%2F2012%2F07%2F06%2Fcustom-cas-theme%2F</url>
    <content type="text"><![CDATA[这篇主要讲 CAS 自定义登录页面，主题名称叫作：cas-theme-twitter-bootstrap，是以 twitter bootstrap 为基础。 在 classes 目录下增加属性文件cas-theme-twitter-bootstrap.properties 配置主题的路径 standard.custom.css.file=themes/cas-theme-twitter-bootstrap/css/bootstrap.css twitter-bootstrap-views.properties 配置 jsp 的路径，这里只修改了登录页面。 casLoginView.url=/WEB-INF/view/jsp/twitter-bootstrap/casLoginView.jsp 修改 cas.properties指向新的属性文件，名称和上边的文件名对应。 cas.themeResolver.defaultThemeName=cas-theme-twitter-bootstrap cas.viewResolver.basename=twitter-bootstrap-views 新建对应的目录 themes/cas-theme-twitter-bootstrap 放入 css 和 image。 WEB-INF/view/jsp/twitter-bootstrap 根据默认的 jsp 修改相应的代码。 如果需要修改界面文字，从 target 下边复制 messages_zh_CN.properties 文件。这些文件在 mvn package 后会相应的增加或者替换到 CAS 的 war 包中。在开发环境可以使用 mvn tomcat6:run 直接进行测试。完整代码在 Github。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>sso</tag>
        <tag>cas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenDJ error code 53]]></title>
    <url>%2F2012%2F06%2F15%2Fopendj-error-code-53%2F</url>
    <content type="text"><![CDATA[By default, the OpenDJ LDAP directory server password policy is set to reject encrypted passwords.So when adding or importing data with encrypted passwords, the server returns some error like this: LDAP: error code 53 – Pre-encoded passwords are not allowed for the password attribute userPassword To allow pre-encoded passwords, the default password policy settings must be changed. This can be done using the dsconfig command line tool in advanced mode: $ dsconfig --advanced -p 4444 -h localhost -D &quot;cn=directory manager&quot; -X &gt;&gt;&gt;&gt; Specify OpenDS LDAP connection parameters Password for user &apos;cn=directory manager&apos;: &gt;&gt;&gt;&gt; OpenDS configuration console main menu What do you want to configure? 1) Access Control Handler 24) Monitor Provider 2) Account Status Notification 25) Network Group Handler 3) Administration Connector 26) Network Group Criteria 4) Alert Handler 27) Network Group Request Filtering Policy 5) Attribute Syntax 28) Network Group Resource Limits 6) Backend 29) Password Generator 7) Certificate Mapper 30) Password Policy 8) Connection Handler 31) Password Storage Scheme 9) Crypto Manager 32) Password Validator 10) Debug Target 33) Plugin 11) Entry Cache 34) Plugin Root 12) Extended Operation Handler 35) Replication Domain 13) Extension 36) Replication Server 14) Global Configuration 37) Root DN 15) Group Implementation 38) Root DSE Backend 16) Identity Mapper 39) SASL Mechanism Handler 17) Key Manager Provider 40) Synchronization Provider 18) Local DB Index 41) Trust Manager Provider 19) Local DB VLV Index 42) Virtual Attribute 20) Log Publisher 43) Work Queue 21) Log Retention Policy 44) Workflow 22) Log Rotation Policy 45) Workflow Element 23) Matching Rule q) quit Enter choice: 30 &gt;&gt;&gt;&gt; Password Policy management menu What would you like to do? 1) List existing Password Policies 2) Create a new Password Policy 3) View and edit an existing Password Policy 4) Delete an existing Password Policy b) back q) quit Enter choice [b]: 3 &gt;&gt;&gt;&gt; Select the Password Policy from the following list: 1) Default Password Policy 2) Root Password Policy c) cancel q) quit Enter choice [c]: 1 &gt;&gt;&gt;&gt; Configure the properties of the Password Policy Property Value(s) -------------------------------------------------------------------- 1) account-status-notification-handler - 2) allow-expired-password-changes false 3) allow-multiple-password-values false 4) allow-pre-encoded-passwords false 5) allow-user-password-changes true 6) default-password-storage-scheme Salted SHA-1 7) deprecated-password-storage-scheme - 8) expire-passwords-without-warning false 9) force-change-on-add false 10) force-change-on-reset false 11) grace-login-count 0 12) idle-lockout-interval 0 s 13) last-login-time-attribute - 14) last-login-time-format - 15) lockout-duration 0 s 16) lockout-failure-count 0 17) lockout-failure-expiration-interval 0 s 18) max-password-age 0 s 19) max-password-reset-age 0 s 20) min-password-age 0 s 21) password-attribute userpassword 22) password-change-requires-current-password false 23) password-expiration-warning-interval 5 d 24) password-generator Random Password Generator 25) password-history-count 0 26) password-history-duration 0 s 27) password-validator - 28) previous-last-login-time-format - 29) require-change-by-time - 30) require-secure-authentication false 31) require-secure-password-changes false 32) skip-validation-for-administrators false 33) state-update-failure-policy reactive ?) help f) finish - apply any changes to the Password Policy c) cancel q) quit Enter choice [f]: 4 &gt;&gt;&gt;&gt; Configuring the &quot;allow-pre-encoded-passwords&quot; property Indicates whether users can change their passwords by providing a pre-encoded value. This can cause a security risk because the clear-text version of the password is not known and therefore validation checks cannot be applied to it. Do you want to modify the &quot;allow-pre-encoded-passwords&quot; property? 1) Keep the default value: false 2) Change it to the value: true ?) help q) quit Enter choice [1]: 2 Press RETURN to continue &gt;&gt;&gt;&gt; Configure the properties of the Password Policy Property Value(s) -------------------------------------------------------------------- 1) account-status-notification-handler - 2) allow-expired-password-changes false 3) allow-multiple-password-values false 4) allow-pre-encoded-passwords true 5) allow-user-password-changes true 6) default-password-storage-scheme Salted SHA-1 7) deprecated-password-storage-scheme - 8) expire-passwords-without-warning false 9) force-change-on-add false 10) force-change-on-reset false 11) grace-login-count 0 12) idle-lockout-interval 0 s 13) last-login-time-attribute - 14) last-login-time-format - 15) lockout-duration 0 s 16) lockout-failure-count 0 17) lockout-failure-expiration-interval 0 s 18) max-password-age 0 s 19) max-password-reset-age 0 s 20) min-password-age 0 s 21) password-attribute userpassword 22) password-change-requires-current-password false 23) password-expiration-warning-interval 5 d 24) password-generator Random Password Generator 25) password-history-count 0 26) password-history-duration 0 s 27) password-validator - 28) previous-last-login-time-format - 29) require-change-by-time - 30) require-secure-authentication false 31) require-secure-password-changes false 32) skip-validation-for-administrators false 33) state-update-failure-policy reactive ?) help f) finish - apply any changes to the Password Policy c) cancel q) quit Enter choice [f]: The Password Policy was modified successfully Press RETURN to continue The equivalent non interactive command is: $ dsconfig set-password-policy-prop \ --policy-name &quot;Default Password Policy&quot; \ --set allow-pre-encoded-passwords:true \ --hostname localhost \ --trustAll \ --port 4444 \ --bindDN &quot;cn=directory manager&quot; \ --bindPassword ****** \ --no-prompt]]></content>
      <categories>
        <category>ldap</category>
      </categories>
      <tags>
        <tag>ldap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统一身份管理和单点登录]]></title>
    <url>%2F2012%2F05%2F08%2Fidentity-management%2F</url>
    <content type="text"><![CDATA[1. 方案一 调用 OpenIDM REST API 管理组织、用户（CRUD，需要开发）。 通过 OpenIDM Core Service 推送数据到 OpenDJ（LDAP）、业务系统数据库。 需要至少提供两台 Linux（RHEL6）主机，增加的系统服务有 OpenDJ、OpenIDM、OpenAM、MySQL、身份管理 Web 服务。 OpenDJ 和 MySQL 在一台机器（做数据备份），其余在另外一台。OpenDJ 和 OpenAM 交叉 HA。 优点： 前期开发量比较小，见效快。 增加新的数据类型非常方便。 同步的功能很强大，不需要客户端开发同步的代码，只需要在服务端写配置文件。 缺点： 后期需要根据不同的业务系统在 OpenIDM 中进行数据映射（需要了解业务系统相关的数据结构）。 随着系统的加入，OpenIDM 的维护工作会越来越重。 不太方便第三方系统测试、调试同步机制（这个工作需要在服务端完成）。 2. 方案二 抛弃 OpenIDM，直接管理 OpenDJ 的数据（CRUD，需要开发）。 增加一个 MQ Server，当在 OpenDJ 中增加数据时，发一条 message 给 MQ Server（消息中组织、用户的数据用 JSON 封装）。 业务系统需要实现一个 Client，message 会被即时推送给业务系统（开发量非常小）。 业务系统收到消息后自行处理。如果处理中出现异常，建议把消息内容先存下来再稍后处理。 如果需要保证数据的一致性，在 MQ 中启动 acknowledgment。 服务端需要提供一个查询所有组织、用户的接口，用于系统第一次对接。 需要至少提供一到两台 Linux（RHEL6）主机，增加的系统服务有 OpenDJ、OpenAM、MQ、身份管理 Web 服务。 OpenDJ 和 MQ 在一台机器，其余在另外一台。OpenDJ 和 OpenAM 交叉 HA。 优点： 灵活性增加。 服务端不用再考虑和了解业务系统的数据结构。 服务端和业务系统异步交互，服务端的后期工作大大降低。 不需要安装 OpenIDM 和 MySQL 两个服务。 因为不直接开放给业务系统调用（身份管理），几乎不用考虑服务端的性能。 缺点： 相对方案一，服务端和客户端开发量都增加。 服务端可能还需要开发一个 SDK 给客户端调用。 需要服务端自己开发管理 OpenDJ（LDAP） 的代码（相对通过调用 OpenIDM REST API 工作量要大）。 需要在服务端部署一个 MQ Server。 需要客户端实现一个 MQ Client ，并且自己处理组织、用户数据。 3. 都需要注意的 图中只关注了数据同步的数据流向，没有体现 App 和 SSO 的关系。 客户端需要有组织、用户表用来同步数据。 对于 OpenDJ（LDAP），OpenAM（SSO），一定要考虑 HA。要求高的最好在不同的机房。所以物理机至少是两台。 要考虑机器配置冗余，每个服务至少要分配 2-3G 内存。 需要有自己的 DNS Server。 如果对 SSO 的安全性要求很高，建议使用 HTTPS 协议，需要有 SSL 证书。自行签发的证书会影响用户体验。 4. 对于新的业务系统 不需要单独的管理组织、用户模块，可以方便的使用统一的组织、用户数据。 不需要实现用户认证功能，只需要和 OpenAM 集成。 5. 对于遗留业务系统 如果不能重新初始化组织、用户数据，那么需要建立遗留业务系统和底层身份管理平台的映射关系。 屏蔽遗留业务系统的认证模块。 如果权限控制要求不是很复杂，可以使用 OpenAM 完成统一的业务系统授权和访问控制。否则可以自己开发 OpenAM 授权插件或者业务系统自己实现。 安装 OpenAM Agent 或者使用 SDK 开发简单的认证接口。可参考开发 OpenAM Spring Security 3 客户端和开发 OpenAM Java 客户端。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>ldap</tag>
        <tag>sso</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进阶使用 OpenIDM 【二】]]></title>
    <url>%2F2012%2F05%2F03%2Fstep-by-step-two-openidm%2F</url>
    <content type="text"><![CDATA[在正式的企业场景中，组织一般具有父子关系。我们来看一下在这种情况下，OpenIDM 如何配置。 1. 增加新的类型 organizationUnit在 openidm/conf/managed.json 中增加 { &quot;name&quot; : &quot;organizationUnit&quot; } 查询 organizationUnit # curl \ --header &quot;X-OpenIDM-Username: openidm-admin&quot; \ --header &quot;X-OpenIDM-Password: openidm-admin&quot; \ http://openam.example.com:9090/openidm/managed/organizationUnit/?_query-id=query-all-ids 结果 {&quot;query-time-ms&quot;:6,&quot;result&quot;:[]} 增加 organizationUnit # curl \ --header &quot;X-OpenIDM-Username: openidm-admin&quot; \ --header &quot;X-OpenIDM-Password: openidm-admin&quot; \ --request PUT \ --data &apos;{ &quot;name&quot;:&quot;ideal&quot;, &quot;dn&quot;:&quot;ou=ideal,o=shanghai,dc=example,dc=com&quot;, &quot;description&quot;:&quot;ideal company&quot; }&apos; \ http://openam.example.com:9090/openidm/managed/organizationUnit/ideal 返回结果 {&quot;_id&quot;:&quot;ideal&quot;,&quot;_rev&quot;:&quot;0&quot;} 查询新增加的 organizationUnit # curl \ --header &quot;X-OpenIDM-Username: openidm-admin&quot; \ --header &quot;X-OpenIDM-Password: openidm-admin&quot; \ http://openam.example.com:9090/openidm/managed/organizationUnit/ideal 返回结果 { &quot;_rev&quot;:&quot;0&quot;, &quot;_id&quot;:&quot;ideal&quot;, &quot;dn&quot;:&quot;ou=ideal,o=shanghai,dc=example,dc=com&quot;, &quot;description&quot;:&quot;ideal company&quot;, &quot;name&quot;:&quot;ideal&quot; } 2. 配置 OpenIDM 同步在 sync.json 中增加 { &quot;name&quot; : &quot;managedOrganizationUnit_hrdb&quot;, &quot;source&quot; : &quot;managed/organizationUnit&quot;, &quot;target&quot; : &quot;system/hrdb/organization&quot;, &quot;properties&quot; : [ { &quot;source&quot; : &quot;description&quot;, &quot;target&quot; : &quot;description&quot; }, { &quot;source&quot; : &quot;name&quot;, &quot;target&quot; : &quot;name&quot; } ], &quot;policies&quot; : [ { &quot;situation&quot; : &quot;CONFIRMED&quot;, &quot;action&quot; : &quot;UPDATE&quot; }, { &quot;situation&quot; : &quot;FOUND&quot;, &quot;action&quot; : &quot;UPDATE&quot; }, { &quot;situation&quot; : &quot;ABSENT&quot;, &quot;action&quot; : &quot;CREATE&quot; }, { &quot;situation&quot; : &quot;AMBIGUOUS&quot;, &quot;action&quot; : &quot;EXCEPTION&quot; }, { &quot;situation&quot; : &quot;MISSING&quot;, &quot;action&quot; : &quot;UNLINK&quot; }, { &quot;situation&quot; : &quot;SOURCE_MISSING&quot;, &quot;action&quot; : &quot;IGNORE&quot; }, { &quot;situation&quot; : &quot;UNQUALIFIED&quot;, &quot;action&quot; : &quot;IGNORE&quot; }, { &quot;situation&quot; : &quot;UNASSIGNED&quot;, &quot;action&quot; : &quot;IGNORE&quot; } ] }, { &quot;name&quot; : &quot;managedOrganizationUnit_ldap&quot;, &quot;source&quot; : &quot;managed/organizationUnit&quot;, &quot;target&quot; : &quot;system/ldap/organizationalUnit&quot;, &quot;properties&quot; : [ { &quot;source&quot; : &quot;description&quot;, &quot;target&quot; : &quot;description&quot; }, { &quot;source&quot; : &quot;name&quot;, &quot;target&quot; : &quot;ou&quot; }, { &quot;source&quot; : &quot;dn&quot;, &quot;target&quot; : &quot;dn&quot; } ], &quot;policies&quot; : [ { &quot;situation&quot; : &quot;CONFIRMED&quot;, &quot;action&quot; : &quot;UPDATE&quot; }, { &quot;situation&quot; : &quot;FOUND&quot;, &quot;action&quot; : &quot;LINK&quot; }, { &quot;situation&quot; : &quot;ABSENT&quot;, &quot;action&quot; : &quot;CREATE&quot; }, { &quot;situation&quot; : &quot;AMBIGUOUS&quot;, &quot;action&quot; : &quot;IGNORE&quot; }, { &quot;situation&quot; : &quot;MISSING&quot;, &quot;action&quot; : &quot;IGNORE&quot; }, { &quot;situation&quot; : &quot;SOURCE_MISSING&quot;, &quot;action&quot; : &quot;IGNORE&quot; }, { &quot;situation&quot; : &quot;UNQUALIFIED&quot;, &quot;action&quot; : &quot;IGNORE&quot; }, { &quot;situation&quot; : &quot;UNASSIGNED&quot;, &quot;action&quot; : &quot;IGNORE&quot; } ] } 修改 provisioner.openicf-ldap.json，在 “objectTypes” 中增加 &quot;organizationalUnit&quot; : { &quot;$schema&quot; : &quot;http://json-schema.org/draft-03/schema&quot;, &quot;id&quot; : &quot;organizationalUnit&quot;, &quot;type&quot; : &quot;object&quot;, &quot;nativeType&quot; : &quot;organizationalUnit&quot;, &quot;properties&quot; : { &quot;preferredDeliveryMethod&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeName&quot; : &quot;preferredDeliveryMethod&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;l&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;l&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;businessCategory&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;businessCategory&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;street&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;street&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;postOfficeBox&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;postOfficeBox&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;postalCode&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;postalCode&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;st&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;st&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;registeredAddress&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;registeredAddress&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;postalAddress&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;postalAddress&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;objectClass&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;objectClass&quot;, &quot;nativeType&quot; : &quot;string&quot;, &quot;flags&quot; : [ &quot;NOT_CREATABLE&quot;, &quot;NOT_UPDATEABLE&quot; ] }, &quot;description&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;description&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;ou&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;required&quot; : true, &quot;nativeName&quot; : &quot;ou&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;physicalDeliveryOfficeName&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;physicalDeliveryOfficeName&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;telexNumber&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;telexNumber&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;teletexTerminalIdentifier&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;teletexTerminalIdentifier&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;userPassword&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;JAVA_TYPE_BYTE_ARRAY&quot; }, &quot;nativeName&quot; : &quot;userPassword&quot;, &quot;nativeType&quot; : &quot;JAVA_TYPE_BYTE_ARRAY&quot; }, &quot;dn&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;required&quot; : true, &quot;nativeName&quot; : &quot;__NAME__&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;telephoneNumber&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;telephoneNumber&quot;, &quot;nativeType&quot; : &quot;string&quot; } } } 重启 OpenIDM，执行 # curl \ --header &quot;X-OpenIDM-Username: openidm-admin&quot; \ --header &quot;X-OpenIDM-Password: openidm-admin&quot; \ --request POST &quot;http://openam.example.com:9090/openidm/sync?_action=recon&amp;mapping=managedOrganizationUnit_hrdb&quot; # curl \ --header &quot;X-OpenIDM-Username: openidm-admin&quot; \ --header &quot;X-OpenIDM-Password: openidm-admin&quot; \ --request POST &quot;http://openam.example.com:9090/openidm/sync?_action=recon&amp;mapping=managedOrganizationUnit_ldap&quot; 返回 {&quot;reconId&quot;:&quot;acc2de0a-59ec-4537-9115-333be932aecd&quot;} 这时查看 OpenDJ 和 MySQL，已经同步成功。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>ldap</tag>
        <tag>sso</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进阶使用 OpenIDM 【一】]]></title>
    <url>%2F2012%2F05%2F02%2Fstep-by-step-one-openidm%2F</url>
    <content type="text"><![CDATA[1. From OpenIDM To MySQL以 samples/sample3 为模板，把新增加的 use,organization 分别同步到 MySQL 的 Users,Organizations 表。如果你看过 使用 OpenIDM RESTful API ,这时在 managedobjects 表中应该有两条数据。 managed/user { &quot;_rev&quot;:&quot;0&quot;, &quot;_id&quot;:&quot;joe&quot;, &quot;email&quot;:[&quot;joe@example.com&quot;], &quot;description&quot;:&quot;My first user&quot;, &quot;familyName&quot;:&quot;smith&quot;, &quot;userName&quot;:&quot;joe&quot;, &quot;givenName&quot;:&quot;joe&quot;, &quot;displayName&quot;:&quot;Felicitas Doe&quot; } managed/organization { &quot;_rev&quot;:&quot;0&quot;, &quot;_id&quot;:&quot;shanghai&quot;, &quot;dn&quot;:&quot;o=shanghai,dc=example,dc=com&quot;, &quot;description&quot;:&quot;shanghai&quot;, &quot;name&quot;:&quot;shanghai&quot; } 执行 # cp -r samples/sample3/conf samples/sample3/tools . 在我使用的这个版本中，CreateScript.groovy 脚本中 __ACCOUNT__ 这段内容的语法有一个错误，参数属性的最后一段应该是 , 这里写成了 ; 。 用以下内容替换 sync.json 文件 { &quot;mappings&quot; : [ { &quot;name&quot; : &quot;managedOrganization_hrdb&quot;, &quot;source&quot; : &quot;managed/organization&quot;, &quot;target&quot; : &quot;system/hrdb/organization&quot;, &quot;properties&quot; : [ { &quot;source&quot; : &quot;description&quot;, &quot;target&quot; : &quot;description&quot; }, { &quot;source&quot; : &quot;name&quot;, &quot;target&quot; : &quot;name&quot; } ], &quot;policies&quot; : [ { &quot;situation&quot; : &quot;CONFIRMED&quot;, &quot;action&quot; : &quot;UPDATE&quot; }, { &quot;situation&quot; : &quot;FOUND&quot;, &quot;action&quot; : &quot;UPDATE&quot; }, { &quot;situation&quot; : &quot;ABSENT&quot;, &quot;action&quot; : &quot;CREATE&quot; }, { &quot;situation&quot; : &quot;AMBIGUOUS&quot;, &quot;action&quot; : &quot;EXCEPTION&quot; }, { &quot;situation&quot; : &quot;MISSING&quot;, &quot;action&quot; : &quot;UNLINK&quot; }, { &quot;situation&quot; : &quot;SOURCE_MISSING&quot;, &quot;action&quot; : &quot;IGNORE&quot; }, { &quot;situation&quot; : &quot;UNQUALIFIED&quot;, &quot;action&quot; : &quot;IGNORE&quot; }, { &quot;situation&quot; : &quot;UNASSIGNED&quot;, &quot;action&quot; : &quot;IGNORE&quot; } ] }, { &quot;name&quot; : &quot;managedUser_hrdb&quot;, &quot;source&quot; : &quot;managed/user&quot;, &quot;target&quot; : &quot;system/hrdb/account&quot;, &quot;properties&quot; : [ { &quot;source&quot; : &quot;email&quot;, &quot;target&quot; : &quot;email&quot; }, { &quot;source&quot; : &quot;userName&quot;, &quot;target&quot; : &quot;uid&quot; }, { &quot;source&quot; : &quot;familyName&quot;, &quot;target&quot; : &quot;lastName&quot; }, { &quot;source&quot; : &quot;givenName&quot;, &quot;target&quot; : &quot;firstName&quot; }, { &quot;source&quot; : &quot;displayName&quot;, &quot;target&quot; : &quot;fullName&quot; }, { &quot;source&quot; : &quot;description&quot;, &quot;target&quot; : &quot;organization&quot; } ], &quot;policies&quot; : [ { &quot;situation&quot; : &quot;CONFIRMED&quot;, &quot;action&quot; : &quot;UPDATE&quot; }, { &quot;situation&quot; : &quot;FOUND&quot;, &quot;action&quot; : &quot;UPDATE&quot; }, { &quot;situation&quot; : &quot;ABSENT&quot;, &quot;action&quot; : &quot;CREATE&quot; }, { &quot;situation&quot; : &quot;AMBIGUOUS&quot;, &quot;action&quot; : &quot;EXCEPTION&quot; }, { &quot;situation&quot; : &quot;MISSING&quot;, &quot;action&quot; : &quot;UNLINK&quot; }, { &quot;situation&quot; : &quot;SOURCE_MISSING&quot;, &quot;action&quot; : &quot;IGNORE&quot; }, { &quot;situation&quot; : &quot;UNQUALIFIED&quot;, &quot;action&quot; : &quot;IGNORE&quot; }, { &quot;situation&quot; : &quot;UNASSIGNED&quot;, &quot;action&quot; : &quot;IGNORE&quot; } ] } ]} provisioner.openicf-scriptedsql.json 文件中只需要修改 configurationProperties 这段内容 &quot;configurationProperties&quot; : { ... &quot;password&quot; : &quot;Your MySQL password&quot;, ... &quot;createScriptFileName&quot; : &quot;/opt/openidm/tools/CreateScript.groovy&quot;, &quot;testScriptFileName&quot; : &quot;/opt/openidm/tools/TestScript.groovy&quot;, &quot;searchScriptFileName&quot; : &quot;/opt/openidm/tools/SearchScript.groovy&quot;, &quot;deleteScriptFileName&quot; : &quot;/opt/openidm/tools/DeleteScript.groovy&quot;, &quot;updateScriptFileName&quot; : &quot;/opt/openidm/tools/UpdateScript.groovy&quot;, &quot;syncScriptFileName&quot; : &quot;/opt/openidm/tools/SyncScript.groovy&quot; } 重启 OpenIDM 后，执行 # curl \ --header &quot;X-OpenIDM-Username: openidm-admin&quot; \ --header &quot;X-OpenIDM-Password: openidm-admin&quot; \ --request POST &quot;http://openam.example.com:9090/openidm/sync?_action=recon&amp;mapping=managedOrganization_hrdb&quot; # curl \ --header &quot;X-OpenIDM-Username: openidm-admin&quot; \ --header &quot;X-OpenIDM-Password: openidm-admin&quot; \ --request POST &quot;http://openam.example.com:9090/openidm/sync?_action=recon&amp;mapping=managedUser_hrdb&quot; 返回 JSON 结果 {&quot;reconId&quot;:&quot;6073945c-60d8-4db3-97e5-e0ac0e5753d6&quot;} 然后查看 MySQL 数据库 Users 和 Organizations ，数据已经成功同步。 2. From OpenIDM To OpenDJ以 samples/sample2 为模板（不要覆盖之前的 sync.json 文件），把新增加的 use,organization 同步到 OpenDJ 。 # cp samples/sample2b/conf/provisioner.openicf-ldap.json conf # cp -r samples/sample2b/script . 在 sync.json 中增加 { &quot;name&quot; : &quot;managedOrganization_ldap&quot;, &quot;source&quot; : &quot;managed/organization&quot;, &quot;target&quot; : &quot;system/ldap/organization&quot;, &quot;properties&quot; : [ { &quot;source&quot; : &quot;description&quot;, &quot;target&quot; : &quot;description&quot; }, { &quot;source&quot; : &quot;name&quot;, &quot;target&quot; : &quot;o&quot; }, { &quot;source&quot; : &quot;dn&quot;, &quot;target&quot; : &quot;dn&quot; } ], &quot;policies&quot; : [ { &quot;situation&quot; : &quot;CONFIRMED&quot;, &quot;action&quot; : &quot;UPDATE&quot; }, { &quot;situation&quot; : &quot;FOUND&quot;, &quot;action&quot; : &quot;LINK&quot; }, { &quot;situation&quot; : &quot;ABSENT&quot;, &quot;action&quot; : &quot;CREATE&quot; }, { &quot;situation&quot; : &quot;AMBIGUOUS&quot;, &quot;action&quot; : &quot;IGNORE&quot; }, { &quot;situation&quot; : &quot;MISSING&quot;, &quot;action&quot; : &quot;IGNORE&quot; }, { &quot;situation&quot; : &quot;SOURCE_MISSING&quot;, &quot;action&quot; : &quot;IGNORE&quot; }, { &quot;situation&quot; : &quot;UNQUALIFIED&quot;, &quot;action&quot; : &quot;IGNORE&quot; }, { &quot;situation&quot; : &quot;UNASSIGNED&quot;, &quot;action&quot; : &quot;IGNORE&quot; } ] }, { &quot;name&quot; : &quot;managedUser_ldap&quot;, &quot;source&quot; : &quot;managed/user&quot;, &quot;target&quot; : &quot;system/ldap/account&quot;, &quot;correlationQuery&quot; : { &quot;type&quot; : &quot;text/javascript&quot;, &quot;file&quot; : &quot;script/ldapBackCorrelationQuery.js&quot; }, &quot;properties&quot; : [ { &quot;source&quot; : &quot;givenName&quot;, &quot;target&quot; : &quot;givenName&quot; }, { &quot;source&quot; : &quot;familyName&quot;, &quot;target&quot; : &quot;sn&quot; }, { &quot;source&quot; : &quot;displayName&quot;, &quot;target&quot; : &quot;cn&quot; }, { &quot;source&quot; : &quot;userName&quot;, &quot;target&quot; : &quot;uid&quot; }, { &quot;source&quot; : &quot;description&quot;, &quot;target&quot; : &quot;description&quot; }, { &quot;source&quot; : &quot;email&quot;, &quot;target&quot; : &quot;mail&quot; } ], &quot;onCreate&quot; : { &quot;type&quot; : &quot;text/javascript&quot;, &quot;source&quot; : &quot;target.dn = &apos;uid=&apos; + source.userName + &apos;,o=shanghai,dc=example,dc=com&apos;;&quot; }, &quot;policies&quot; : [ { &quot;situation&quot; : &quot;CONFIRMED&quot;, &quot;action&quot; : &quot;UPDATE&quot; }, { &quot;situation&quot; : &quot;FOUND&quot;, &quot;action&quot; : &quot;LINK&quot; }, { &quot;situation&quot; : &quot;ABSENT&quot;, &quot;action&quot; : &quot;CREATE&quot; }, { &quot;situation&quot; : &quot;AMBIGUOUS&quot;, &quot;action&quot; : &quot;IGNORE&quot; }, { &quot;situation&quot; : &quot;MISSING&quot;, &quot;action&quot; : &quot;IGNORE&quot; }, { &quot;situation&quot; : &quot;SOURCE_MISSING&quot;, &quot;action&quot; : &quot;IGNORE&quot; }, { &quot;situation&quot; : &quot;UNQUALIFIED&quot;, &quot;action&quot; : &quot;IGNORE&quot; }, { &quot;situation&quot; : &quot;UNASSIGNED&quot;, &quot;action&quot; : &quot;IGNORE&quot; } ] } 修改 provisioner.openicf-ldap.json，在 “objectTypes” 中增加 &quot;organization&quot; : { &quot;$schema&quot; : &quot;http://json-schema.org/draft-03/schema&quot;, &quot;id&quot; : &quot;organization&quot;, &quot;type&quot; : &quot;object&quot;, &quot;nativeType&quot; : &quot;organization&quot;, &quot;properties&quot; : { &quot;preferredDeliveryMethod&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeName&quot; : &quot;preferredDeliveryMethod&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;seeAlso&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;seeAlso&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;x121Address&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;x121Address&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;l&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;l&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;o&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;required&quot; : true, &quot;nativeName&quot; : &quot;o&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;businessCategory&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;businessCategory&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;street&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;street&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;postOfficeBox&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;postOfficeBox&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;postalCode&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;postalCode&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;st&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;st&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;registeredAddress&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;registeredAddress&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;postalAddress&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;postalAddress&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;objectClass&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;objectClass&quot;, &quot;nativeType&quot; : &quot;string&quot;, &quot;flags&quot; : [ &quot;NOT_CREATABLE&quot;, &quot;NOT_UPDATEABLE&quot; ] }, &quot;description&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;description&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;internationaliSDNNumber&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;internationaliSDNNumber&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;searchGuide&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;searchGuide&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;physicalDeliveryOfficeName&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;physicalDeliveryOfficeName&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;telexNumber&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;telexNumber&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;teletexTerminalIdentifier&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;teletexTerminalIdentifier&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;userPassword&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;JAVA_TYPE_BYTE_ARRAY&quot; }, &quot;nativeName&quot; : &quot;userPassword&quot;, &quot;nativeType&quot; : &quot;JAVA_TYPE_BYTE_ARRAY&quot; }, &quot;dn&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;required&quot; : true, &quot;nativeName&quot; : &quot;__NAME__&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;telephoneNumber&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;telephoneNumber&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;destinationIndicator&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;destinationIndicator&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;facsimileTelephoneNumber&quot; : { &quot;type&quot; : &quot;array&quot;, &quot;items&quot; : { &quot;type&quot; : &quot;string&quot;, &quot;nativeType&quot; : &quot;string&quot; }, &quot;nativeName&quot; : &quot;facsimileTelephoneNumber&quot;, &quot;nativeType&quot; : &quot;string&quot; } } } 重启 OpenIDM，执行 # curl \ --header &quot;X-OpenIDM-Username: openidm-admin&quot; \ --header &quot;X-OpenIDM-Password: openidm-admin&quot; \ --request POST &quot;http://openam.example.com:9090/openidm/sync?_action=recon&amp;mapping=managedOrganization_ldap&quot; # curl \ --header &quot;X-OpenIDM-Username: openidm-admin&quot; \ --header &quot;X-OpenIDM-Password: openidm-admin&quot; \ --request POST &quot;http://openam.example.com:9090/openidm/sync?_action=recon&amp;mapping=managedUser_ldap&quot; 返回 {&quot;reconId&quot;:&quot;60a9fa62-00ca-4dff-a538-94f09c161fde&quot;} 这时查看 OpenDJ，已经同步成功。 3. 定时同步除了可以调用 REST API 同步之外，OpenIDM 也提供了 Cron 的方式进行调度。可以分别为上边的 4 个接口建立 scheduler-recon 文件。以 managedOrganizationUnit_ldap 为例 # cp samples/sample2/conf/scheduler-recon.json conf # mv conf/scheduler-recon.json conf/scheduler-recon_managedOrganization_ldap.json 打开文件，enabled 修改为 true，schedule 和 mapping 修改为相应的配置 { &quot;enabled&quot; : true, &quot;type&quot;: &quot;cron&quot;, &quot;schedule&quot;: &quot;1 * * * * ?&quot;, &quot;invokeService&quot;: &quot;sync&quot;, &quot;invokeContext&quot;: { &quot;action&quot;: &quot;reconcile&quot;, &quot;mapping&quot;: &quot;managedOrganization_ldap&quot; } } 重启 OpenIDM 后， Organization 和 User 会分别同步到 OpenDJ 和 MySQL。 如果需要停止调度任务，除了修改 scheduler-recon.json 文件，可能还需要删除 configobjects 表中的 org.forgerock.openidm.scheduler 相关纪录。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>ldap</tag>
        <tag>sso</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重置 MySQL root 密码]]></title>
    <url>%2F2012%2F04%2F26%2Freset-mysql-root-password-on-mac%2F</url>
    <content type="text"><![CDATA[停止 MySQL 服务，运行 # mysqld_safe --skip-grant-tables &gt;/dev/null 2&gt;&amp;1 &amp; # mysql -u root mysql mysql&gt; update user set password = Password(&apos;new password&apos;) where User = &apos;root&apos;; mysql&gt; flush privileges; mysql&gt; exit; # killall mysqld; 启动 MySQL 服务，运行 # mysql -r root -p Enter password:new password Welcome to the MySQL monitor. Commands end with ; or \g. Your MySQL connection id is 402 Server version: 5.5.16 MySQL Community Server (GPL) Copyright (c) 2000, 2011, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement. mysql&gt;]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用 OpenIDM RESTful API]]></title>
    <url>%2F2012%2F04%2F20%2Fusing-openidm-with-rest-api%2F</url>
    <content type="text"><![CDATA[1. 使用默认的类型 user在安装完成以后，使用下边的方法查看 OpenIDM 仓库的中所有用户 # curl \ --header &quot;X-OpenIDM-Username: openidm-admin&quot; \ --header &quot;X-OpenIDM-Password: openidm-admin&quot; \ http://openam.example.com:9090/openidm/managed/user/?_query-id=query-all-ids 返回的 JSON 显示结果为空。 {&quot;query-time-ms&quot;:1,&quot;result&quot;:[]} 增加用户 # curl \ --header &quot;X-OpenIDM-Username: openidm-admin&quot; \ --header &quot;X-OpenIDM-Password: openidm-admin&quot; \ --request PUT \ --data &apos;{ &quot;userName&quot;:&quot;joe&quot;, &quot;givenName&quot;:&quot;joe&quot;, &quot;familyName&quot;:&quot;smith&quot;, &quot;email&quot;:[&quot;joe@example.com&quot;], &quot;displayName&quot;:&quot;Felicitas Doe&quot;, &quot;description&quot;:&quot;My first user&quot; }&apos; \ http://openam.example.com:9090/openidm/managed/user/joe 返回的 JSON 结果 {&quot;_id&quot;:&quot;joe&quot;,&quot;_rev&quot;:&quot;0&quot;} 查询新增加的用户 # curl \ --header &quot;X-OpenIDM-Username: openidm-admin&quot; \ --header &quot;X-OpenIDM-Password: openidm-admin&quot; \ http://openam.example.com:9090/openidm/managed/user/joe 返回的 JSON 结果 { &quot;_rev&quot;:&quot;0&quot;, &quot;_id&quot;:&quot;joe&quot;, &quot;email&quot;:[&quot;joe@example.com&quot;], &quot;description&quot;:&quot;My first user&quot;, &quot;familyName&quot;:&quot;smith&quot;, &quot;userName&quot;:&quot;joe&quot;, &quot;givenName&quot;:&quot;joe&quot;, &quot;displayName&quot;:&quot;Felicitas Doe&quot; } Java 代码在 Github 。 2. 在 managed.json 中增加新类型 organization{ &quot;name&quot; : &quot;organization&quot; } 查询 organization # curl \ --header &quot;X-OpenIDM-Username: openidm-admin&quot; \ --header &quot;X-OpenIDM-Password: openidm-admin&quot; \ http://openam.example.com:9090/openidm/managed/organization/?_query-id=query-all-ids 结果 {&quot;query-time-ms&quot;:2,&quot;result&quot;:[]} 增加 organization # curl \ --header &quot;X-OpenIDM-Username: openidm-admin&quot; \ --header &quot;X-OpenIDM-Password: openidm-admin&quot; \ --request PUT \ --data &apos;{ &quot;name&quot;:&quot;shanghai&quot;, &quot;dn&quot;:&quot;o=shanghai,dc=example,dc=com&quot;, &quot;description&quot;:&quot;shanghai&quot; }&apos; \ http://openam.example.com:9090/openidm/managed/organization/shanghai 返回结果 {&quot;_id&quot;:&quot;shanghai&quot;,&quot;_rev&quot;:&quot;0&quot;} 查询新增加的 organization # curl \ --header &quot;X-OpenIDM-Username: openidm-admin&quot; \ --header &quot;X-OpenIDM-Password: openidm-admin&quot; \ http://openam.example.com:9090/openidm/managed/organization/shanghai 返回结果 { &quot;_rev&quot;:&quot;0&quot;, &quot;_id&quot;:&quot;shanghai&quot;, &quot;dn&quot;:&quot;o=shanghai,dc=example,dc=com&quot;, &quot;description&quot;:&quot;shanghai&quot;, &quot;name&quot;:&quot;shanghai&quot; }]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>ldap</tag>
        <tag>sso</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 RHEL6 上安装 OpenIDM]]></title>
    <url>%2F2012%2F04%2F18%2Finstalling-openidm-on-rhel6%2F</url>
    <content type="text"><![CDATA[确认 Java 环境，需要 update 24 以上版本。 # java -version java version &quot;1.6.0_31&quot; Java(TM) SE Runtime Environment (build 1.6.0_31-b04) Java HotSpot(TM) 64-Bit Server VM (build 20.6-b01, mixed mode) 下载 # wget http://download.forgerock.org/downloads/openidm/snapshot/openidm-2.1.0-SNAPSHOT.zip 安装 # cp openidm-2.1.0-SNAPSHOT.zip /opt # cd /opt # unzip openidm-2.1.0-SNAPSHOT.zip 默认情况，OpenIDM 监听在 8080 和 8443 端口，这里因为和 OpenAM 用的同一台 Server，所以修改为 9090，9443。 # cd openidm # vim conf/jetty.xml 如果在生产环境，需要替换默认的 OrientDB。这里替换为 MySQL。下载 MySQL Connector/J 解压缩以后 # cp mysql-connector-java-5.1.19-bin.jar /opt/openidm/bundle/ # cd /opt/openidm/conf # mv repo.orientdb.json repo.orientdb.json.bak # cp ../samples/misc/repo.jdbc.json . # mysql -u root -p &lt; /opt/openidm/db/scripts/mysql/openidm.sql # mysql -u root -p Enter password: Welcome to the MySQL monitor. Commands end with ; or \g. Your MySQL connection id is 4 Server version: 5.1.52 Source distribution mysql&gt; use openidm; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed mysql&gt; show tables; +-------------------------+ | Tables_in_openidm | +-------------------------+ | auditaccess | | auditactivity | | auditrecon | | configobjectproperties | | configobjects | | genericobjectproperties | | genericobjects | | internaluser | | links | | managedobjectproperties | | managedobjects | | objecttypes | +-------------------------+ 12 rows in set (0.00 sec) 在启动 OpenIDM 之前，如果需要的话，修改 repo.jdbc.json # vim repo.jdbc.json &quot;connection&quot; : { &quot;dbType&quot; : &quot;MYSQL&quot;, &quot;jndiName&quot; : &quot;&quot;, &quot;driverClass&quot; : &quot;com.mysql.jdbc.Driver&quot;, &quot;jdbcUrl&quot; : &quot;jdbc:mysql://localhost:3306/openidm&quot;, &quot;username&quot; : &quot;root&quot;, &quot;password&quot; : &quot;&quot;, &quot;defaultCatalog&quot; : &quot;openidm&quot;, &quot;maxBatchSize&quot; : 100, &quot;maxTxRetry&quot; : 5 } 启动 OpenIDM # cd /opt/openidm # ./startup.sh Using OPENIDM_HOME: /opt/openidm Using OPENIDM_OPTS: -Xmx1024m Using LOGGING_CONFIG: -Djava.util.logging.config.file=/opt/openidm/conf/logging.properties Using boot properties at /opt/openidm/conf/boot/boot.properties OpenIDM version &quot;2.1.0-SNAPSHOT&quot; (revision: 1063) -&gt; scr list Id State Name [ 16] [active ] org.forgerock.openidm.config.starter [ 7] [active ] org.forgerock.openidm.external.rest [ 11] [active ] org.forgerock.openidm.provisioner.openicf.connectorinfoprovider [ 1] [active ] org.forgerock.openidm.router [ 18] [active ] org.forgerock.openidm.scheduler [ 13] [active ] org.forgerock.openidm.restlet [ 6] [unsatisfied ] org.forgerock.openidm.external.email [ 15] [unsatisfied ] org.forgerock.openidm.repo.orientdb [ 5] [active ] org.forgerock.openidm.sync [ 3] [active ] org.forgerock.openidm.script [ 2] [active ] org.forgerock.openidm.scope [ 9] [active ] org.forgerock.openidm.http.contextregistrator [ 17] [active ] org.forgerock.openidm.config [ 0] [active ] org.forgerock.openidm.audit [ 14] [active ] org.forgerock.openidm.repo.jdbc [ 4] [active ] org.forgerock.openidm.managed [ 12] [active ] org.forgerock.openidm.provisioner.openicf [ 8] [active ] org.forgerock.openidm.authentication [ 10] [active ] org.forgerock.openidm.provisioner -&gt; 如果看到 email 和 orientdb 是 unsatisfied，repo.jdbc 是 active 就成功了。如果有其它的 unsatisfied检查 openidm/logs 或者查看 Troubleshooting。 现在访问：http://openam.example.com:9090/system/console, 使用 admin/admin 登录控制台。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ldap</tag>
        <tag>sso</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开发 OpenAM Spring Security 3 客户端应用]]></title>
    <url>%2F2012%2F04%2F16%2Fopenam-integrate-with-spring-security-3%2F</url>
    <content type="text"><![CDATA[在开始 Maven 之前，需要先引入一个包，这个包的作用和原来的 Agent 的功能是一样的。这个包需要自己从源码编译，mvn install 或者 mvn deploy 加入到自己的仓库中。这样在 pom.xml 中可以引入 &lt;dependency&gt; &lt;groupId&gt;com.sun.identity.provider&lt;/groupId&gt; &lt;artifactId&gt;opensso-springsecurity&lt;/artifactId&gt; &lt;version&gt;0.2&lt;/version&gt; &lt;/dependency&gt; 确保 AMConfig.properties 和 applicationContext-security.xml 里的 OpenAM 相关配置正确。 com.sun.identity.loginurl=http://openam.example.com:8080/openam/UI/Login 然后运行 mvn package，打包以后放到 Tomcat 运行。这里要确认没有配置 Tomcat Agent的全局 OpenAM Filter，这里也不需要在项目的 web.xml 中增加 Filter 配置。完整的代码在 Github。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>ldap</tag>
        <tag>sso</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开发 OpenAM Java 客户端应用]]></title>
    <url>%2F2012%2F04%2F12%2Fusing-openam-develop-dlient-applications%2F</url>
    <content type="text"><![CDATA[在 Agent 安装完成之后，可以使用自带的 agentsample 应用登录。这里主要讲一下如何在 SSO之后拿到 SSOToken，以及相关 Session 信息的获取。完整的代码在 Github。 Agent 的安装在上一篇已经介绍，这里需要先配置一个 Policies，然后在客户端项目 web.xml 中加入 &lt;filter&gt; &lt;filter-name&gt;Agent&lt;/filter-name&gt; &lt;display-name&gt;Agent&lt;/display-name&gt; &lt;description&gt;SJS Access Manager Tomcat Policy Agent Filter&lt;/description&gt; &lt;filter-class&gt;com.sun.identity.agents.filter.AmAgentFilter&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;Agent&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;dispatcher&gt;REQUEST&lt;/dispatcher&gt; &lt;dispatcher&gt;INCLUDE&lt;/dispatcher&gt; &lt;dispatcher&gt;FORWARD&lt;/dispatcher&gt; &lt;dispatcher&gt;ERROR&lt;/dispatcher&gt; &lt;/filter-mapping&gt; 在项目中获取 token 相关内容 SSOTokenManager manager = SSOTokenManager.getInstance(); SSOToken token = manager.createSSOToken(request); if (manager.isValidToken(token)) { java.security.Principal principal = token.getPrincipal(); out.println(&quot;SSOToken Principal name: &quot; + principal.getName()); out.println(&quot;&lt;br /&gt;&quot;); } 分别部署两个相同的应用到 Tomcat，取名 Client1，Client2。访问任意应用，另外一个应用也自动登录。 参考 OpenAM 10.0.0 Developer’s Guide OpenAM 10.0.0 Installation Guide]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>ldap</tag>
        <tag>sso</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装 Tomcat Policy Agent]]></title>
    <url>%2F2012%2F04%2F06%2Finstalling-the-apache-tomcat-policy-agent%2F</url>
    <content type="text"><![CDATA[本文参照 Installing the Apache Tomcat Policy Agent，在开始之前必须停止 Tomcat。 在 OpenAM 中配置 Tomcat Agent登录 OpenAM，Access Control - Top Level Realm - Agents - J2EE，在 Agent 下边点击按钮 New Name: tomcatAgent Password: 123456 Configuration: Centralized Server URL: http://openam.example.com:8080/openam Agent URL: http://website.example.com:8080/agentapp 安装 Tomcat Policy Agent创建密码文件（在需要配置 Agent 的机器） # umask 366 # echo 123456 &gt; /var/tmp/passwd 安装 # wget http://download.forgerock.org/downloads/openam/j2eeagents/stable/3.0.3/tomcat_v6_agent_303.zip # cp tomcat_v6_agent_303.zip /opt/tomcat6 # cd /opt/tomcat6 # unzip tomcat_v6_agent_303.zip # j2ee_agents/tomcat_v6_agent/bin/agentadmin --install Tomcat Server Config Directory : /opt/tomcat6/conf OpenSSO server URL : http://openam.example.com:8080/openam $CATALINA_HOME environment variable : /opt/tomcat6 Tomcat global web.xml filter install : false Agent URL : http://website.example.com:8080/agentapp Agent Profile name : tomcatAgent Agent Profile Password file name : /var/tmp/passwd 拷贝示例程序 # cp j2ee_agents/tomcat_v6_agent/etc/agentapp.war webapps # cp j2ee_agents/tomcat_v6_agent/sampleapp/dist/agentsample.war webapps 如果是同一个浏览器，先注销 OpenAM，启动 Tomcat，访问 agentsample，会被重定向到 OpenAM 的登录页面，这时使用 amadmin/password 登录是有问题的。 在 OpenAM 中配置 Policy再次登录 OpenAM，Access Control - Top Level Realm - Subjects，在 User 下边点击 New，新建 4 个测试用户 ID : user001 First Name : User Last Name : One Full Name : User One Password : firstuser Confirm Password : firstuser User Status : Active ID : user002 First Name : User Last Name : Two Full Name : User Two Password : seconduser Confirm Password : seconduser User Status : Active ID : user003 First Name : User Last Name : Three Full Name : User Three Password : thirduser Confirm Password : thirduser User Status : Active ID : user004 First Name : User Last Name : Four Full Name : User Four Password : fourthuser Confirm Password : fourthuser User Status : Active Access Control - Top Level Realm - Policies，点击 New Policy，在 Rules 下边点击 New，选择 URL Policy Agent Name: URLPolicy Resource Name: http://website.example.com:8080/agentsample/* Actions : Select and allow both GET and POST 在 Subjects 下边点击 New，选择 OpenAM Identity Subject（如果选择 Authenticated Users，不限制用户） Name: userAccess Exclusive : Not ticked 在 New Policy 的 General Name：samplePolicy 点击 Save. 如果是同一个浏览器，先注销 OpenAM，访问 agentsample，会被重定向到 OpenAM 的登录页面，这时使用 user001，user002 可以正常登录，使用其它用户不可以。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ldap</tag>
        <tag>sso</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 RHEL6 上安装 OpenAM]]></title>
    <url>%2F2012%2F04%2F05%2Finstall-openam-on-rhel6%2F</url>
    <content type="text"><![CDATA[由于参加 Cloud Foundry 和清明假期，这个系列中断了几天。从今天开始继续。官网“provides the community with a new home for Sun Microsystems’ OpenSSO product.”和 CAS 类似，也是 SSO 的一个实现。本人也简单玩过 CAS，相对来讲，OpenAM 提供了图形安装界面，CAS 基本都需要修改配置文件。并且 OpenAM 对应用系统的侵入性可能更小一些。 这里使用了 Nightly Build，因为9 以前的版本默认还不支持 OpenDJ。具体的安装过程可以看 Installing OpenAM Core Services，这里只写一些关键点。 在安装之前，需要设置 JVM 参数，否则会报 OOM。主要是 PermSize 的问题。在 catalina.sh 中加入 export JAVA_OPTS=&apos;-Xms1024m -Xmx2048m -XX:PermSize=256m -XX:MaxPermSize=256m&apos; 准备 war 文件 # cp openam_20120228.war /opt/tomcat6/webapps # mv openam_20120228.war openam.war 在 OpenDJ 中建立 Base DN 这里是测试环境，没有 DNS，所以需要修改 hosts。OpenDJ 和 OpenAM 都安装到 openam.example.com，OpenDJ 监听在 1389 端口。强烈建议生产环境使用 DNS，因为在配置中需要多次用到域名属性。 这时可以启动 tomcat，访问 http://openam.example.com:8080/openam 进行配置。 在安装过程中，如果遇到错误，查看 install.log，尝试删除 OpenDJ 中两个 Base DN 下的所有子条目，并执行以下命令重新配置 OpenAM rm -rf $HOME/openam $HOME/.openssocfg Step 2 Step 3 这步发现一个 OpenAM 本身的问题，OpenAM 本身支持多国语言，在客户端系统是中文环境和英文环境下， 在 Step 3 中显示的内容稍有不同，在中文环境下不显示 OpenDJ 选项（可能是 OpenAM 版本问题）。 Step 4 OpenDJ 配置完成后，使用 amadmin/password 登录系统。 如果要完成后边的 Sample，需要下载 Example.ldif 导入 OpenDJ。还需要下载 Nightly Build，在这个包中，会有一些附加工具和示例应用。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ldap</tag>
        <tag>sso</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 RHEL6 上安装 Java]]></title>
    <url>%2F2012%2F03%2F29%2Finstall-java-on-rhel6%2F</url>
    <content type="text"><![CDATA[卸载老版本的 Java # java -version # rpm -qa|grep java # rpm -e --nodeps java-1.5.0-gcj-1.5.0.0-29.1.el6.x86_64 bin 安装 # chmod a+x jdk-6u31-linux-x64.bin # cd /opt/ # mkdir java # cd java # /home/bati/Downloads/jdk-6u31-linux-x64.bin Java(TM) SE Development Kit 6 successfully installed. ...... Press Enter to continue..... Done. 安装到执行 bin 文件的目录。 rpm 安装 # chmod a+x jdk-6u31-linux-x64-rpm.bin # ./jdk-6u31-linux-x64-rpm.bin 安装到了 /usr/java 配置环境变量 # vim /etc/profile 增加以下内容 export JAVA_HOME=/usr/java/jdk1.6.0_31 export PATH=$JAVA_HOME/bin:$PATH 即时生效： # source /etc/profile # java -version]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 RHEL6 上安装 OpenDJ]]></title>
    <url>%2F2012%2F03%2F29%2Finstall-opendj-directory-server-on-rhel6%2F</url>
    <content type="text"><![CDATA[官网”OpenDJ is a extension of the Sun Microsystems’ initiated OpenDS project and offers a fully supported product for it.”接下来的这几篇文章会介绍 ForgeRock Open Platform 的 OpenDJ、OpenIDM、OpenAM 三个产品的安装、配置，以及如何使用他们来搭建企业用户管理、访问认证的基础平台。 确认 Java 环境，否则请参考在 RHEL6 上安装 Java # java -version java version &quot;1.6.0_31&quot; Java(TM) SE Runtime Environment (build 1.6.0_31-b04) Java HotSpot(TM) 64-Bit Server VM (build 20.6-b01, mixed mode) 下载 # wget http://download.forgerock.org/downloads/opendj/2.4.5/OpenDJ-2.4.5.zip 安装 # cp OpenDJ-2.4.5.zip /opt # cd /opt # unzip OpenDJ-2.4.5.zip # mv OpenDJ-2.4.5 opendj # cd opendj 如果在图形界面 # ./setup 这里以 shell 安装为例 # ./setup --cli OpenDJ 2.4.5 安装程序正在初始化，请稍候... 您希望将哪些内容用作目录服务器的初始超级用户 DN？ [cn=Directory Manager]: 请提供用于初始超级用户的密码: password 请重新输入密码以进行确认: password 您希望目录服务器使用哪个端口接受来自 LDAP 客户端的连接？ [389]: 1389 您希望管理连接器在哪个端口上接受连接？ [4444]: 是否要在服务器中创建基 DN？ (yes / no) [yes]: 提供目录数据的基 DN: [dc=example,dc=com]: 用于填充数据库的选项: 1) 仅创建基条目 2) 将数据库保留为空 3) 从 LDIF 文件中导入数据 4) 加载自动生成的样例数据 输入选项 [1]: 是否要启用 SSL？ (yes / no) [no]: 是否要启用 StartTLS？ (yes / no) [no]: 是否要在完成配置时启动服务器？ (yes / no) [yes]: 安装摘要 ============= LDAP 侦听器端口: 389 管理连接器端口: 4444 LDAP 安全访问: 已禁用 超级用户 DN: cn=Directory Manager 目录数据: 创建新的基 DN dc=example,dc=com。 基 DN 数据: 仅创建基条目 (dc=example,dc=com) 在完成配置时启动服务器 您希望执行哪些操作？ 1) 使用上面的参数设置服务器 2) 再次提供安装参数 3) 打印等效的非交互命令行 4) 取消并退出 输入选项 [1]: 请参见 /tmp/opends-setup-1826847628910167129.log 以了解有关此操作的详细日志。 正在配置目录服务器 ..... 完成。 正在创建基条目 dc=example,dc=com ..... 完成。 正在启动目录服务器 ................ 停止 # bin/stop-ds 启动 # bin/start-ds 启动控制面板 # bin/control-panel 加入到 service 方式一（以 root 运行） # bin/create-rc-script -f /etc/init.d/opendj 加入到 service 方式二（如果没有 -u 参数会以 root 运行）。生产环境建议以这种方式运行，但 OpenDJ 的某些版本有 Bug：OPENDJ-17 # useradd -s /sbin/nologin opendj # chown -R opendj:opendj /opt/opendj # bin/create-rc-script -f /etc/init.d/opendj -u opendj 查看 service 启动设置，345 已经生效，Reboot 之后 OpenDJ 自动启动 # chkconfig --list|grep opendj opendj 0:关闭 1:关闭 2:关闭 3:启用 4:启用 5:启用 6:关闭 运行 # service opendj { start | stop | restart } 更详细的安装文档，请参照 官方文档 或者在 CentOS6 上安装 OpenDJ（GUI）。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ldap</tag>
        <tag>sso</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 RHEL6 上安装 Apache2]]></title>
    <url>%2F2012%2F03%2F29%2Finstall-apache2%2F</url>
    <content type="text"><![CDATA[最近在配置 OpenAM ，在参考这篇文章时遇到了一个 [error] Certificate not found: &#39;Server-Cert&#39; 的问题。在尝试很多次之后无解，所以下载最新的 apache2.2.22 替换系统自带的 apache2.2.15，看能不能解决问题。 首先，卸载自带的 Apache2 System - Administration - Add/Remove Sofeware - Web Services - Web Server 然后，安装 Apache2.2.22 # wget http://apache.etoak.com//httpd/httpd-2.2.22.tar.gz # tar zxvf httpd-2.2.22.tar.gz # cd httpd-2.2.22 # ./configure --prefix=/opt/apache2 # make # make install 如果遇到 configure: error: no acceptable C compiler found in $PATH，参考这篇文章。 启动 Apache # /opt/apache2/bin/apachectl start 停止 Apache # /opt/apache2/bin/apachectl stop 把 Apache 加入到系统服务 # cp /opt/apache2/bin/apachectl /etc/rc.d/init.d/httpd 修改文件 # cd /etc/rc.d/init.d/ # vim httpd 加入以下内容 ### # Comments to support chkconfig on RedHat Linux # chkconfig: 2345 90 90 # description:http server ### 启动 Apache # service httpd start 停止 Apache # service httpd stop 加入到系统启动列表 # chkconfig --add httpd 系统启动自动运行 # chkconfig --level 345 httpd on]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>apache</tag>
        <tag>sso</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 RHEL6 上安装 gcc]]></title>
    <url>%2F2012%2F03%2F28%2Finstall-gcc%2F</url>
    <content type="text"><![CDATA[默认情况下，RHEL 不会安装 gcc，在编译安装软件时会遇到 configure: error: no acceptable C compiler found in $PATH 的问题。 先查找名称： # yum list|grep gcc 安装： # yum install gcc.x86_64 gcc-c++.x86_64 gcc-objc++.x86_64]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>gcc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 RHEL6 上安装 phpLDAPAdmin]]></title>
    <url>%2F2012%2F03%2F10%2Finstall-phpldapadmin-on-rhel6%2F</url>
    <content type="text"><![CDATA[安装 php： # yum install php php-ldap 配置 Apache： # vim /etc/httpd/conf/httpd.conf 加入如下设置： AddType application/x-httpd-php .php AddType application/x-httpd-php-source .phps 安装 phpLDAPAdmin： # tar xzvf phpldapadmin-1.2.2.tgz # mv phpldapadmin-1.2.2 phpldapadmin # cp -R phpldapadmin /var/www/html 打开 phpLDAPAdmin 配置文件： # cd /var/www/html/phpldapadmin/config # cp config.php.example config.php # vim config.php 找到以下内容，去掉注解，修改参数： $servers-&gt;setValue(&apos;server&apos;,&apos;host&apos;,&apos;127.0.0.1&apos;); $servers-&gt;setValue(&apos;server&apos;,&apos;port&apos;,389); $servers-&gt;setValue(&apos;server&apos;,&apos;base&apos;,array(&apos;dc=my-domain,dc=com&apos;)); #olcSuffix $servers-&gt;setValue(&apos;login&apos;,&apos;auth_type&apos;,&apos;cookie&apos;); $servers-&gt;setValue(&apos;login&apos;,&apos;bind_id&apos;,&apos;cn=Manager,dc=my-domain,dc=com&apos;); #olcRootDN 修改配置文件： # vim /etc/openldap/slapd.d/cn=config/olcDatabase={2}bdb.ldif 修改 olcSuffix,olcRootDN，对应上边的配置，增加 olcRootPW（登录密码）。 打开防火墙的 389 端口，启动 OpenLDAP： # service slapd start 打开防火墙的 80 端口，启动 Apache： # service httpd start 访问 http://localhost/phpldapadmin]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ldap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 RHEL6 上安装 OpenLDAP]]></title>
    <url>%2F2012%2F03%2F09%2Finstall-openldap%2F</url>
    <content type="text"><![CDATA[安装需要的包： # yum install openldap openldap-clients openldap-servers 启动服务： # service slapd start 修改密码： # slappasswd New password: Re-enter new password: 参考 RHEL6 官方文档。 OpenLDAP 官方文档。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ldap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置 RHEL6 本地源]]></title>
    <url>%2F2012%2F03%2F08%2Fconfig-yum-on-rhel6%2F</url>
    <content type="text"><![CDATA[如果你没有注册或没有配置本地源的话，一般都会出现下面的情况： Loaded plugins: product-id, refresh-packagekit, security, subscription-manager Updating certificate-based repositories. Setting up Install Process Nothing to do 下面我们要以光盘为 yum 源，你也可以把光盘里面的文件 cp 到系统的某个目录里面： # mount /dev/cdrom /media mount: block device /dev/sr0 is write-protected, mounting read-only 备份 repo 文件： # cd /etc/yum.repos.d/ # cp rhel-source.repo rhel-source.repo.bak 编辑 repo 文件: # vim rhel-source.repo 内容如下： [InstallMedia] name=local_yum baseurl=file:///media gpgcheck=0 enabled=1 执行清理缓存命令： # yum clean all]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>yum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在 Parallels for Mac 上安装 RHEL6]]></title>
    <url>%2F2012%2F03%2F07%2Finstall-rhel6%2F</url>
    <content type="text"><![CDATA[昨天想测试一下 RHEL6 上的 lvs 和 piranha，所以在自己的 Mac 上使用 Parallels 安装了两个虚拟机。在安装过程中，遇到了一些问题，记录一下安装过程。 设置 Parallels VM 选择 Install system with basic video driver 这一步，选择 Skip，是验证安装镜像完整性的。跳过之后选择语言、存储、分区。 在自定义安装的服务器类型这一步，有基本，数据库，WEB，虚拟主机，最小化安装等好多种选择。这里根据需要做出自己的选择，会影响到包的安装。我在这里选择桌面（否则安装完成后是命令行界面）。下边的弹性存储、负载平衡器、高可用全部勾选。如果还希望增加其它组件，可以选择“现在自定义”。 在自定义安装包这一步，可以自由选择需要安装的组件。在上一步做的设置会影响到这一步的默认选中状态。这里不建议选择安装：Apache、Tomcat、JDK 等组件，最好自己从官网下载最新版本。 在此之后就可以喝杯咖啡，重启系统了。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
</search>
